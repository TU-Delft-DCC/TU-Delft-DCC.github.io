[
  {
    "objectID": "listings.html",
    "href": "listings.html",
    "title": "Catalogue",
    "section": "",
    "text": "While the search functionality (top right corner) provides easy access to a particular guide searching by keywords, you can also use this listings page (catalogue) to explore all available guides across all topics.\nEach guide has keywords at the top, and these are clickable. Clicking on a category will bring you back to this page filtered to relevant guides. You can either browse by category using the filters on the right, sort by title or date, or use the search box to find specific topics of interest.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nAccess and reuse\n\n3 min\n\n\nData\n\nData Access\n\nData Reuse\n\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\nAccessing and requesting Project Data Storage space\n\n4 min\n\n\nData\n\nStorage\n\nProject Data Storage\n\n\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\nArchiving\n\n3 min\n\n\nSoftware\n\nArchive\n\n4TU\n\nZenodo\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nArchiving and publishing data\n\n1 min\n\n\nData\n\nLicenses\n\nRelease\n\nPublish\n\nArchive\n\nOffboarding\n\nOwnership\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\nAutomation for software development\n\n1 min\n\n\nSoftware\n\nAutomation\n\nGitHub\n\nGitLab\n\nCI/CD\n\n\n\n\n\n\n\nAug 22, 2025\n\n\n\n\n\n\n\n\n\n\nBranch management\n\n7 min\n\n\nSoftware\n\nVersion Control\n\nCollaboration\n\nBranches\n\nGitflow\n\nGitHub Flow\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\nCI with GitLab\n\n9 min\n\n\nSoftware\n\nAutomation\n\nGitLab\n\nCI/CD\n\nDocker\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\nCITATION.cff\n\n3 min\n\n\nSoftware\n\nDocumentation\n\nCitation\n\n\n\nHow to cite software\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\nCode documentation\n\n1 min\n\n\nSoftware\n\nDocumentation\n\nPython\n\nMATLAB\n\nR\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nCode of conduct\n\n1 min\n\n\nSoftware\n\nDocumentation\n\nCode of Conduct\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\nCode quality\n\n1 min\n\n\nSoftware\n\nCode Quality\n\nCode Style\n\nRefactoring\n\n\n\nOverview of code quality tools and services\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\nCode smells\n\n2 min\n\n\nSoftware\n\nRefactoring\n\n\n\nOverview of common code smells and how to address them\n\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\nCode style and tools\n\n2 min\n\n\nSoftware\n\nCode Quality\n\nCode Style\n\n\n\nOverview of code style guides, static analysis tools, and formatters.\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\nCollaboration\n\n5 min\n\n\nSoftware\n\nVersion Control\n\nCollaboration\n\nForks\n\nPull Requests\n\nGitHub Flow\n\nBranches\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\nConfigure SSL/TLS certificates\n\n6 min\n\n\nInfrastructure\n\nServers\n\nVPS\n\nCybersecurity\n\n\n\nRequesting and configuring SSL certificates on TU Delft VPS\n\n\n\nDec 2, 2025\n\n\n\n\n\n\n\n\n\n\nContributing guidelines\n\n2 min\n\n\nSoftware\n\nDocumentation\n\nContributing Guidelines\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\nCreating a Python package\n\n12 min\n\n\nSoftware\n\nPackage\n\nPython\n\npyproject.toml\n\nuv\n\n\n\n\n\n\n\nApr 2, 2025\n\n\n\n\n\n\n\n\n\n\nCreating an R package\n\n4 min\n\n\nSoftware\n\nPackage\n\nR\n\ndevtools\n\nusethis\n\nroxygen2\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nData backup\n\n4 min\n\n\nData\n\nStorage\n\nData Backup\n\n\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\nData collection\n\n2 min\n\n\nData\n\nCollection\n\nData Conventions\n\nData Access\n\nData Reuse\n\neLabJournal\n\nRSpace\n\n\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\nData conventions\n\n3 min\n\n\nData\n\nData Conventions\n\n\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\nData licensing\n\n3 min\n\n\nData\n\nLicenses\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\nData processing\n\n2 min\n\n\nData\n\nData Processing\n\nVersion Control\n\n\n\n\n\n\n\nApr 6, 2025\n\n\n\n\n\n\n\n\n\n\nData security\n\n3 min\n\n\nData\n\nStorage\n\nData Security\n\n\n\n\n\n\n\nJul 2, 2025\n\n\n\n\n\n\n\n\n\n\nData sharing\n\n4 min\n\n\nData\n\nStorage\n\nData Sharing\n\n\n\n\n\n\n\nJul 1, 2025\n\n\n\n\n\n\n\n\n\n\nData storage\n\n2 min\n\n\nData\n\nStorage\n\nData Security\n\nData Sharing\n\nData Backup\n\n\n\n\n\n\n\nJun 23, 2025\n\n\n\n\n\n\n\n\n\n\nDead code\n\n7 min\n\n\nSoftware\n\nRefactoring\n\nDead Code\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\nDeep Nesting\n\n5 min\n\n\nSoftware\n\nRefactoring\n\nDeep Nesting\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\nDevelopment workflow\n\n1 min\n\n\nSoftware\n\nVersion Control\n\nCollaboration\n\nProject Organization\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\nDocumentation\n\n2 min\n\n\nSoftware\n\nDocumentation\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\nDuplicated code\n\n3 min\n\n\nSoftware\n\nRefactoring\n\nDuplicated Code\n\n\n\nFixing duplicated code in your codebase\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\nEnvironment and dependency management\n\n1 min\n\n\nSoftware\n\nProject Organization\n\nEnvironments\n\nDependencies\n\nPython\n\nMATLAB\n\nR\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\n\n\n\n\n\n\n\n\nEnvironment and dependency management in MATLAB\n\n1 min\n\n\nSoftware\n\nEnvironments\n\nDependencies\n\nMATLAB\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\nEnvironment and dependency management in Python\n\n4 min\n\n\nSoftware\n\nEnvironments\n\nDependencies\n\nPython\n\nconda\n\nvenv\n\nvirtualenv\n\npyproject.toml\n\nuv\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\nEnvironment and dependency management in R\n\n2 min\n\n\nSoftware\n\nEnvironments\n\nDependencies\n\nR\n\nconda\n\nrenv\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\nFAIR Software\n\n1 min\n\n\nSoftware\n\nFAIR\n\n\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\nFAIR checklist for research software\n\n5 min\n\n\nSoftware\n\nFAIR\n\nChecklist\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\nFAIR data\n\n2 min\n\n\nData\n\nFAIR\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n2 min\n\n\nData\n\nGetting Started\n\n\n\n\n\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n3 min\n\n\nSoftware\n\nGetting Started\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n1 min\n\n\nInfrastructure\n\nGetting Started\n\n\n\n\n\n\n\nOct 27, 2025\n\n\n\n\n\n\n\n\n\n\nGitHub Actions\n\n11 min\n\n\nSoftware\n\nAutomation\n\nGitHub\n\nCI/CD\n\n\n\n\n\n\n\nAug 22, 2025\n\n\n\n\n\n\n\n\n\n\nGitLab pipelines\n\n1 min\n\n\nSoftware\n\nAutomation\n\nGitLab\n\nCI/CD\n\n\n\n\n\n\n\nAug 22, 2025\n\n\n\n\n\n\n\n\n\n\nHard coding\n\n5 min\n\n\nSoftware\n\nRefactoring\n\nHard-coding\n\n\n\nExplanation of hard coding and how to address it\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\nHosting\n\n2 min\n\n\nSoftware\n\nDocumentation\n\nHosting\n\nGitHub Pages\n\nRead the Docs\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\nInappropriate Intimacy\n\n3 min\n\n\nSoftware\n\nRefactoring\n\nTight Coupling\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\nLarge Classes\n\n4 min\n\n\nSoftware\n\nRefactoring\n\nLarge Classes\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\nLong Method\n\n6 min\n\n\nSoftware\n\nRefactoring\n\nLong Methods\n\n\n\nRefactor long methods into smaller, more focussed functions\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\nMATLAB documentation\n\n1 min\n\n\nSoftware\n\nDocumentation\n\nMATLAB\n\n\n\n\n\n\n\n\n\n\n\n\n\nMany arguments\n\n6 min\n\n\nSoftware\n\nRefactoring\n\nMany Arguments\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\nMore testing concepts\n\n3 min\n\n\nSoftware\n\nTesting\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\nMount Project Drive on server\n\n5 min\n\n\nData\n\nStorage\n\nProject Data Storage\n\nVPS\n\n\n\n\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\n\n\n\nMoving data to remote servers\n\n5 min\n\n\nInfrastructure\n\nData\n\nServers\n\nscp\n\nCitrix\n\n\n\nSecurely transfer data to and from TU Delft virtual servers using SCP or Citrix.\n\n\n\nJul 9, 2025\n\n\n\n\n\n\n\n\n\n\nOffboarding and ownership\n\n1 min\n\n\nData\n\nOffboarding\n\nOwnership\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\nOnline services\n\n3 min\n\n\nSoftware\n\nCode Quality\n\nCode Coverage\n\n\n\nOverview of code quality and security tools and practices\n\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\nPackaging\n\n1 min\n\n\nSoftware\n\nPackage\n\nPython\n\nR\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nPackaging, releases and archiving\n\n1 min\n\n\nSoftware\n\nPackage\n\nRelease\n\nArchive\n\n\n\n\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\nPlanning\n\n2 min\n\n\nData\n\nPlanning\n\nDMP\n\nPrivacy\n\n\n\n\n\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n1 min\n\n\nData\n\nPrivacy\n\n\n\n\n\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\nProject management\n\n5 min\n\n\nSoftware\n\nVersion Control\n\nCollaboration\n\nProject Management\n\nGitHub\n\nIssues\n\nProject Board\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\nProject structure\n\n8 min\n\n\nSoftware\n\nProject Organization\n\nRepository Structure\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\nProject templates and reusability\n\n4 min\n\n\nSoftware\n\nProject Organization\n\nRepository Structure\n\nGit Subtree\n\nGit Submodules\n\nTemplates\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\nPublishing data\n\n2 min\n\n\nData\n\nRelease\n\nPublish\n\nArchive\n\n4TU\n\nZenodo\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\nPython code documentation\n\n8 min\n\n\nSoftware\n\nDocumentation\n\nPython\n\nDocstrings\n\nCode comments\n\n\n\n\n\n\n\nFeb 27, 2025\n\n\n\n\n\n\n\n\n\n\nR documentation\n\n4 min\n\n\nSoftware\n\nDocumentation\n\nR\n\nroxygen2\n\nVignettes\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nREADME\n\n2 min\n\n\nSoftware\n\nDocumentation\n\nREADME\n\n\n\nWriting a good README\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\nRefactoring\n\n7 min\n\n\nSoftware\n\nCode Quality\n\nRefactoring\n\n\n\nOverview of refactoring code practices\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\nRelease your Python package\n\n3 min\n\n\nSoftware\n\nPackage\n\nRelease\n\nPython\n\nPyPI\n\nTestPyPI\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nRelease your R package\n\n3 min\n\n\nSoftware\n\nPackage\n\nRelease\n\nR\n\nCRAN\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nReleases\n\n3 min\n\n\nSoftware\n\nPackage\n\nRelease\n\nPython\n\nR\n\nPyPI\n\nCRAN\n\nChangelog\n\nSemVer\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\nRemote Servers\n\n3 min\n\n\nInfrastructure\n\nServers\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\nRequest a VPS\n\n5 min\n\n\nInfrastructure\n\nServers\n\nVPS\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\nSet up SSH tunneling for a VPS\n\n5 min\n\n\nInfrastructure\n\nServers\n\nVPS\n\nSSH Tunneling\n\nSSH Keys\n\nBastion Host\n\nSCP\n\nCybersecurity\n\n\n\nhow to set up a single-step SSH connection to a VPS using SSH tunneling\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\nSetting up a GitLab runner for MATLAB\n\n12 min\n\n\nSoftware\n\nAutomation\n\nMATLAB\n\nGitLab\n\nCI/CD\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide effects and external state\n\n2 min\n\n\nSoftware\n\nRefactoring\n\nSide Effects\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\nSoftware design principles\n\n8 min\n\n\nSoftware\n\nDesign\n\nArchitecture\n\nC4\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\nSoftware licenses\n\n7 min\n\n\nSoftware\n\nDocumentation\n\nLicenses\n\n\n\nLicensing software developed at TU Delft\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\nSoftware management plan\n\n3 min\n\n\nSoftware\n\nManagement\n\nSMP\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\nSoftware testing\n\n2 min\n\n\nSoftware\n\nTesting\n\n\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\nStorage options\n\n12 min\n\n\nData\n\nStorage\n\nProject Data Storage\n\n\n\nData storage options at TU Delft\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\nTesting in MATLAB\n\n3 min\n\n\nSoftware\n\nTesting\n\nMATLAB\n\n\n\nWriting and running tests with MATLAB\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\nTesting in Python\n\n3 min\n\n\nSoftware\n\nTesting\n\nPython\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\nTesting in R\n\n3 min\n\n\nSoftware\n\nTesting\n\nR\n\n\n\n\n\n\n\nApr 3, 2025\n\n\n\n\n\n\n\n\n\n\nTesting strategy\n\n5 min\n\n\nSoftware\n\nTesting\n\n\n\nGuidelines for researchers new to testing and experienced test writers.\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nTooling\n\n9 min\n\n\nSoftware\n\nDocumentation\n\nSphinx\n\nJupyterBook\n\nMkDocs\n\nQuarto\n\npkgdown\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\n\n\n\n\n\n\n\n\nTooling\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nTypes of tests\n\n2 min\n\n\nSoftware\n\nTesting\n\n\n\nDifferent types of tests to ensure your software works as expected.\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nWeb Servers\n\n6 min\n\n\nInfrastructure\n\nServers\n\nVPS\n\nApache\n\nNginx\n\n\n\n\n\n\n\nAug 27, 2025\n\n\n\n\n\n\n\n\n\n\nWorkflow management\n\n11 min\n\n\nSoftware\n\nProject Organization\n\nWorkflow Management\n\nSnakemake\n\n\n\n\n\n\n\nApr 24, 2025\n\n\n\n\n\n\n\n\n\n\neLabJournal and RSpace\n\n2 min\n\n\nData\n\nELN\n\nRSpace\n\neLabJournal\n\n\n\n\n\n\n\nMay 8, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/catalog.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Catalogue**"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/collaboration.html",
    "href": "docs/software/development_workflow/collaboration.html",
    "title": "Collaboration",
    "section": "",
    "text": "Effective collaboration is a fundamental aspect of any successful software development project.\nKey points:",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Collaboration"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/collaboration.html#collaborative-workflow",
    "href": "docs/software/development_workflow/collaboration.html#collaborative-workflow",
    "title": "Collaboration",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\nFollowing the GitHub Flow model, everything starts from the main branch. Developers can create feature branches from the main branch to isolate their work. Once ready, changes are merged back to main.\n\n\n\n\n\n\nTip Workflow variations\n\n\n\nWhile the GitHub Flow workflow focuses on main, some opt for a hybrid approach to include a develop branch either as a staging area for pre-release testing or ongoing integration (aggregating features before merging to main). This approach borrows elements from GitFlow (e.g., a long-lived develop branch) without adopting its full branching model.\n\n\nA common workflow in a collaborative development project.\n\nCreate a feature branch: Start by creating a new branch of the main branch (or develop if used). This branch should have a descriptive name to give an idea of the work that will be done, such as a new feature or a bug fix. This separation allows you to work independently without affecting the main codebase.\nMake changes and commit: Work on your branch, making the necessary changes to the code. Commit these changes with clear, descriptive messages.\nOpen a pull request: Once you have made your changes, open a pull request (PR). PRs allow for discussion, review, and additional changes if necessary.\nReview: Before merging, your changes might go through a review process where other team members can give feedback. Some projects may require a review before merging is allowed.\nMerge: Finally, once your changes are reviewed and tested, you can merge the pull request into main (or develop, if applicable). This incorporates your contributions into the project, making them part of the official codebase.\nDelete feature branch: Feature branches should be short-lived, thus avoiding potential conflicts due to the divergence of the code.\n\n\n\n\n\n\n\nNoteSequence diagram of workflow\n\n\n\n\n\n\n\n\n\n\nsequenceDiagram\n    participant A as Author\n    participant R as Reviewer\n    A-&gt;&gt;A: Write some code in your branch or fork\n    A-&gt;&gt;R: Open a pull request\n    R-&gt;&gt;R: Add comments to a review\n    R-&gt;&gt;A: Submit a review\n    loop Until approved\n        A-&gt;&gt;R: Address or respond to review comments\n        R-&gt;&gt;A: Clarify or resolve comments\n    end\n    R-&gt;&gt;A: Approve pull request\n    A-&gt;&gt;A: Merge pull request\n    A-&gt;&gt;A: Delete branch\n\n\n\n\n\n\n\n\n\n\nForking\nExternal collaborators who do not have the same administrative rights to the repository can fork the project. They make their changes on their forked repository in a new feature branch. Steps 3-5 remain the same.\n\n\n\n\n\n\nTip Create ‚ÄúDraft‚Äù Pull Requests!\n\n\n\nWith Draft PR‚Äôs you:\n\nwant to signal that a pull request is just the start of the conversation and your code isn‚Äôt in any state to be judged.\nor have no intention of ever merging it, but you‚Äôd still like people to check it out locally and give you feedback.\nor opened a pull request without any code at all in order to get the discussion started.\n\n\n\n\n\nConflict resolution\nConflicts occur when two or more changes compete with each other, typically during a merge or rebase operation in Git. With pull requests, code reviews and testing you can catch potential conflicts before they are merged into the main codebase.\nConflicts usually come up and are resolved during a pull request:\n\nReview Conflicts: When a conflict is detected in a pull request, GitHub will alert you. Start by reviewing the conflicting files to understand the nature of the conflict.\nPull and Merge Locally: Fetch the latest changes from the main branch and attempt to merge them into your feature/develop branch locally. This will allow you to resolve conflicts on your local machine.\nResolve Conflicts: This might involve choosing one change over another or merging the changes manually.\nTest Changes: After resolving conflicts, thoroughly test your changes to ensure that the merged code works as expected.\nCommit and Push: Once conflicts are resolved and changes are tested, commit the resolved conflicts and push your changes back to the feature/develop branch on GitHub.\nComplete the Pull Request: After resolving conflicts and pushing your changes, review the pull request again to ensure everything is in order. If all checks pass and your team approves the changes, you can complete the merge into the main branch.\n\nEffective conflict resolution ensures that changes can be integrated smoothly and that the project remains on track. Conflicts cannot always be avoided, but they can be managed. Clear communication and adherence to established practices is the way to go.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nFor more information on Draft PRs - Introducing Draft Pull Requests.\nGitHub - Resolving a merge conflict\nCode Refinery - Lesson on conflict resolution",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Collaboration"
    ]
  },
  {
    "objectID": "docs/tud-support/index.html",
    "href": "docs/tud-support/index.html",
    "title": "Research Support Staff Guide",
    "section": "",
    "text": "üöß Under construction! üèóÔ∏è"
  },
  {
    "objectID": "docs/software/testing/test_types.html",
    "href": "docs/software/testing/test_types.html",
    "title": "Types of tests",
    "section": "",
    "text": "In writing tests for research software, we often differentiate between four types of tests: unit tests, integration tests, and end-to-end tests. In addition, regression tests are used to ensure that recent code changes haven‚Äôt adversely affected existing features or functionality.\n\n\n\nTesting pyramid ¬© 2023 SketchingDev\n\n\n\nUnit tests\nA unit test is a type of test where individual units or components of the software application are tested in isolation from the rest of the application. A unit can be a function, method, or class. The main purpose of unit testing is to validate that each unit of the software performs as designed.\n\n\nIntegration tests\nIntegration testing is a level of software testing where individual units are combined and tested as a group. The purpose is to verify that the units work together as expected and that the interfaces between them function correctly. Integration tests aims to expose defects in the interactions between integrated components.\n\n\nEnd-to-end tests\nEnd-to-end testing is focused on checking the entire system from start to finish, simulating real use cases. The goal is to verify the software functions as a whole from the user‚Äôs perspective.\n\n\nRegression tests\nRegression testing aims to verify that recent code changes haven‚Äôt adversely affected existing features or functionality. It involves re-running previously executed test cases to ensure that the software still behaves as expected after modifications. The primary purpose of regression testing is to catch unintended side effects of code changes and ensure that new features or bug fixes haven‚Äôt introduced regressions or broken existing functionality elsewhere in the code. Regression tests can include both unit tests and integration tests, as well as higher-level tests.\n\n\nDesigning a test case\nFor more complex integration, regression, or end-to-end tests, it can be useful to first describe the test case in words.\n\nDescription: Description of test case\nPreconditions: Conditions that need to be met before executing the test case\nTest Steps: To execute test cases, you need to perform some actions. Mention all the test steps in detail and the order of execution\nTest Data: If required, list data that needed to execute the test cases\nExpected Result: The result we expect once the test cases are executed\nPostcondition: Conditions that need to be achieved when the test case was successfully executed",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Test types"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html",
    "href": "docs/software/testing/r_test.html",
    "title": "Testing in R",
    "section": "",
    "text": "R offers several testing frameworks, but the most widely used is the testthat package. It provides a straightforward way to write unit tests for your R code. The package is designed to be easy to use, even for those who are new to testing.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#setting-up-testthat",
    "href": "docs/software/testing/r_test.html#setting-up-testthat",
    "title": "Testing in R",
    "section": "Setting up testthat",
    "text": "Setting up testthat\nTo get started with testthat, you need to install the package and set up a testing structure in your R project. You need to have a tests directory in your project root. Inside this directory, you need a subdirectory called testthat. You can create this with:\n# Install packages if not already installed, and load them\ninstall.packages(c(\"devtools\", \"testthat\", \"usethis\", \"covr\"))\n# and then run\nusethis::use_testthat()\nIt will set up the tests/testthat directory and a testthat.R file. This file will contain the setup code for your tests. It will also add the testthat package to your package‚Äôs DESCRIPTION file under the ‚ÄòSuggests‚Äô field.\n\n\n\n\n\n\nTip Tip\n\n\n\nPlease visit our Project structure guide for a recommendation on how to structure your R project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#writing-tests-with-testthat",
    "href": "docs/software/testing/r_test.html#writing-tests-with-testthat",
    "title": "Testing in R",
    "section": "Writing tests with testthat",
    "text": "Writing tests with testthat\nYour files in the tests/testthat directory should be named test-*.R, where * is a descriptive name for the test. The testthat.R file will source all the test files in the tests/testthat directory, so you do not need to load them manually. They must match the same structure as the functions under your /R directory. For example, if you have a function my_function() in R/my_function.R, you should create a test file tests/testthat/test-my_function.R to test it.\nSo let‚Äôs say we have a trivial function count_tu_delft_faculties() in your project that counts the number of faculties at TU Delft. TU Delft has 8 faculties, so the function should return 8. You would create a file R/count_tu_delft_faculties.R with the following content:\n#' Count the number of faculties at TU Delft\n#'\n#' @return The number of faculties at TU Delft\n#' @export\ncount_tu_delft_faculties &lt;- function() {\n    # This is a placeholder function\n    return(8)\n}\nThis function is a simple placeholder that returns the number of faculties at TU Delft. In a real-world scenario, this function would contain logic to count the faculties dynamically, perhaps by querying a database or an API.\nThen, you would create a test file tests/testthat/test-count_tu_delft_faculties.R to test this function. The test file should contain tests that check if the function returns the expected value (8 in this case).\ntest_that(\"description of the test\", {\n  # Call the function\n  result &lt;- count_tu_delft_faculties()\n  \n  # Test that it returns the expected value\n  expect_identical(result, 8)\n  \n  # We could add more expectations\n  expect_type(result, \"double\")\n  # etc.\n})\nYou can use various expect_* functions to check the results of your code.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#running-tests",
    "href": "docs/software/testing/r_test.html#running-tests",
    "title": "Testing in R",
    "section": "Running tests",
    "text": "Running tests\nYou should run your tests regularly to ensure that your code is working as expected. You can run all the tests in your package using:\ndevtools::test()\nThis will run all the tests in the tests/testthat directory and report any failures or errors. You can also run individual test files by specifying the file name:\ndevtools::test_file(\"tests/testthat/test-count_tu_delft_faculties.R\")\nThis will run only the tests in the specified file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#test-coverage",
    "href": "docs/software/testing/r_test.html#test-coverage",
    "title": "Testing in R",
    "section": "Test coverage",
    "text": "Test coverage\nTo check the test coverage of your package, you can use devtools::test_coverage(). This will generate a report showing which part of your code is covered by tests and which are not.\nYou can also use the covr package to have a more detailed control over coverage reporting. To check the coverage of your package:\ncovr::package_coverage()\nYou can also use covr::report() to generate an HTML report of the coverage.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nR packages book - Testing\nIntroduction to testing R code\ntestthat package\ncovr package",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html",
    "href": "docs/software/testing/matlab.html",
    "title": "Testing in MATLAB",
    "section": "",
    "text": "This guide explains how to write and execute tests in MATLAB. For additional details, refer to the official MATLAB documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html#writing-tests",
    "href": "docs/software/testing/matlab.html#writing-tests",
    "title": "Testing in MATLAB",
    "section": "Writing tests",
    "text": "Writing tests\nMATLAB supports script-based, function-based, and class-based unit tests, allowing for a range of testing strategies from simple to advanced use cases. See the MATLAB documentation for more information:\n\nMatlab - Ways to write unit tests\n\nScript-based testing\nFunction-based testing\nClass-based testing\n\n\n\nConvention for writing tests\n\nIt is recommened place tests in a separate folder, typically named tests/.\nPrefix test files with ‚Äútest‚Äù followed by the file that is tested. For example, a test for the file DrawRandomNumber.m should be called TestDrawRandomNumber.m. Matlab will recognize any scripts that are prefixed or suffixed with the string ‚Äútest‚Äù as tests.\n\n\n\nClass-based unit tests\nBecause of the limited features of the script- and function-based testing, this guide will discuss class-based testing. Class-based tests give you access to shared test fixtures, test parameterization, and grouping tests into categories. Check out our additional testing concepts for more information about these concepts.\n\n\n\n\n\n\nTip Introduction to class-based testing\n\n\n\nCheck out this short MATLAB video on writing class-based tests.\n\n\nYou can find an example below with the matlab syntax for writing Class-based unit tests:\nclassdef TestSumNumbers &lt; matlab.unittest.TestCase\n    methods (Test)\n        function testSumNumbers(testCase)\n            result = sumNumbers(2, 3);            \n            testCase.verifyEqual(result, 5)\n        end\n    end\nend\n Check out the MATLAB documentation for an introductory example: Write Simple Test Case Using Classes\n\n\n\n\n\n\nNoteAnnotated class-based unit test example\n\n\n\n\n\n% Test classes are created by inheriting (&lt; symbol) the  Matlab Testing \n% framework.\n%\n% e.g. classdef nameOfTest &lt; matlab.unittest.TestCase\n%      end\n\nclassdef (TestTags = {'Unit'}) test_example &lt; matlab.unittest.TestCase \n%                              It's convention to name the test file \n%                              test_\"filename being tested\".m\n%\n%         TestTags are an optional feature that are useful for identifying \n%         what kind of test you're coding, as you might only want to run \n%         certain tests that are related.\n\n    properties \n        % Class properties are not required, but are useful to contain \n        % common parameters between tests.\n    end\n    \n    methods (TestClassSetup) \n        % TestClassSetup methods are not required, but are usually used to\n        % setup common testing variables, or loading data. These methods\n        % are executed *prior to* the (Test) methods.\n    end\n    \n    methods (TestClassTeardown) \n        % TestClassTeardown methods are not required, but are useful to\n        % delete any files created during the test execution. These methods\n        % are executed *after* the (Test) methods.\n    end\n    \n    methods (Test) % Each test is it's own method function, and takes \n                   % testCase as their only argument.\n\n        function test_sumNumbers_returns_expected_value_for_integer_case(testCase) \n        % Use very descriptive test method names - this helps for debugging\n        % when error occurs.\n                        \n            % Call the function you'd like to test, e.g:\n%             actualValue = sumNumbers(2,2); % Test example integer case, 2+2\n            % Since the function sumNumbers is not defined, the test will\n            % fail. Instead, we will define the actual value.\n            actualValue = 4;\n\n\n            expectedValue = 4; % We know that we expect that 2+2 = 4\n\n            testCase.assertEqual(expectedValue, actualValue)\n            % Assert functions are the core of unit tests; if it fails,\n            % test log will return failed tests and details.\n            %\n            % They are called as methods of the testCase object.\n            %\n            % Example assert methods:\n            %\n            % assertEqual(expected, actual): Passes if the two input values\n            %                                are equal.\n            % assertTrue(boolValue): Passes if the value or statement is \n            %                        true (e.g. 2&gt;1)\n            % assertFalse(boolValue): Passes if the value or statement is\n            %                         false (e.g. 1==0)\n            %\n            % See Matlab's documentation for more assert methods: \n            % https://www.mathworks.com/help/matlab/ref/matlab.unittest.qualifications.assertable-class.html\n        end\n    end\n\nend",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html#executing-tests",
    "href": "docs/software/testing/matlab.html#executing-tests",
    "title": "Testing in MATLAB",
    "section": "Executing tests",
    "text": "Executing tests\n\n1. Running tests in the MATLAB Command Window\nYou can run tests through the MATLAB Command Window, by executing the following command in the root of your repository:\nresults = runtests(pwd, \"IncludeSubfolders\", true);\n\n% The argument `pwd` specifies the current working directory\n% `IncludeSubfolders` specifies whether to include subfolders in the search for tests\nMATLAB will automatically find all tests. If you make use of tags to categorize tests, you can run specific tags with:\nresults = runtests(pwd, \"IncludeSubfolders\", true, \"Tag\", '&lt;tag-name&gt;');\n For more details: runtests() documentation\n\nCustom testsuite script\nWe have a custom script available to run tests in a more structured way. It can be useful to:\n\nrun tests with specific tags\nignore specific tests\ngenerate various test reports\n\nWhen placed in the folder tests/, the script can be executed by running the following command in the MATLAB Command Window:\nrun_testsuite('TestTag', 'Unit')\n\n\n\n2. Script editor\nYou can run tests interactively by opening a test file in the MATLAB Editor and selecting ‚ÄúRun All‚Äù or ‚ÄúRun Current Test‚Äù.\n\n\n\nMathWorks, Script editor testing, MATLAB Documentation, link to image.\n\n\n For more details: Script Editor documentation\n\n\n3. MATLAB Test Browser App\nThe Test Browser app (available since R2023a) enables you to run script-based, function-based, and class-based tests interactively. You can run all tests, selected tests, or individual tests.\n\n\n\nMathWorks, MATLAB test browser, MATLAB Documentation, link to image.\n\n\n For more details: MATLAB Test browser documentation\n\n\nSimulink testing\nFor Simulink models, MATLAB provides Simulink Test for simulation-based testing.\n\nSimulink Test - Introduction video\nSimulink Test - Examples",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/index.html",
    "href": "docs/software/testing/index.html",
    "title": "Software testing",
    "section": "",
    "text": "When you‚Äôre writing software ‚Äì especially for research ‚Äì it‚Äôs important to make sure your programs work as expected. Testing is like a safety net: it helps you catch mistakes early and keeps your code reliable as you add new features or make changes. In research software, tests are even more crucial because they help ensure that your results are accurate and that others can reproduce your work. Beyond detecting bugs early, it is an investment in the quality and long-term maintainability of your codebase.\n\n\n\n Approach to testing\nA guide to help you get started with testing your software.\n\nLearn more ¬ª\n\n\n\n Test types\nDifferent types of tests to ensure your software works as expected.\n\nLearn more ¬ª\n\n\n\n Additional concepts\nMore concepts to help you write better tests.\n\nLearn more ¬ª\n\n\n\n Python testing\nTesting your Python code.\n\nLearn more ¬ª\n\n\n\n MATLAB testing\nTesting your MATLAB code.\n\nLearn more ¬ª\n\n\n\n R testing\nTesting your R code.\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\nTipRecommended courses\n\n\n\n\nThe Turing Way - Testing\nCodeRefinery - Lesson on testing\nSoftware Carpentry - Testing and Continuous Integration with Python\nR Packages - Testing",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html",
    "href": "docs/software/releases_archiving/releases/releases_cran.html",
    "title": "Release your R package",
    "section": "",
    "text": "After building and testing your R package, you can release it to CRAN. This process involves several steps to ensure that your package is ready for distribution. Unlike PyPI for Python, CRAN has specific requirements and guidelines that must be followed.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html#preparing-for-a-cran-submission",
    "href": "docs/software/releases_archiving/releases/releases_cran.html#preparing-for-a-cran-submission",
    "title": "Release your R package",
    "section": "Preparing for a CRAN submission",
    "text": "Preparing for a CRAN submission\nEnsure that your package is well-documented, tested, and follows CRAN policies. This includes having a DESCRIPTION file with all necessary fields filled out, a NAMESPACE file, and proper documentation for all functions. We already mentioned the use of devtools::check() and devtools::build() in the R packaging guide. These functions will help you prepare your package for submission. The package should pass all tests and checks without errors or warnings. The tarball created by devtools::build() will be used for submission.\nAdditionally, you would want to run devtools::check_rhub() to check your package on different platforms and R versions. The difference between devtools::check() and devtools::check_rhub() is that the former checks your package on your local machine, while the latter checks it on a remote server with different configurations.\nThis will help you identify any potential issues that may arise when users try to install your package on different systems. It can check your package on various platforms, including Windows, macOS, and different Linux distributions. It will also check for common issues that may arise when users try to install your package on different systems. While R-hub includes Windows builders, many developers specifically use devtools::check_win_devel() and devtools::check_win_release() before CRAN submission, as Windows compatibility issues are common.\nReview the CRAN policies and guidelines to ensure your package complies with their requirements. You can find the policies here.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html#submitting-to-cran",
    "href": "docs/software/releases_archiving/releases/releases_cran.html#submitting-to-cran",
    "title": "Release your R package",
    "section": "Submitting to CRAN",
    "text": "Submitting to CRAN\nEnsure that your package size is within the limits set by CRAN. You can check the size of your package using file.info() on the tarball created in the previous step. Then:\n\nCreate a CRAN account: If you don‚Äôt already have one, create an account on the CRAN website.\nSubmit your package: Use devtools::submit_cran() to submit your package to CRAN. This function will prompt you to enter your CRAN username and password, and it will automatically upload your package tarball to CRAN.\nFill out the submission form: After submitting your package, you will be directed to a web form where you can provide additional information about your package, such as a description, title, and any other relevant details.\nWait for CRAN review: After submitting your package, it will be reviewed by CRAN. This is an important distinction from PyPI, where packages are typically available immediately after submission. The CRAN team will check your package for compliance with their policies and guidelines.\n\nIf your package passes the review, it will be published on CRAN.\nIf there are issues, they will contact you for additional information or to request changes to your package.\n\nRespond to feedback: If the CRAN team requests changes or has questions about your package, make any necessary adjustments.\nRelease your package: Once your package has been approved, it will be available on CRAN for others to install and use.\n\n\nAdditional resources\nThe R packages book also has a detailed section on the CRAN submission process, including how to handle feedback from CRAN maintainers and how to respond to requests for changes.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nR packages book - CRAN submission\nCRAN policies\nCRAN submission process",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html",
    "title": "Creating an R package",
    "section": "",
    "text": "In this guide we will briefly touch the necessary steps to create an R package. For a more detailed guide, please refer to the excellent R packages book written by Hadley Wickham and Jennifer Bryan. The book is available online for free and is licensed under the CC BY-NC-ND 4.0 license. The book covers everything from the basics of package creation to advanced topics. It provides clear explanations, practical examples, and helpful tips to guide you through the process of building high-quality R packages.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#prerequisites",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#prerequisites",
    "title": "Creating an R package",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou will need the following packages installed to develop R packages:\ninstall.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"usethis\"))\n\n\n\n\n\n\nTip Tip\n\n\n\nPlease refer to our R code documentation guide on how to document your R code. The guide covers the use of roxygen2 for generating documentation, which is a key part of creating an R package.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#creating-the-package-structure",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#creating-the-package-structure",
    "title": "Creating an R package",
    "section": "Creating the package structure",
    "text": "Creating the package structure\nTo create an R package, we can use the devtools and usethis packages which include a variety of tools aimed at package development. The devtools package provides a set of functions that simplify the process of package development in R. The usethis package is another option for creating an R package structure. It provides a set of functions that help you create and manage R packages, including functions for creating package skeletons, adding dependencies, and managing package metadata. The two packages are often used together. For example, you can use usethis to create a package structure and then use devtools to add functions, tests, and documentation.\nEither way, you can create a package structure using the following command:\n# Using devtools\ndevtools::create(\"your_package_name\")\n# Using usethis\nusethis::create_package(\"your_package_name\")\nAlternatively, you can create a package structure using the RStudio IDE. This is a more user-friendly option for those who prefer a graphical interface. To create a package structure in RStudio, follow these steps:\n\nOpen RStudio and create a new project.\nSelect ‚ÄúNew Directory‚Äù and then ‚ÄúR Package‚Äù.\nChoose a name for your package and select the location where you want to create it.\nClick ‚ÄúCreate Project‚Äù to generate the package structure.\n\nEvery option will create a basic folder structure with essential files including DESCRIPTION, NAMESPACE, and R/ directory. The DESCRIPTION file contains metadata about your package, such as its name, version, author, and dependencies. The NAMESPACE file defines the functions and objects that will be exported from your package. The R/ directory is where you will write your package functions.\n\n\n\n\n\n\nTip Tip\n\n\n\nPlease visit our Project structure guide for a recommendation on how to structure your R project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#adding-functions-and-documentation",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#adding-functions-and-documentation",
    "title": "Creating an R package",
    "section": "Adding functions and documentation",
    "text": "Adding functions and documentation\nOnce you have the package structure for your R project, you will need to:\n\nEdit the DESCRIPTION file and fill in the package metadata, including the package name, version, author, licensing information and dependencies.\nAll your package functions should be placed in the R/ directory. You can create multiple R scripts in this directory to organize your functions logically.\nUse roxygen2 and devtools::document() to generate the documentation for your package based on the comments in your code. This will create .Rd files in the man/ directory.\nThe NAMESPACE file should be generated automatically by roxygen2 based on @export tags.\nUse devtools::build_vignettes() to build the package vignettes. This will create HTML files in the vignettes/ directory.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#testing-and-building-the-package",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#testing-and-building-the-package",
    "title": "Creating an R package",
    "section": "Testing and building the package",
    "text": "Testing and building the package\nOnce you have added your functions and documentation, you can test and build your package using the following steps:\n\nUse devtools::test() to run your package tests. This will run any tests you have written in the tests/ directory.\nTo load and test your package, use devtools::load_all(). This will load your package into the R session, allowing you to test it interactively.\nUse devtools::check() to check your package for any issues or errors. This will run a series of checks on your package and report any problems.\nUse devtools::build() to create a tarball of your package. This will create a .tar.gz file in the parent directory of your package.\nUse devtools::install() to install your package locally for testing. This will install the package from the tarball you just created.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nR packages book\nusethis package\ndevtools package",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#next-steps",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#next-steps",
    "title": "Creating an R package",
    "section": "Next steps",
    "text": "Next steps\nThe next step would be to publish your package. Visit our Release your R package guide for information on how to publish your package to CRAN.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging.html",
    "href": "docs/software/releases_archiving/packaging/packaging.html",
    "title": "Packaging",
    "section": "",
    "text": "Packaging allows developers to bundle their code into a format that is easily installable and manageable across different environments. By creating a package/library, you ensure that all necessary components, including dependencies and configuration files, are included in a single unit, simplifying the installation process for users. Here you can find how to package your Python or R projects for distribution.\n\n\n\n Create a Python package\nBundle your Python project.\n\nLearn more ¬ª\n\n\n\n Create an R package\nBundle your R project.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html",
    "href": "docs/software/releases_archiving/archiving.html",
    "title": "Archiving",
    "section": "",
    "text": "Archiving your software ensures long-term availability and accessibility. By archiving your software, you can preserve its state at a specific point in time, making it easier for others to access and use it in the future.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html#zenodo",
    "href": "docs/software/releases_archiving/archiving.html#zenodo",
    "title": "Archiving",
    "section": "Zenodo",
    "text": "Zenodo\nZenodo supports the archiving of research outputs, including software releases. Zenodo can automatically archive releases from GitHub repositories and assign a DOI, making each version citable.\nTo use Zenodo with GitHub:\n\nLink your GitHub account to Zenodo to allow access to repository information.\nEnable the repository you want to archive on the Zenodo dashboard.\nCreate a new release on GitHub. Zenodo will automatically archive this release and issue a DOI.\nYou can then share the DOI link provided by Zenodo in your project‚Äôs README or documentation, or paper, so a specific version of your software can be referenced.\n\n\n\n\n\n\n\nTip Tips\n\n\n\n\nZenodo can only access public repositories.\nIf you need to archive a repository from an organization, the owner of the organization might have to authorize the Zenodo application to access it.\nYou can also try out Zenodo Sandbox before archiving your projects to Zenodo. Zenodo Sandbox mimics the main Zenodo platform and is designed to test out the functionality of Zenodo without accidentally making mistakes with the real software/data and the main site. Since it‚Äôs an exact mirror of Zenodo it provides the same user experience and all the same tools and the same interface.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nReferencing and citing content",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html#tu.researchdata",
    "href": "docs/software/releases_archiving/archiving.html#tu.researchdata",
    "title": "Archiving",
    "section": "4TU.ResearchData",
    "text": "4TU.ResearchData\n4TU.ResearchData is another platform that offers reliable archiving of research data and software. Their servers are located in the Netherlands, and they are committed to long-term preservation. 4TU.ResearchData offers at least 15 years of archival storage.\nTo get started:\n\nLog in to your 4TU.ResearchData account (using institutional access).\nFrom the dashboard navigate to upload a new project.\nEither choose open access or you also have the option to choose embargoed or restricted access.\nUpload your relevant files. 4TU.ResearchData supports Git for version control. Either just drag and drop datasets, or to deposit software you can push your Git repository to the 4TU remote. Add the remote:\n\ngit remote add 4tu [link automatically generated by 4TU] Then, push your repository:\ngit push 4tu --all\ngit push 4tu --tags\n\nYou will have a DOI number reserved, and versioning is also supported.\nMaintain and update your archive as necessary to reflect any significant changes or additions to the software.\n\n\n\n\n\n\n\nTip Information\n\n\n\n\nWhen you are ready to publish your software on 4TU.ResearchData, ensure to choose the ‚ÄúSoftware deposit‚Äù option in the ‚ÄòFiles‚Äô section at the bottom of the upload form. This option allows you to upload your software files directly from your Git repository. If you have additional files, you can also manually drag your software files from your local drive into the upload box.\n4TU.ResearchData also has a sandbox available.\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nMore information on 4TU.ResearchData can be found in their FAQ.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/fair_software/software_management_plan.html",
    "href": "docs/software/fair_software/software_management_plan.html",
    "title": "Software management plan",
    "section": "",
    "text": "A software management plan (SMP) helps to implement best practices during software development and ensures that software is accessible and reusable in the short and longer term. The Netherlands eScience Center and NWO, the Dutch Research Council, have taken the initiative to develop (national) guidelines for software management plans, which resulted in the publication of the Practical guide to Software Management Plans in October 2022.\nAs an introduction to Software Management Plans, TU Delft has created a video:\n\n‚ÄúNavigating Research Data and Software: A Practical Guide for PhD Supervisors 2025‚Äù, TU Delft Library. CC-BY-4.0\n\n\n\n\n\n\nTip Tip\n\n\n\nThe Intermediate Research Software Development course from The Carpentries is a good resource to learn the core principle of intermediate-level software development and best practices collaborating in a team.\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nSoftware Development Plan: A quick start on building software development plans.\nSoftware Development Life Cycle: A guide on how to follow the software development lifecycle (SDLC) systematically, and different SDLC methodologies.\nWriting Software Management Plans: A guide from the Software Sustainability Institute on how to write research software management plans.\nSoftware Management Plans documentation: A detailed documentation about software management plans, including up-to-date templates.\nA Framework for Understanding Research Software: A proposal for categorizing different types of research software, and a framework for sustaining research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "Software management plan"
    ]
  },
  {
    "objectID": "docs/software/fair_software/checklist.html",
    "href": "docs/software/fair_software/checklist.html",
    "title": "FAIR checklist for research software",
    "section": "",
    "text": "This checklist provides a set of recommendations for FAIR software. It outlines best practices and guidelines to ensure the quality, reproducibility, and sustainability of software projects. The checklist covers various aspects, such as version control, documentation, testing, licensing, and collaboration, providing a comprehensive framework for improving your software development process.\nTo support implementation, each checklist section contains a tab with pre-formatted markdown templates (FAIR cards). These can be copied directly into your repository as issues to systematically track progress in adopting FAIR research software best practices.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "FAIR checklist for research software"
    ]
  },
  {
    "objectID": "docs/software/fair_software/checklist.html#checklist",
    "href": "docs/software/fair_software/checklist.html#checklist",
    "title": "FAIR checklist for research software",
    "section": "Checklist",
    "text": "Checklist\n\nVersion controlVersion control (.md format)\n\n\nEssential\n\nUse git as a version control system\nUpload your project on GitHub or TU Delft GitLab\n\nRecommended\n\nMake your repository public\nConsider your branch hygiene\nUse a branching model (e.g.¬†GitFlow)\nUse meaningful commit messages\n\n\n\n_Essential_\n- [ ] Use [git](https://www.atlassian.com/git) as a version control system \n- [ ] Upload your project on [GitHub](https://github.com/) or [TU Delft GitLab](https://gitlab.tudelft.nl/)\n\n_Recommended_  \n- [ ] Make your repository [public](https://coderefinery.github.io/social-coding/)\n- [ ] Consider your [branch hygiene](https://coderefinery.github.io/git-branch-design/)\n- [ ] Use a branching model (e.g. [GitFlow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow))\n- [ ] Use [meaningful commit messages](https://www.git-scm.com/book/en/v2/Distributed-Git-Contributing-to-a-Project#_commit_guidelines)\n\n\n\n\nCollaborationCollaboration (.md format)\n\n\nEssential\n\nMake use of GitHub issues\n\nRecommended\n\nContribution guidelines\nCode of conduct\n\n\n\n_Essential_  \n- [ ] Make use of [GitHub issues](https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues)\n\n_Recommended_\n- [ ] [Contribution guidelines](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/setting-guidelines-for-repository-contributors)\n- [ ] [Code of conduct](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-code-of-conduct-to-your-project)\n\n\n\n\nProject documentationProject documentation (.md format)\n\n\nEssential\n\nREADME\nApply a TU Delft pre-approved LICENSE\nCITATION\n\n\n\n_Essential_  \n- [ ] [README](https://www.makeareadme.com)\n- [ ] Apply a TU Delft pre-approved [LICENSE](https://zenodo.org/records/4629635)\n- [ ] [CITATION](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files)\n\n\n\n\nSoftware documentationSoftware documentation (.md format)\n\n\nEssential\n\nSource code documentation (docstrings)\nDocument your project dependencies\nInstallation instructions\nUser documentation\n\nRecommended\n\nDeveloper documentation and setup\nExamples and tutorials (e.g.¬†Jupyter Notebooks)\n\nOptional\n\nDocumentation tools (Sphinx, JupyterBook, Quarto)\nBuild an API reference from docstrings\nHosting (GitHub Pages, Readthedocs)\n\n\n\n_Essential_  \n- [ ] Source code documentation ([docstrings](https://numpydoc.readthedocs.io/en/latest/format.html))\n- [ ] Document your project dependencies\n- [ ] Installation instructions\n- [ ] User documentation\n\n_Recommended_  \n- [ ] Developer documentation and setup\n- [ ] Examples and tutorials (e.g. Jupyter Notebooks)\n\n_Optional_\n- [ ] Documentation tools ([Sphinx](https://coderefinery.github.io/documentation/sphinx/), [JupyterBook](https://jupyterbook.org/intro.html), [Quarto](https://quarto.org/docs/guide/))\n- [ ] Build an [API reference](https://developer.lsst.io/python/numpydoc.html) from docstrings\n- [ ] Hosting ([GitHub Pages](https://pages.github.com/), [Readthedocs](https://readthedocs.org/))\n\n\n\n\nSoftware testingSoftware testing (.md format)\n\n\nEssential\n\nInstallation/execution verification\n\nRecommended\n\nDefensive programming\nTest your software with integration tests and unit tests\nMake use of Continuous Integration to automate testing\n\nOptional\n\nCode coverage check (e.g.¬†Sonarcloud, codecov)\n\n\n\n_Essential_\n- [ ] Installation/execution verification\n\n_Recommended_\n- [ ] [Defensive programming](https://swcarpentry.github.io/python-novice-inflammation/10-defensive.html)\n- [ ] Test your software with [integration tests](https://the-turing-way.netlify.app/reproducible-research/testing/testing-integrationtest.html) and [unit tests](https://the-turing-way.netlify.app/reproducible-research/testing/testing-unittest.html)\n- [ ] Make use of [Continuous Integration](https://coderefinery.github.io/testing/continuous-integration/) to automate testing\n\n_Optional_\n- [ ] Code coverage check (e.g. [Sonarcloud](https://sonarcloud.io/), [codecov](https://about.codecov.io))\n\n\n\n\nSoftware qualitySoftware quality (.md format)\n\n\nEssential\n\nOrganize your project for reproducibility\nRecord and manage your software dependencies\n\nRecommended\n\nMake refactoring part of your workflow\nFollow best coding practices\n\nRecommended for Python\n\nFollow PEP8 guidelines\nUse a tool for dependency management (e.g.¬†poetry)\nUse linter (e.g.¬†pylint, flake8)\nUse a formatter (e.g.¬†black)\n\n\n\n_Essential_\n- [ ] [Organize](https://coderefinery.github.io/reproducible-research/organizing-projects/) your project for reproducibility\n- [ ] [Record and manage](https://coderefinery.github.io/reproducible-research/dependencies/) your software dependencies \n\n_Recommended_\n- [ ] Make [refactoring](https://refactoring.guru/refactoring) part of your workflow\n- [ ] Follow [best coding practices](https://alan-turing-institute.github.io/rse-course/html/module07_construction_and_design/index.html)\n\n_Recommended for Python_\n- [ ] Follow [PEP8 guidelines](https://realpython.com/python-pep8/)\n- [ ] Use a tool for dependency management (e.g. [poetry](https://python-poetry.org/docs/))\n- [ ] Use linter (e.g. [pylint](https://pypi.org/project/pylint/), [flake8](https://pypi.org/project/flake8/))\n- [ ] Use a formatter (e.g. [black](https://github.com/psf/black))\n\n\n\n\nReleasesReleases (.md format)\n\n\nEssential\n\nObtain a DOI (Zenodo or 4TU.ResearchData)\n\nRecommended\n\nUse semantic versioning\nCreate tagged releases (GitHub)\nCHANGELOG\nUpload to registry (e.g.¬†PyPI, conda)\nReleasing guide\n\nOptional\n\nContinuous Integration for automated build and release\n\n\n\n_Essential_  \n- [ ] Obtain a DOI ([Zenodo](https://zenodo.org/) or [4TU.ResearchData](https://data.4tu.nl/info/about-your-data/getting-started))\n\n_Recommended_  \n- [ ] Use [semantic versioning](https://semver.org/)\n- [ ] Create tagged releases ([GitHub](https://docs.github.com/en/repositories/releasing-projects-on-github))\n- [ ] [CHANGELOG](https://keepachangelog.com/en/1.0.0/)\n- [ ] Upload to [registry](https://github.com/NLeSC/awesome-research-software-registries) (e.g. [PyPI](https://realpython.com/pypi-publish-python-package/), [conda](https://conda.io/projects/conda-build/en/latest/user-guide/tutorials/build-pkgs.html))\n- [ ] [Releasing guide](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository)\n\n_Optional_\n- [ ] [Continuous Integration](https://the-turing-way.netlify.app/reproducible-research/ci/ci-options.html) for automated build and release\n\n\n\n\nExample repositories\n\neScience Center - matchms - Matchms is an open-source Python package to import, process, clean, and compare mass spectrometry data.\nTU Delft - Transposonmapper - Transposonmapper is an open-source python package and Docker image for mapping transposons from sequencing data.\n\n\nFor more information on the principles behind FAIR software, please have a look at the following resources:\n\nThe Turing Way - Guide for Reproducible Research - general guide to reproducible research\nTowards FAIR principles for research software - publication on the translation of FAIR principles for data to FAIR principles for software\nFrom FAIR research data toward FAIR and open research software\nFAIR Principles for Research Software\n\n\n\n\nAcknowledgements\nThe checklist was in part based on the checklist provided by the eScience Center, licensed under CC BY 4.0.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "FAIR checklist for research software"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html",
    "href": "docs/software/documentation/tooling.html",
    "title": "Tooling",
    "section": "",
    "text": "There are various tools available that can help you create, manage, and deploy project documentation more effectively.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#sphinx",
    "href": "docs/software/documentation/tooling.html#sphinx",
    "title": "Tooling",
    "section": "Sphinx",
    "text": "Sphinx\nSphinx is a versatile documentation tool that is well-suited for documenting Python projects due to its easy integration with Python‚Äôs docstrings. Its capabilities extend beyond Python, making it a great solution for creating comprehensive documentation for projects in various programming languages (e.g.¬†MATLAB).\nSome key features of Sphinx include:\n\nCross-referencing code and documentation across files.\nAutomatic generation of documentation from docstrings.\nSyntax highlighting for code examples.\nSupport for extensions and custom themes.\nMultiple output formats.\n\n\nGetting started with Sphinx\n\n\n\n\n\n\nTip Tip\n\n\n\nTo get started with Sphinx, we recommend the Coderefinery lesson on Sphinx and Markdown\n\n\n\nInstall dependency: You can install Sphinx in various ways, either through apt-get for Linux, Homebrew for macOS, or through Chocolatey for Windows. Assuming you have Python on your machine you can install it through conda or pip.\nSetup documentation: Create a directory for your documentation (/docs), and run sphinx-quickstart in that directory. The default answers to the questions are fine.\nConfigure Sphinx: Once you have the conf.py and index.rst files, you will need to modify them further. The index.rst file acts as the front page of your documentation and the root of the table of contents. The conf.py file is the main configuration file for the Sphinx documentation. It holds all your extensions and controls various aspects of the build process that can be customized to suit your needs. For example, sphinx.ext.autodoc is used for pulling documentation from docstrings, and sphinx.ext.mathjax for displaying mathematical content.\n\nBuilt-in extensions\nThird-party extensions\n\nWrite content: Add content to your documentation. In addition to reStructureText, Sphinx also integrates with markdown documentation through the MyST parser.\nBuild documentation: Once you have added the documentation files, you can build the documentation from the folder /docs with sphinx-build . _build/ or make html.\nFurther customization: You can customize the look of your documentation by changing themes in the conf.py file.\n\n\n\n\n\n\n\nTip Sphinx configuration template\n\n\n\n\n\n\n\n\n\nNoteconfig.py\n\n\n\n\n\n# Configuration file for the Sphinx documentation builder\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, as shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \"Project\"\ncopyright = \"year, name\"\nauthor = \"name\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# The full version, including alpha/beta/rc tags\nrelease = \"0.1.0\"\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"myst_parser\", # MyST markdown parser\n    \"sphinxcontrib.matlab\", # Required for MATLAB\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx_copybutton\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_tabs.tabs\"\n]\n\nmyst_enable_extensions = [\n    \"linkify\",\n]\n\n# MATLAB settings for autodoc\n# here = os.path.dirname(os.path.abspath(__file__))\n# matlab_src_dir = os.path.abspath(os.path.join(here, \"..\"))\n# primary_domain = \"mat\"\n\n# Napoleon settings\nnapoleon_google_docstring = True\n# napoleon_numpy_docstring = True\n# napoleon_use_param = False\n# napoleon_preprocess_types = True\n\n# This value contains a list of modules to be mocked up.\n# autodoc_mock_imports = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\n# html_theme = \"sphinx_book_theme\"\nhtml_theme = \"sphinx_rtd_theme\"\n# html_theme = \"pydata_sphinx_theme\"\n\n\nhtml_title = \"title\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n\n When using Sphinx extensions or custom themes beyond core functionality, a requirements.txt file is mandatory for reproducible documentation builds across environments. For example, this is required for hosting platforms like Read the Docs.\n\n\n\n\n\n\nNoterequirements.txt\n\n\n\n\n\n# Core\nsphinx\n\n# Sphinx extensions\nsphinxcontrib-matlabdomain # Only for matlab source code\nsphinx-tabs\nsphinx-copybutton\n\n# MyST parser\nmyst-parser\nlinkify-it-py\n\n# Themes\npydata-sphinx-theme\nsphinx-book-theme\nsphinx-rtd-theme\n\n\n\n\n\n\n\n\n\n\n\nTip Example repositories using Sphinx for Python:\n\n\n\n\nPython‚Äôs official documentation is created using Sphinx\nRead The Docs - the platform for hosting documentation is itself documented using Sphinx.\nNumPy\n\n\n\n\n\nSphinx autodoc\nOnce the Sphinx config.py is set up, you can generate the API reference documentation by using the sphinx-autodoc extension. By creating .rst files with the autodoc syntax, Sphinx will build the API reference.\n\n\nSphinx-matlabdomain\nFor documenting MATLAB projects, Sphinx can be extended for MATLAB. The sphinxcontrib-matlabdomain extension allows Sphinx to interpret and render MATLAB specific documentation. The extension can be installed through pip install sphinxcontrib-matlabdomain and add the extension to the conf.py file.\n\n\n\n\n\n\nTip Example repositories using sphinx for MATLAB:\n\n\n\n\nENIGMA Toolbox - provides documentation in both Python and MATLAB, generated by Sphinx and hosted using Read the Docs.\nCobra Toolbox\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCoderefinery lesson on Sphinx and Markdown\nGetting started with Sphinx",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#jupyter-book",
    "href": "docs/software/documentation/tooling.html#jupyter-book",
    "title": "Tooling",
    "section": "Jupyter Book",
    "text": "Jupyter Book\nJupyter Book uses Sphinx to convert notebooks and Markdown documents into an interactive publishing framework (with executable content). It integrates Jupyter Notebooks with Sphinx‚Äôs documentation capabilities, enabling features like cell execution and output caching directly within the documentation. Jupyter Book is essentially a specialized wrapper around Sphinx and the MyST-NB extension, designed to make publishing content easier.\n\n\n\n\n\n\n The TU Delft OPEN Interactive Textbooks platform uses Jupyter Book to create textbooks.\n\n\n\n\nFeatures\n\nJupyter Book can integrate outputs by allowing code execution within the content, making it ideal for tutorials, courses, and technical documentation that require live examples.\nJupyter Book uses Markdown for Jupyter (MyST) which extends the traditional Markdown syntax to include features normally available in reStructuredText (reST). This makes it easier to include complex formatting and dynamic content directly in Markdown files.\nJupyter Book can execute notebook cells and cache outputs. This means that content including code outputs can be generated once and reused.\n\n\n\nGetting started\nJupyterBook has extensive documentation on getting started with building a book.\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nHow Jupyter Book and Sphinx relate to one another",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#mkdocs",
    "href": "docs/software/documentation/tooling.html#mkdocs",
    "title": "Tooling",
    "section": "MkDocs",
    "text": "MkDocs\nMkDocs is a static site generator that uses markdown for all documentation, simplifying the writing process, and is configured with a single YAML file. It is lightweight compared to Sphinx but less feature-rich for complex use cases, and is best suitable for straightforward project documentation without heavy API generation needs.\n\nGetting started\n\nYou can install it through pip (pip install mkdocs). Then you can initialize your MkDocs project by running mkdocs new your_project_name.\nPlace your markdown documentation in your docs directory and define the structure in your mkdocs.yml file.\nYou can preview your site locally and see live updates as you make changes by running mkdocs serve.\nWhen you want to publish your documentation run mkdocs build.\nMkDocs is designed to be hosted on almost any static file server and works well with GitHub Pages.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nMkDocs official site that includes a Getting Started and User Guide.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#quarto",
    "href": "docs/software/documentation/tooling.html#quarto",
    "title": "Tooling",
    "section": "Quarto",
    "text": "Quarto\nSimilar to Jupyter Book, Quarto is a publishing framework that allows you to create dynamic documents, presentations, reports, websites, and more. It supports multiple programming languages, including Python, R, and Julia, enabling the inclusion of executable code, interactive visualizations, equations, and rich formatting directly within the documents.\n\n\n\n\n\n\n‚Æï All of these guides are created with Quarto!\n\n\n\n\nGetting started\n\nDownloading: You can download the installer for your operating system from the Quarto website.\nRunning Quarto: You can run Quarto either from your command line or from VS Code, JupyterLab, RStudio, or any text editor. For VS Code you will need to install a Quarto extension. It is a stand-alone application and does not require Python.\nMarkdown flavour: Quarto projects use .qmd files which are a Markdown flavour.\n\n\n\n\n\n\n\nNote Basic structure of a Quarto file\n\n\n\n\n\n    ---\n    title: \"Your Document Title\"\n    format: html # Or pdf, word, etc.\n    ---\n\n    # Introduction\n\n    Some text...\n\n    ## Section 1\n\n    Some text...\n\n    ```python\n    # This is a code block\n    import pandas as pd\n    data = pd.read_csv(\"data.csv\")\n    print(data.head())\n    ```\n\n    ## Section 2\n\n    Some more text....\n\n\n\n\n\nAdding content: Write your text using standard Markdown syntax and add code blocks.\nBuilding documentation:\n\nTo compile a Quarto document, use quarto render your-file-name.qmd. This command converts your .qmd file into the output format specified in the file‚Äôs header (e.g., HTML, PDF).\nYou can watch a file or directory for changes and automatically re-render with quarto preview your-file-name.qmd, which is useful to see live updates.\n\nAdditional features:\n\nQuarto supports cross-referencing figures, tables, and other elements within your document. You can also use BibTeX for citations.\nYou can have interactive components for web outputs (e.g.¬†embeded Plotly charts).\nExtensive options for custom styles and layouts.\n\nPublishing: Quarto documents are portable and can be shared as is, allowing others to compile them on their own systems or published by hosting the output files on a server like GitHub Pages.\n\n\n\n\n\n\n\nTip Examples\n\n\n\n\nQuarto gallery\n\n\n\n\n\n\n\n\n\nNote PDF engine\n\n\n\nIn order to create PDFs you will need to install a LaTeX engine if you do not have one installed already. You could use a lightweight distribution like TinyTeX, which you can install with quarto install tool tinytex.\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGetting started with Quarto\nComprehensive guide to using Quarto\nCarpentries Incubator - Introduction to Working with Quarto documents",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#tools-for-r",
    "href": "docs/software/documentation/tooling.html#tools-for-r",
    "title": "Tooling",
    "section": "Tools for R",
    "text": "Tools for R\nR project documentation generally includes in-line comments and function documentation using roxygen2. Additionally, comprehensive examples and usage guides are often provided through vignettes, which are included within an R package itself.\nTo extend roxygen2 documentation into a static website for your package you can use pkgdown. pkgdown automatically generates a website from your package‚Äôs documentation and vignettes, similar to how Sphinx is used for Python projects.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\npkgdown",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/index.html",
    "href": "docs/software/documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation serves as a bridge between the developer and user, and effectively communicating and explaining the code is as important as the code itself. Often, two types of documentation are distinguished - user and developer documentation. Both are essential for the success of a software project, and they serve different purposes.\n\nUser documentation\nUser documentation is aimed at those who will use the software. This documentation typically includes user manuals and tutorials, possibly FAQs and troubleshooting guides. The focus is on simplicity and accessibility, ensuring that anyone can understand how to use the software.\n\n\n\n\n README\nHow to write a good README.\n\nLearn more ¬ª\n\n\n\n Licenses\nApply an open-source license.\n\nLearn more ¬ª\n\n\n\n CITATION\nCite your software.\n\nLearn more ¬ª\n\n\n\n\n\nDeveloper documentation\nDeveloper documentation targets developers who need to understand the internal parts of the software for purposes of development, maintenance, or integration. It can include additional details such as API documentation and development guidelines. Developer documentation is more detailed providing insights necessary for modifying and enhancing the software.\n\n\n\n\n Code Documentation\nDocumenting your codebase.\n\nLearn more ¬ª\n\n\n\n Tooling\nDeploy your documentation.\n\nLearn more ¬ª\n\n\n\n Hosting\nHost your documentation.\n\nLearn more ¬ª\n\n\n\n Contributing Guidelines\nDefine how to contribute to your project.\n\nLearn more ¬ª\n\n\n\n Code of Conduct\nSet expectations for respectful, inclusive collaboration among contributors.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/software/documentation/contributing_guidelines.html",
    "href": "docs/software/documentation/contributing_guidelines.html",
    "title": "Contributing guidelines",
    "section": "",
    "text": "A well-maintained README provides an overview of your project‚Äôs current state, while a CONTRIBUTING guide encourages user/developer involvement. Together, these documents help maintain project clarity and make it easier to manage contributions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Contributing guidelines"
    ]
  },
  {
    "objectID": "docs/software/documentation/contributing_guidelines.html#setting-up-a-contributing-guide",
    "href": "docs/software/documentation/contributing_guidelines.html#setting-up-a-contributing-guide",
    "title": "Contributing guidelines",
    "section": "Setting up a CONTRIBUTING guide",
    "text": "Setting up a CONTRIBUTING guide\nThere are no strict guidelines for a contributing guide and the content will depend on the project size, the number of collaborators, and your particular workflow. Consider including:\n\nIntroduction: Welcome the contributors and express appreciation for community contributions.\nAdd a code of conduct: This helps to maintain a respectful and inclusive environment.\nHow to contribute: Explain precise contribution guidelines\n\nIssue tracking: Explain how to report issues (bugs, feature requests, etc.).\nPull requests: Detail the process for submitting pull requests. This includes instructions on forking the repository, creating a branch, making changes, and the follow-up steps for a successful pull request.\nCode review process: Describe how contributions will be reviewed and integrated.\n\nCommunity and communication: List the channels through which contributors can communicate and set their expectations regarding the responsiveness and availability of project maintainers.\nStyle guide and coding standards: Providing a (separate) coding style guide or documenting coding standards would be best practice. This way contributors would ensure consistency across the codebase.\nLegal implications: Inform contributors about the licensing under which their contributions will be used and any intellectual property considerations they should be aware of.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGitHub‚Äôs guide to setting guidelines for repository contributors\nGitHub‚Äôs own CONTRIBUTING guide",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Contributing guidelines"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html",
    "href": "docs/software/documentation/code_documentation/r_documentation.html",
    "title": "R documentation",
    "section": "",
    "text": "The standard approach for documenting R projects is using roxygen2, a package that enables inline documentation for R scripts and packages. It is tightly integrated with the R ecosystem, making it straightforward to generate user-friendly documentation in the form of help files (.Rd files). roxygen2 allows you to write documentation alongside your code as specially formatted comments, which are then parsed and converted into the appropriate documentation format as .Rd files.\nKey features of roxygen2:\n\nInline documentation: Document functions, arguments, and return values directly in the script.\nIntegration with R‚Äôs help option: Generates .Rd files, which are then converted into the help pages users can access with ?function_name.\nCross-referencing: You can easily link to other functions within the documentation.\nNamespace management: Automatically manages the NAMESPACE file for proper imports and exports. A NAMESPACE file is typical for CRAN submissions.\n\n\n\n\nInstall with install.packages(\"roxygen2\").\nEnsure you are in the correct project directory and it has the standard R package structure.\nWriting documentation: Documentation is written as comments starting with #' directly above your function definitions. roxygen2 processes this and stores .Rd files in the man/ directory. Common tags include:\n\n@param: Describes function arguments.\n@return: Describes the return value.\n@examples: Provides example usage.\n@import: For importing functions from other packages.\n@inheritParams: To inherit parameter descriptions from another documented function.\n@export: Makes the function available to package users.\n@seealso : For cross-referencing.\n\nGenerating documentation: Run roxygenise() to convert your documentation into .Rd files. You can also use devtools::document()since it is a wrapper around roxygenise().\nWhen packaging your R project, you can use devtools::check() to ensure your documentation is consistent with your function definitions.\n\n\n\n\n\n\n\nTip roxygen2 example\n\n\n\n\n\n#' Summarize a numeric vector\n#'\n#' This function calculates the mean, median, and standard deviation of a given \n#' numeric vector and returns the results in a data frame.\n#'\n#' @param x A numeric vector.\n#' @return A data frame with the following columns:\n#' \\describe{\n#'   \\item{mean}{The mean of the numeric vector.}\n#'   \\item{median}{The median of the numeric vector.}\n#'   \\item{sd}{The standard deviation of the numeric vector.}\n#' }\n#' @details This function provides a quick summary for a numeric vector, returning \n#' measures of central tendency (mean and median) and a measure of \n#' dispersion (standard deviation) using R‚Äôs base functions mean(), median(), and sd().\n#' @examples\n#' # Basic example\n#' summarize_vector(c(1, 2, 3, 4, 5))\n#' \n#' @export\nsummarize_vector &lt;- function(x) {\n  if (!is.numeric(x)) stop(\"Input must be a numeric vector.\")\n  \n  mean_val &lt;- mean(x)\n  median_val &lt;- median(x)\n  sd_val &lt;- sd(x)\n  \n  return(data.frame(mean = mean_val, median = median_val, sd = sd_val))\n}\n\n\n\n\n\n\n\n\n\nTip Example repositories using roxygen2:\n\n\n\n\nggplot2 on GitHub\ndplyr on GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html#roxygen2",
    "href": "docs/software/documentation/code_documentation/r_documentation.html#roxygen2",
    "title": "R documentation",
    "section": "",
    "text": "The standard approach for documenting R projects is using roxygen2, a package that enables inline documentation for R scripts and packages. It is tightly integrated with the R ecosystem, making it straightforward to generate user-friendly documentation in the form of help files (.Rd files). roxygen2 allows you to write documentation alongside your code as specially formatted comments, which are then parsed and converted into the appropriate documentation format as .Rd files.\nKey features of roxygen2:\n\nInline documentation: Document functions, arguments, and return values directly in the script.\nIntegration with R‚Äôs help option: Generates .Rd files, which are then converted into the help pages users can access with ?function_name.\nCross-referencing: You can easily link to other functions within the documentation.\nNamespace management: Automatically manages the NAMESPACE file for proper imports and exports. A NAMESPACE file is typical for CRAN submissions.\n\n\n\n\nInstall with install.packages(\"roxygen2\").\nEnsure you are in the correct project directory and it has the standard R package structure.\nWriting documentation: Documentation is written as comments starting with #' directly above your function definitions. roxygen2 processes this and stores .Rd files in the man/ directory. Common tags include:\n\n@param: Describes function arguments.\n@return: Describes the return value.\n@examples: Provides example usage.\n@import: For importing functions from other packages.\n@inheritParams: To inherit parameter descriptions from another documented function.\n@export: Makes the function available to package users.\n@seealso : For cross-referencing.\n\nGenerating documentation: Run roxygenise() to convert your documentation into .Rd files. You can also use devtools::document()since it is a wrapper around roxygenise().\nWhen packaging your R project, you can use devtools::check() to ensure your documentation is consistent with your function definitions.\n\n\n\n\n\n\n\nTip roxygen2 example\n\n\n\n\n\n#' Summarize a numeric vector\n#'\n#' This function calculates the mean, median, and standard deviation of a given \n#' numeric vector and returns the results in a data frame.\n#'\n#' @param x A numeric vector.\n#' @return A data frame with the following columns:\n#' \\describe{\n#'   \\item{mean}{The mean of the numeric vector.}\n#'   \\item{median}{The median of the numeric vector.}\n#'   \\item{sd}{The standard deviation of the numeric vector.}\n#' }\n#' @details This function provides a quick summary for a numeric vector, returning \n#' measures of central tendency (mean and median) and a measure of \n#' dispersion (standard deviation) using R‚Äôs base functions mean(), median(), and sd().\n#' @examples\n#' # Basic example\n#' summarize_vector(c(1, 2, 3, 4, 5))\n#' \n#' @export\nsummarize_vector &lt;- function(x) {\n  if (!is.numeric(x)) stop(\"Input must be a numeric vector.\")\n  \n  mean_val &lt;- mean(x)\n  median_val &lt;- median(x)\n  sd_val &lt;- sd(x)\n  \n  return(data.frame(mean = mean_val, median = median_val, sd = sd_val))\n}\n\n\n\n\n\n\n\n\n\nTip Example repositories using roxygen2:\n\n\n\n\nggplot2 on GitHub\ndplyr on GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html#vignettes",
    "href": "docs/software/documentation/code_documentation/r_documentation.html#vignettes",
    "title": "R documentation",
    "section": "Vignettes",
    "text": "Vignettes\nVignettes are detailed guides or tutorials included with R packages, providing users with in-depth explanations and examples, complementing the shorter help files. They are useful for demonstrating package functionality and use cases. Vignettes are automatically compiled and included in the package documentation.\nYou can create a vignette by running:\nusethis::use_vignette(\"your_vignette_name\")\nThis creates a template in the vignettes/ directory with the necessary YAML header and structure. You can combine text, code chunks, and outputs using standard R Markdown syntax. Include explanations, usage examples, and visualizations to enhance clarity.\nOnce written, build the vignette using:\ndevtools::build_vignettes()\nThis generates HTML or PDF versions of the vignette, which are then included in the package documentation. Use devtools::install(build_vignettes = TRUE) to test your package and built vignettes together (see how a user would experience them).\n\n\n\n\n\n\nNote Learn More\n\n\n\n\nExplore vignette(\"roxygen2\") in R\nroxygen2 documentation\nFunction documentation from R Packages\nWriting vignettes\nCRAN Submission guidelines",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/matlab_documentation.html",
    "href": "docs/software/documentation/code_documentation/matlab_documentation.html",
    "title": "MATLAB documentation",
    "section": "",
    "text": "üöß Coming soon! ‚è≥",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "MATLAB projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/citation.html",
    "href": "docs/software/documentation/citation.html",
    "title": "CITATION.cff",
    "section": "",
    "text": "It‚Äôs straightforward to cite research papers, but with software sometimes it‚Äôs not as obvious. It is recommended to place a CITATION.cff file in the root of your repository to inform others about the preferred way to cite the software. GitHub can automatically parse the .cff file to create citation snippets in APA or BibTeX format. If you‚Äôd prefer the software to be cited through a journal publication, you can mention this in the README and in the CITATION.cff file.\n\n\n\n\n\n\nTip An example of a CITATION.cff\n\n\n\n\n\ncff-version: 1.2.0\nmessage: \"If you are using this software, please cite it as shown below.\"\nauthors:\n- family-names: \"Doe\"\n  given-names: \"Jane\"\n  orcid: \"https://orcid.org/9999-9999-9999-9999\"\ntitle: \"Name of your software\"\nversion: 1.0.1\ndoi: \"11.1111/11111\"\ndate-released: 2024-12-31\nlicense: MIT\nurl: \"https://github.com/your_repo\"\n\n\n\nWhen citing a paper that is linked to the software you can use preferred-citation argument.\n\n\n\n\n\n\nTip An example of a CITATION file citing a research article\n\n\n\n\n\ncff-version: 1.2.0\nmessage: \"If you are using this software, please cite it as shown below.\"\nauthors:\n- family-names: \"Doe\"\n  given-names: \"Jane\"\n  orcid: \"https://orcid.org/9999-9999-9999-9999\"\ntitle: \"Name of your software\"\nversion: 1.0.1\ndoi: \"11.1111/11111\"\ndate-released: 2024-12-31\nlicense: MIT\nurl: \"https://github.com/your_repo\"\npreferred-citation:\n    type: article\n    authors:\n    - family-names: \"Doe\"\n      given-names: \"Jane\"\n      orcid: \"https://orcid.org/9999-9999-9999-9999\"\n    doi: \"11.1111/11111\"\n    journal: \"The title of the journal\"\n    month: 12\n    start: 19 # the first page number\n    end: 29 #the last page number\n    title: \"Name of your submitted paper\"\n    issue: 9\n    volume: 2\n    year: 2024\n    \n\n\n\n\n\n\n\n\n\nTip How the citation would look on GitHub\n\n\n\n\n\nOn GitHub, it will show in either APA or BibTeX formatting, as they are the currently supported formats. If you add a CITATION.cff file to your repository, then a label for citing will automatically be generated and will show up on the right sidebar of the repository.\nAPA\n\nDoe, J. (2024). Name of your software (Version 1.0.1) [Computer software]. https://doi.org/11.1111/11111\n\nBibTeX\n\n@software{Joe_Name_of_your_software_2024, author = {Doe, Jane}, doi = {11.1111/11111}, month = {12}, title = {{Name of your software}}, url = {https://github.com/your_repo}, version = {1.0.1}, year = {2024} }\n\nThis is an example of software citation.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCITATION.cff documentation\nGitHub documentation on CITATION files - this resource also includes how to cite something other than software or a journal article.\nGenerate CITATION.cff files\nCitation File Format GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "CITATION"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html",
    "href": "docs/software/development_workflow/reusing_projects.html",
    "title": "Project templates and reusability",
    "section": "",
    "text": "Templates can help you to standardize your software development process.\n\n\nYou can turn an existing repository into a template, so you and others can generate new repositories with the same directory structure, branches, and files. Note, the template repository cannot include files stored using Git LFS.\n\n\n\n\n\n\nTip Repository templates\n\n\n\n\nCreating a template repository\nRepository template example to make your code more compliant with FAIR principles\n\n\n\n\n\n\nCookiecutter creates Python projects from project templates. The advantage of using Cookiecutter is that new projects are set up quickly from a standardized template structure and can include everything needed to get started on a project, such as directory layouts, sample code, and even integrations with tools and services.\n\n\n\n\n\n\nTip Tutorials\n\n\n\n\nTutorial for Cookiecutter\nFor installation instructions, have a look at Cookiecutter installation instructions.\n\n\n\n\n\n\n\n\n\nTip Cookiecutter templates\n\n\n\n\nCookiecutter templates on GitHub\nCookiecutter PyPackage: template for distributing Python libraries.\n\nGitHub - cookiecutter-pypackage\nGitHub - Netherlands eScience Center template\n\nCookiecutter Machine Learning: template for machine learning projects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html#project-templates",
    "href": "docs/software/development_workflow/reusing_projects.html#project-templates",
    "title": "Project templates and reusability",
    "section": "",
    "text": "Templates can help you to standardize your software development process.\n\n\nYou can turn an existing repository into a template, so you and others can generate new repositories with the same directory structure, branches, and files. Note, the template repository cannot include files stored using Git LFS.\n\n\n\n\n\n\nTip Repository templates\n\n\n\n\nCreating a template repository\nRepository template example to make your code more compliant with FAIR principles\n\n\n\n\n\n\nCookiecutter creates Python projects from project templates. The advantage of using Cookiecutter is that new projects are set up quickly from a standardized template structure and can include everything needed to get started on a project, such as directory layouts, sample code, and even integrations with tools and services.\n\n\n\n\n\n\nTip Tutorials\n\n\n\n\nTutorial for Cookiecutter\nFor installation instructions, have a look at Cookiecutter installation instructions.\n\n\n\n\n\n\n\n\n\nTip Cookiecutter templates\n\n\n\n\nCookiecutter templates on GitHub\nCookiecutter PyPackage: template for distributing Python libraries.\n\nGitHub - cookiecutter-pypackage\nGitHub - Netherlands eScience Center template\n\nCookiecutter Machine Learning: template for machine learning projects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html#reusing-projects-and-repositories",
    "href": "docs/software/development_workflow/reusing_projects.html#reusing-projects-and-repositories",
    "title": "Project templates and reusability",
    "section": "Reusing projects and repositories",
    "text": "Reusing projects and repositories\nOne of the easiest ways to reuse code across projects is by packaging it into an installable library that can be used as a dependency. Alternatively, you can integrate external code into your project using Git submodules or Git subtree.\n\n\n\n\n\n\nWarningPractices to avoid\n\n\n\n\nStoring commonly-used folders in a separate folder on your system and adding the folder to the PATH. Other users/developers will not have access to these folders.\nDirect copy-and-pasting of code as you lose any upstream changes to the external repository.\n\n\n\n\nWhat‚Äôs the Difference?\n\n\n\n\n\n\n\n\nFeature\nGit Submodules\nGit Subtree\n\n\n\n\nHow it works\nAdds an external repository inside your project as a separate Git reference.\nMerges an external repository‚Äôs contents into your project‚Äôs directory structure.\n\n\nVersion Control\nTracks a specific commit of the external repository (not automatically updated).\nThe external repository‚Äôs commits are fully merged into your project‚Äôs commit history.\n\n\nUpdating\nRequires running git submodule update --remote to pull new changes.\nUpdates by merging changes from the external repository into your project.\n\n\nIdeal For\nKeeping external code separate while still using it in your project.\nFully integrating external code while keeping its history.\n\n\n\nIn short:\n\nUse Git submodules when you want to include an external repository but keep it separate, track its exact version, and update it manually.\nUse Git subtree if you want to fully integrate an external repository‚Äôs code into your project while keeping its commit history.\n\n\n\nGit submodules\nA Git submodule allows you to add a separate Git repository inside another repository as a subdirectory. It is a record that points to a specific commit in another external repository. Submodules are useful for incorporating external code or libraries into your project while keeping them separate and easily updatable.\n\n\n\n\n\n\nTipGit submodule commands\n\n\n\n\n\n\nAdding a submodule\nTo add an external repository as a submodule inside your project, use:\ngit submodule add &lt;repo-url&gt;\n\n\nCloning a repository with submodules\nWhen cloning a repository that contains submodules, follow these steps: 1. Clone the repository:\ngit clone &lt;repo-url&gt;\n\nInitialize the submodules:\n\ngit submodule init\n\nFetch the submodule content:\n\ngit submodule update\nAlternatively, you can use the shorthand command to clone and initialize submodules:\ngit clone --recurse-submodules &lt;repo-url&gt;\n\n\nUpdating submodules\nTo update the submodules to the latest commit, use:\ngit submodule update --remote\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nSimplified Git submodules tutorial\nGuide on Git submodules - comprehensive guide that covers everything from the basics to advanced workflows\n\n\n\n\n\n\n\n\n\nWarningIf you are using GitHub Desktop\n\n\n\n\n\nIf you are using GitHub Desktop, be aware that there can be limitations when working with submodules. While GitHub Desktop supports basic submodule functionality, some operations may require using the command line. Known issues include\n\ndifficulties in initializing submodules\nswitching branches with submodules\nvisualizing submodule changes.\n\nFor more details, check out this discussion or visit the GitHub Desktop issue tracker.\n\n\n\n\n\nGit subtree\nUnlike Git submodules, Git subtree merges the history of one repository into another as a subdirectory. This makes the external repository‚Äôs files appear as if they are part of your project while still allowing updates.\nFor more details, check out this Git subtree guide.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html",
    "href": "docs/software/development_workflow/project_management.html",
    "title": "Project management",
    "section": "",
    "text": "Git is a distributed version control system that enables you to track changes in your code over time. Platforms like GitHub, GitLab and Bitbucket extend the features of git by providing a centralized location for storing repositories, collaborating, and providing powerful tools to plan, organize, and track your work efficiently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#version-control-platforms",
    "href": "docs/software/development_workflow/project_management.html#version-control-platforms",
    "title": "Project management",
    "section": "Version control platforms",
    "text": "Version control platforms\nThe choice between GitHub, GitLab and Bitbucket depends on your required features, privacy and other preferences, but all are Git-based platforms for version control. While numerous detailed comparisons exist online, here we will focus on GitHub.\n\n\n\n\n\n\nWarning TU Delft GitLab\n\n\n\nTU Delft has its own GitLab instance hosted on campus. For more information, please visit the help page documentation.\n Choosing a Repository Manager - for TU Delft Researchers\n\n\nSimilarly, whether you are using a version control system through your terminal or Integrated Development Environment (IDE), or using a GUI like GitHub Desktop, the core functionality remains the same.\n\n\n\n\n\n\nTip Learn more about GitHub Desktop\n\n\n\n\n\nGitHub Desktop is a great choice if you are just starting out with version control, providing a user-friendly graphical interface that simplifies Git operations. It makes it easy to visualize changes, create branches, and manage pull requests without needing to use command-line Git commands.\n Getting started with GitHub Desktop",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#github-issues",
    "href": "docs/software/development_workflow/project_management.html#github-issues",
    "title": "Project management",
    "section": "GitHub issues",
    "text": "GitHub issues\nGitHub issues help you keep track of tasks, bugs, feature ideas in your project. They are like a to-do list items that everyone on your team can see and update.\nHow to use issues effectively:\n\nUse descriptive titles: Write short, specific titles that make it easy to understand what the issue is about.\nProvide detailed descriptions: Include all relevant information, steps to reproduce (if reporting a bug), and expected outcomes in the issue description. This ensures that anyone working on the issue has all the necessary context.\nUse labels: Labels act like tags to help you organize and prioritize tasks.\nAssign people: By assigning someone (or yourself) you let others know that you are picking up and working on this issue.\nLink related issues: Connect related work by linking issues to provide context (add #issue_number to reference an issue).\n\nGitHub Issues make it easier to manage your project, collaborate with others, and keep track of progress. As your project grows, you can use additional tools like milestones and project boards while still benefiting from well-organized issues.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nQuickstart for GitHub Issues\nMastering GitHub Issues",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#project-boards",
    "href": "docs/software/development_workflow/project_management.html#project-boards",
    "title": "Project management",
    "section": "Project boards",
    "text": "Project boards\nProject boards on GitHub are designed for planning, organizing, and tracking work within a project. They serve as visual management interfaces that integrate directly with GitHub issues and pull requests. Project boards can be configured as Kanban boards, tables, or roadmaps, offering various layouts to suit different project management needs. Project boards can be particularly useful for visualizing the overall progress of your project and identifying bottlenecks in your workflow. They provide a high-level view that complements the detailed tracking offered by issues.\n\nMilestones\nUsing milestones you can break down large projects into smaller, more manageable parts. While project boards offer a visual, dynamic interface to manage and track your tasks, milestones serve as structured markers that help you monitor progress toward key project phases/goals.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nExample project board - TU Delft Astrodynamics Toolkit (Tudat)\nDefine a milestone\nGitHub Guides for Issues and Projects",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#managing-projects-on-github",
    "href": "docs/software/development_workflow/project_management.html#managing-projects-on-github",
    "title": "Project management",
    "section": "Managing projects on GitHub",
    "text": "Managing projects on GitHub\nWhen managing your project on GitHub, we recommend two approaches:\n\na simplified approach using only issues, or\na more structured approach using milestones and project boards.\n\nBoth methods have advantages, and the choice depends on the size of the project, its complexity, and your preferences.\n\nSimplified approachStructured approach\n\n\nYou can use a simplified approach and just track your progress in the form of issues and work without defining milestones or using a project board. This can be particularly useful for smaller projects or when you are just starting out with GitHub.\nIn this approach, you would:\n\nCreate issues for each task or feature you need to work on.\nOptionally, use labels to categorize and prioritize your issues.\nAssign issues to yourself or team members/collaborators.\nUse comments to update progress and discuss any challenges.\nClose issues as you complete them.\n\nThis method allows for a flexible workflow while still maintaining a good level of organization and transparency in your project. If your project grows or becomes more complex, you can always adopt milestones and project boards for more structured project management.\n\n\nWhen working on a big project, it‚Äôs helpful to create a roadmap - a simple plan that outlines what needs to be done and when. A roadmap gives you and your team a clear view of what‚Äôs happening now and what‚Äôs coming next.\nTo create a roadmap, it is useful to map out the key milestones and the tasks needed to accomplish the milestones. You can then use GitHub milestones and project boards to track progress and manage your project:\n\nDefine key milestones.\nCreate a milestone in GitHub\nAdd related issues to your milestone.\nSet up a project board.\nAdd issues to your project board.\nUse task lists in your issues to break down the work.\nAssign tasks to team members.\nLinking milestones, issues and pull requests to track progress.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#github-benefits-for-researchers-and-organisations",
    "href": "docs/software/development_workflow/project_management.html#github-benefits-for-researchers-and-organisations",
    "title": "Project management",
    "section": "GitHub benefits for researchers and organisations",
    "text": "GitHub benefits for researchers and organisations\nResearchers at TU Delft are eligible to receive GitHub Educational benefits, which includes\n\nGitHub Team plan at no cost (check out the benefits)\nGitHub Codespaces\nGitHub Pages for private repositories\nAnd more!\n\nTo qualify for the benefits, you must:\n\nHave a GitHub account\nBe an educator, faculty member, or researcher at a recognized educational institution\nBe able to provide documentation from your institute demonstrating your employment\n\n\n\n\n\n\n\n Apply for Educational benefits",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/r_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/r_envs_dependencies.html",
    "title": "Environment and dependency management in R",
    "section": "",
    "text": "R users often rely on RStudio Projects and renv to manage their development environments. RStudio Projects organize your workspace by managing file paths and configurations, while renv tracks and restores package dependencies to ensure reproducibility. Together, they provide a structured and reliable workflow for managing R projects.\nAlthough Conda can be used to create isolated R environments, it is less common in R workflows. Conda is most useful when managing multiple R versions, working in a Python + R setup, or handling system dependencies that are difficult to install through CRAN.\n\nProjects\nRStudio Projects provide an organized workspace for your analyses and scripts, ensuring that file paths and working directories remain consistent across sessions. When you open an RStudio Project, it automatically sets the project‚Äôs root directory as your working directory and loads project-specific settings stored in the .Rproj file. Using RStudio Projects helps you to keep your code, data, and output bundled together, and avoid issues with file paths.\nCreating a new RStudio Project:\n\nIn RStudio, go to File ‚Üí New Project.\nChoose whether to create a new directory, use an existing directory, or clone a project.\nFollow the prompts to configure your project settings.\n\n\n\nrenv\nThe renv package manages R package dependencies within a project. It creates a reproducible snapshot of your package environment, ensuring that collaborators (or your future self) can recreate the exact setup. Some key renv commands:\nInitialize renv in your project:\n# From your R console within the project directory:\nrenv::init()\nThis creates a dedicated library and a renv.lock file that tracks all installed packages. If a renv.lock file exists, renv::init() will automatically install the recorded dependencies. Otherwise, it sets up a new environment.\nCreate a snapshot of your dependencies:\nrenv::snapshot()\nThis updates the renv.lock file to reflect the current package versions in your project.\nTo restore an environment from a lockfile:\nrenv::restore()\nThis reinstalls packages according to the renv.lock file and reconstructs the environment.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nUsing RStudio Projects\nrenv for R",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "R"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/matlab_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/matlab_envs_dependencies.html",
    "title": "Environment and dependency management in MATLAB",
    "section": "",
    "text": "MATLAB does not use virtual environments like Python, where isolated environments manage dependencies. Instead, MATLAB handles project-specific dependencies using:\n\nToolboxes - Pre-packaged libraries that must be licensed and available\nMATLAB projects - A feature that manages paths and environments for a project\nPath management - Manually adding paths to the MATLAB search path with addpath and rmpath\n\nTo check dependencies in a project:\n\nUse requiredfilesandproducts to identify required MathWorks toolboxes for a script of function.\nUse the the Dependency Analyzer to detect file dependencies.\n\n\nCustom MATLAB Dependency Manager\nTo offer a solution for managing dependencies in MATLAB through a dependency file, we have created a Dependency Manager:\n DependencyManager",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/code_quality/refactoring.html",
    "href": "docs/software/code_quality/refactoring.html",
    "title": "Refactoring",
    "section": "",
    "text": "‚ÄúAlways leave the code you‚Äôre editing a little better than you found it.‚Äù\nRobert C. Martin (Uncle Bob)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Refactoring"
    ]
  },
  {
    "objectID": "docs/software/code_quality/refactoring.html#what-is-refactoring",
    "href": "docs/software/code_quality/refactoring.html#what-is-refactoring",
    "title": "Refactoring",
    "section": "What is refactoring?",
    "text": "What is refactoring?\nRefactoring is the process of restructuring existing code without changing its external behaviour. It improves maintainability and readability, making future developments smoother and reducing the likelihood of bugs. Key benefits include:\n\nImproving readability - Writing code that is easier to understand, benefits both yourself and future developers.\nReducing complexity - Simplifying complex structures by breaking down large functions or removing unnecessary dependencies.\nOptimizing design - Creating a more robust and adaptable codebase for long-term growth.\nEliminating redundancies - Removing duplicate or unnecessary code.\nEnsuring consistency - Following a consistent coding style for a cleaner, more maintanable codebase.\n\n\nWhen should you refactor?\n\n\n\nCC-BY-4.0 ¬© 2021 Balaban et al.\n\n\n\nRule of three: If you find yourself writing the same or similar code for the third time, it‚Äôs time to refactor.\nBefore adding a feature: Cleaning up existing code makes it easier to integrate a new functionality.\nWhen fixing a bug: Cleaning up surrounding code can help uncover and fix the issue faster.\nDuring code reviews: Refactoring during code reviews can prevent issues from becoming part of the public codebase and streamline the development process.\nWhen you spot a code smell: Addressing code smells early prevents them from evolving into more serious bugs.\n\n\n Learn more: When to refactor?\n\n\n\nHow to refactor code effectively?\nRefactoring should be done gradually, improving code in small controlled steps without introducing new functionalities. Keep these principles in mind:\n Maintain clean code - Aim for clarity, simplicity, and readability.\n Always work in small steps - So that you can easily identify whether a change to the code changes its behaviour.\n Avoid adding new features - Focus on improving structure, not functionality.\n Ensure tests pass - Verify that all existing tests succeed before starting with refactoring. If there are no tests, consider writing some basic tests first to cover the existing functionality.\n Test often - So that you can be sure the behaviour remains unchanged.\n Commit often - Use a version control system and commit often, so that you can easily revert changes if something goes wrong.\n Remember, you can stop at any point - Refactoring can be an endless task if you aim for perfection. Instead aim to leave the code in a better state than you found it.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Refactoring"
    ]
  },
  {
    "objectID": "docs/software/code_quality/refactoring.html#farleys-refactoring-method",
    "href": "docs/software/code_quality/refactoring.html#farleys-refactoring-method",
    "title": "Refactoring",
    "section": "Farley‚Äôs refactoring method",
    "text": "Farley‚Äôs refactoring method\nRefactoring can be approached in various ways. Here is a simple four-step method proposed by Dave Farley in his online course ‚ÄúRefactoring legacy code‚Äù.1 This method emphasizes safety and gradual improvement.\n\n1. Write approval tests\nCreate software tests for the code that will be refactored. Approval tests are software tests that check the outputs of a program or part of a program. Approval tests are important in refactoring because we need to know if changes in the code affect its behaviour.\n\n\n2. Reduce clutter\nRemove unnecessary code, such as unused (dead) code, and repeated code. While doing so, be cautious when removing code, but take some chances when reducing clutter. To safely reduce clutter, rely on version control to undo changes, and on approval tests to check that code changes do not affect its behaviour.\n\n\n3. Reduce cyclomatic complexity\nCyclomatic complexity refers to the number of logical branches or pathways used in the code to implement functionality and behaviour. The overuse of if statements and loops is an indication of code with high levels of cyclomatic complexity.\n\n\n\n\n\n\nTip Example\n\n\n\n\n\n\nPythonR\n\n\nAs an example, consider the following code snippet with high cyclomatic complexity due to multiple branching statements:\ndef get_discount(customer_type, is_holiday):\n    if customer_type == \"VIP\":\n        if is_holiday:\n            return 0.3\n        else:\n            return 0.2\n    elif customer_type == \"MEMBER\":\n        if is_holiday:\n            return 0.2\n        else:\n            return 0.1\n    else:\n        if is_holiday:\n            return 0.1\n        else:\n            return 0.0\nInstead, we can refactor this code to reduce its cyclomatic complexity by using a dictionary and conditional logic:\ndef get_discount(customer_type, is_holiday):\n    base_discounts = {\"VIP\": 0.2, \"MEMBER\": 0.1}\n    discount = base_discounts.get(customer_type, 0.0)\n    if is_holiday:\n        discount += 0.1\n    return discount\n\n\nAs an example, consider the following code snippet with high cyclomatic complexity due to multiple branching statements:\nget_discount &lt;- function(customer_type, is_holiday) {\n  if (customer_type == \"VIP\") {\n    if (is_holiday) {\n      return(0.3)\n    } else {\n      return(0.2)\n    }\n  } else if (customer_type == \"MEMBER\") {\n    if (is_holiday) {\n      return(0.2)\n    } else {\n      return(0.1)\n    }\n  } else {\n    if (is_holiday) {\n      return(0.1)\n    } else {\n      return(0.0)\n    }\n  }\n}\nInstead, we can refactor this code to reduce its cyclomatic complexity by using a named vector and conditional logic:\nget_discount &lt;- function(customer_type, is_holiday) {\n  base_discounts &lt;- c(VIP = 0.2, MEMBER = 0.1)\n  discount &lt;- base_discounts[customer_type] %||% 0.0\n  if (is_holiday) {\n    discount &lt;- discount + 0.1\n  }\n  return(discount)\n}\n\n\n\n\n\n\nTo reduce cyclomatic complexity:\n\nReduce branching or pathways in the code.\nBring related code together, and keep unrelated code apart.\nLook for blocks of code that can be separated in methods or functions; this is known as method extraction.\n\n\n\n4. Composing methods\nAt the last step, focus on improving the structure and readability of the code by extracting methods or functions from existing code blocks. This involves breaking down large methods into smaller, more manageable pieces that each perform a single task or function.\nWhen composing methods, do the following:\n\nMake each extracted method (or function) tell its own story. This requires understanding the context of the code within a program and how it is expected to be read and interpreted by other developers.\nIdeally, each method tells a single, well-structured and easy-to-understand story. If that is not the case, the code is poorly written and should be refactored.\nRename things (functions, classes, variables) so that their behaviour is clear in the code.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRefactoring techniques from Refactoring.Guru\neScience Center - Lesson on refactoring\nThe Alan Turing Institute - Refactoring\nDave Farley‚Äôs Refactoring Course",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Refactoring"
    ]
  },
  {
    "objectID": "docs/software/code_quality/refactoring.html#footnotes",
    "href": "docs/software/code_quality/refactoring.html#footnotes",
    "title": "Refactoring",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFarley, D. (n.d.) Refactoring legacy code (Online Course). CD.Training. https://courses.cd.training/courses/refactoring-tutorial‚Ü©Ô∏é",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Refactoring"
    ]
  },
  {
    "objectID": "docs/software/code_quality/index.html",
    "href": "docs/software/code_quality/index.html",
    "title": "Code quality",
    "section": "",
    "text": "‚ÄúEveryone knows that debugging is twice as hard as writing a program in the first place. So if you‚Äôre as clever as you can be when you write it, how will you ever debug it?‚Äù\nBrian W. Kernighan\n\nThe quality of your research software plays a crucial role in its reliability, maintainability, and scalability. Writing clean code means developing code that is easy to read, understand - not for just you, but for others as well. Well-structured code simplifies debugging and allows for future modifications and extensions, ensuring your software remains useful and adaptable over time.\n\n\n\n Code Style\nConventions and guidelines used to write and format code.\n\nLearn more ¬ª\n\n\n\n Refactoring\nRestructuring existing code without changing its external behavior.\n\nLearn more ¬ª\n\n\n\n Code Smells\nSymptoms of poor code quality that can indicate deeper problems.\n\nLearn more ¬ª\n\n\n\n Online services\nServices that provide code quality analysis.\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nThe Turing Way - Writing Robust Code\nUtrecht University - Workshop on Writing Reproducible Code",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html",
    "href": "docs/software/code_quality/code_smells/side_effects.html",
    "title": "Side effects and external state",
    "section": "",
    "text": "Side effects occur when a function modifies external state or interacts with the outside world beyond simply returning a value. This makes code less predictable, harder to test, and more difficult to debug.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#symptoms",
    "href": "docs/software/code_quality/code_smells/side_effects.html#symptoms",
    "title": "Side effects and external state",
    "section": "Symptoms",
    "text": "Symptoms\n\nUnexpected changes to the global state\nNon-deterministic behavior\nHidden dependencies\n\n\n\n\n\n\n\nTip\n\n\n\nPure functions are deterministic (always return the same output for the same input) and have no side-effects.\nInstead, non-pure functions often:\n\nModify global variables or shared state, leading to unintended behavior.\nChange input parameters (mutating function arguments).\nPerform I/O operations like reading from or writing to files, databases, or APIs.\nGenerate random numbers, making them non-deterministic.\nDepend on external state, meaning results may change due to external factors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#example---function-with-side-effects",
    "href": "docs/software/code_quality/code_smells/side_effects.html#example---function-with-side-effects",
    "title": "Side effects and external state",
    "section": "Example - Function with side effects",
    "text": "Example - Function with side effects\n\nPythonR\n\n\n# Modifies global state (side effect)\ndata = []\n\ndef add_item(item):\n    data.append(item)  # Changes an external variable\n\nadd_item(\"A\")\nprint(data)  # ['A'] - Output depends on previous calls\n\n\n# Modifies global state (side effect)\ndata &lt;- c()\n\nadd_item &lt;- function(item) {\n  data &lt;&lt;- c(data, item)  # Changes external variable using &lt;&lt;-\n}\n\nadd_item(\"A\")\nprint(data)  # [1] \"A\" - Output depends on previous calls\n\n\n\n\nSolutions\n\n1. Separate pure and non-pure functions\nKeep your computational logic (pure) separate from side-effect operations (non-pure).\n\nPythonR\n\n\ndef process_data(data):  # Pure function: no external state modification\n    return [x**2 for x in data]\n\ndef save_to_file(filename, data):  # Non-pure: writes to a file\n    with open(filename, \"w\") as f:\n        f.write(\"\\n\".join(map(str, data)))\n\n# Usage\nnumbers = [1, 2, 3]\nprocessed = process_data(numbers)\nsave_to_file(\"output.txt\", processed)\n\n\nprocess_data &lt;- function(data) {  # Pure function: no external state modification\n  return(data^2)\n}\n\nsave_to_file &lt;- function(filename, data) {  # Non-pure: writes to a file\n  writeLines(as.character(data), con = filename)\n}\n\n# Usage\nnumbers &lt;- c(1, 2, 3)\nprocessed &lt;- process_data(numbers)\nsave_to_file(\"output.txt\", processed)\n\n\n\n\n\n2. Avoid mutating global variables\nUse function parameters and return values instead of modifying external variables.\n\nPythonR\n\n\ndef add_item(data, item):\n    return data + [item]  # Returns a new list instead of modifying global state\n\ndata = []\ndata = add_item(data, \"A\")  # Safe: no side effects\n\n\nadd_item &lt;- function(data, item) {\n  return(c(data, item))  # Returns a new vector instead of modifying global state\n}\n\ndata &lt;- c()\ndata &lt;- add_item(data, \"A\")  # Safe: no side effects",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/side_effects.html#key-takeaways",
    "title": "Side effects and external state",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nEnsure that each function or module has a single responsibility.\nBreak down complex functions into smaller, focused functions that perform specific tasks.\nIsolate non-pure functions with side effects from pure functions.\n\n\n\n\nCC-BY-4.0 CodeRefinery",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html",
    "href": "docs/software/code_quality/code_smells/long_method.html",
    "title": "Long Method",
    "section": "",
    "text": "‚ÄúFunctions should do one thing. They should do it well. They should do it only.‚Äù\nRobert C. Martin (Uncle Bob)\nA ‚Äúlong method‚Äù is a common code smell where a method or function becomes overly long and handles multiple responsibilities at once. This makes the code hard to read, understand, test, and maintain. Long methods often indicate that a function is doing too much and may benefit from being broken into smaller, more focussed helper functions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#symptoms",
    "href": "docs/software/code_quality/code_smells/long_method.html#symptoms",
    "title": "Long Method",
    "section": "Symptoms",
    "text": "Symptoms\nA long method often:\n\nPerforms multiple tasks rather than a single, well-defined responsibility.\nHas deeply nested control structures, making it harder to follow.\nIncludes multiple sections of logic that could be extracted into separate functions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#example---long-method",
    "href": "docs/software/code_quality/code_smells/long_method.html#example---long-method",
    "title": "Long Method",
    "section": "Example - Long method",
    "text": "Example - Long method\nBelow is an example of a function that is doing too much:\n\nPythonR\n\n\ndef load_data(filepath: str):\n    # Check if data file exists\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(\"File not found\")\n\n    _, extension = os.path.splitext(filepath)\n\n    # Load data based on file extension\n    if extension == \".json\":\n        with open(filepath, \"r\") as file:\n            # If file extension is .json: load json data\n            data = json.load(file)\n    elif extension == \".pickle\":\n        with open(filepath, \"rb\") as file:\n            # If file extension is .pickle: load pickled data\n            data = pickle.load(file)\n    elif extension == \".csv\":\n        # If file extension is .csv: load csv data\n        data = read_csv(filepath)\n    else:\n        raise ValueError(f\"Unsupported file format: {extension}\")\n\n    # Verify content of data set\n    if not isinstance(data, (list, dict, pd.DataFrame)):\n        raise ValueError(\"Invalid data format\")\n\n    return data\n\n\nload_data &lt;- function(filepath) {\n  # Check if data file exists\n  if (!file.exists(filepath)) {\n    stop(\"File not found\")\n  }\n\n  # Extract file extension\n  extension &lt;- tools::file_ext(filepath)\n\n  # Load data based on file extension\n  if (extension == \"json\") {\n    # If file extension is .json: load json data\n    data &lt;- jsonlite::read_json(filepath)\n  } else if (extension == \"rds\") {\n    # If file extension is .rds: load serialized R object\n    data &lt;- readRDS(filepath)\n  } else if (extension == \"csv\") {\n    # If file extension is .csv: load csv data\n    data &lt;- readr::read_csv(filepath) # or read.csv(filepath)\n  } else {\n    stop(paste(\"Unsupported file format:\", extension))\n  }\n\n  # Verify content of data set\n  if (!is.data.frame(data) && !is.list(data)) {\n    stop(\"Invalid data format\")\n  }\n\n  return(data)\n}\n\n\n\n\nIssues\n\nThe function is handling file validation, data loading, and data verification, which are separate concerns.\nIt is now difficult to test individual parts in isolation.\nAdding support for new file types requires modifying a large function.\n\n\n\nSolution\nIdentify logical blocks of code within the long method/function and extract them into separate methods with descriptive names. We should aim to make each method responsible for a singular task and compose more complex functionalities from modular components.\n\nExample solution long method\n\nPythonR\n\n\ndef load_data(filepath: str) -&gt; Data:\n    verify_filepath(filepath: str)\n    data = read_data(filepath: str)\n    verify_data(data)\n    return data\n\n# Helper function to verify file path\ndef verify_filepath(filepath: str):\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(\"File not found\")\n\n# Helper function to read data from file based on its extension\ndef read_data(filepath: str) -&gt; Data:\n    # Extract file extension\n    _, extension = os.path.splitext(filepath)\n\n    # Create dictionary mapping file extensions to read functions\n    data_types = {\n        \".json\": read_from_json,\n        \".pickle\": read_from_pickle,\n        \".csv\": read_from_csv,\n    }\n\n    # Select read function based on file extension\n    try:\n        read_function = data_types[extension]\n    except KeyError:\n        raise ValueError(f\"Unsupported file format: {extension}\")\n    return data_types[extension](filepath)\n\n# Placeholder for helper functions to read data from different file formats\ndef read_from_json(filepath: str): pass\ndef read_from_pickle(filepath: str): pass\ndef read_from_csv(filepath: str): pass\n\n\n# Main function that orchestrates the workflow\nload_data &lt;- function(filepath) {\n  verify_filepath(filepath)\n  data &lt;- read_data(filepath)\n  verify_data(data)\n  return(data)\n}\n\n# Helper function to verify file path\nverify_filepath &lt;- function(filepath) {\n  if (!file.exists(filepath)) {\n    stop(\"File not found\")\n  }\n}\n\n# Helper function to read data from file based on its extension\nread_data &lt;- function(filepath) {\n  # Extract file extension\n  extension &lt;- tools::file_ext(filepath)\n\n  # Create list mapping file extensions to read functions\n  data_readers &lt;- list(\n    json = read_from_json,\n    rds = read_from_rds,\n    csv = read_from_csv\n  )\n\n  # Select read function based on file extension\n  if (!(extension %in% names(data_readers))) {\n    stop(paste(\"Unsupported file format:\", extension))\n  }\n\n  return(data_readers[[extension]](filepath))\n}\n\n# Helper function to verify data\nverify_data &lt;- function(data) {\n  if (!is.data.frame(data) && !is.list(data)) {\n    stop(\"Invalid data format\")\n  }\n}\n\n# Placeholder helper functions to read data from different file formats\nread_from_json &lt;- function(filepath) {\n  jsonlite::read_json(filepath)\n}\n\nread_from_rds &lt;- function(filepath) {\n  readRDS(filepath)\n}\n\nread_from_csv &lt;- function(filepath) {\n  readr::read_csv(filepath) # or read.csv(filepath)\n}",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/long_method.html#key-takeaways",
    "title": "Long Method",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nBreaking a long method into smaller, well-named helper functions makes the code easier to read and understand.\nEach function now has a single responsibility, reducing complexity and making future modifications more manageable.\nWith isolated functions, individual components can be tested independently, leading to more reliable and maintainable code.\n\n By breaking the long method into smaller helper functions, we improve the overall structure and maintainability of the code.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\neScience Center - Slides on writing modular code\nCarpentries Incubator - Modular Code Development",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html",
    "title": "Inappropriate Intimacy",
    "section": "",
    "text": "This code smell occurs when one part of the system knows too much about the internal details of another, leading to tight coupling. When components are too dependent on each other, it becomes difficult to modify or extend the system without breaking other parts.\nA good design principle to follow is the Law of Demeter, also known as the ‚ÄúDon‚Äôt talk to strangers‚Äù rule. It suggests that a module should only interact with its direct dependencies rather than deeply nested objects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#symptoms",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#symptoms",
    "title": "Inappropriate Intimacy",
    "section": "Symptoms",
    "text": "Symptoms\n\nA class accesses properties of another object‚Äôs properties, exposing too much detail.\nChanges in one part of the code require changes in multiple other places.\nBecause multiple classes depend on each other‚Äôs internal structures, small changes can cause unintended issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#example---violating-law-of-demeter",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#example---violating-law-of-demeter",
    "title": "Inappropriate Intimacy",
    "section": "Example - Violating Law of Demeter",
    "text": "Example - Violating Law of Demeter\nIn this example, a SensorSystem directly accesses the TemperatureSensor internal attributes, creating tight coupling.\nclass TemperatureSensor:\n    def __init__(self, temperature):\n        self.temperature = temperature  # Internal detail exposed\n\nclass SensorSystem:\n    def __init__(self, sensor):\n        self.sensor = sensor\n\n    def get_temperature(self):\n        # Law of Demeter violation: Directly accessing sensor's attribute\n        return self.sensor.temperature\n\n# Usage\nsensor = TemperatureSensor(25)\nsystem = SensorSystem(sensor) \ntemperature = system.get_temperature()\nprint(temperature)  # 25\nProblem: The SensorSystem class depends on the internal structure of TemperatureSensor. If the way temperature is stored changes (e.g., a new sensor model), SensorSystem must also change.\n\nSolutions\n\nExample solution - Using getter methods\nInstead of directly accessing attributes, define getter methods in TemperatureSensor to limit exposure.\nclass TemperatureSensor:\n    def __init__(self, temperature):\n        self._temperature = temperature  # Use a private variable\n\n    def get_temperature(self):\n        return self._temperature  # Encapsulated access\n\nclass SensorSystem:\n    def __init__(self, sensor):\n        self.sensor = sensor\n\n    def get_temperature(self):\n        return self.sensor.get_temperature()  # Indirect access through method\n\n# Usage\nsensor = TemperatureSensor(25)\nsystem = SensorSystem(sensor)\nprint(system.get_temperature())  # 25\nWhy is this better?\n\nThe SensorSystem no longer needs to know the internal structure of TemperatureSensor.\nIf TemperatureSensor changes, only get_temperature() needs to be updated, not every place it‚Äôs used.\n\n\n\nExample solution - Removing the dependency\nA better design is to pass only the needed data instead of an entire object.\nclass SensorSystem:\n    def __init__(self, temperature):\n        self.temperature = temperature\n\n    def get_temperature(self):\n        return self.temperature  # Works directly with the value\n\n# Usage\ntemperature = 25\nsystem = SensorSystem(temperature)\nprint(system.get_temperature())  # 25\nWhy is this better?\n\nSensorSystem no longer depends on TemperatureSensor, making it more modular and reusable.\nWorks even if the source of temperature data changes (e.g., from a file, API, or another sensor).\n\n\n\n\n\n\n\nBalance between dependecy injection and encapsulation. If the data is simple and does not require complex operations, pass it directly. If the data is complex or requires additional logic, encapsulate it in a class.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#key-takeaways",
    "title": "Inappropriate Intimacy",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFollow the Law of Demeter - Only interact with direct dependencies.\nEncapsulate data - Use getters and setters to access and modify data.\nReduce dependencies - pass only the necessary information to other components.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRealPython - Python Classes\nRealPython - Getters and Setters",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html",
    "href": "docs/software/code_quality/code_smells/duplication.html",
    "title": "Duplicated code",
    "section": "",
    "text": "‚ÄúPerfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.‚Äù\nAntoine de Saint-Exup√©ry\nDuplicated code occurs when similar or identical blocks of code appear multiple times within a codebase. This can increase maintenance efforts, as changes in one place might require corresponding changes elsewhere, leading to inconsistencies and higher changes or errors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#symptoms",
    "href": "docs/software/code_quality/code_smells/duplication.html#symptoms",
    "title": "Duplicated code",
    "section": "Symptoms",
    "text": "Symptoms\n\nThe same logic appears in multiple places, sometimes with minor variations.\nFixing a bug required modifying the same code in multiple places.\nAdding a new feature results in copy-pasting existing code rather than reusing it.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#example---duplicate-code-in-functions",
    "href": "docs/software/code_quality/code_smells/duplication.html#example---duplicate-code-in-functions",
    "title": "Duplicated code",
    "section": "Example - Duplicate code in functions",
    "text": "Example - Duplicate code in functions\n\nPythonR\n\n\ndef time_of_flight_ball(initial_velocity, launch_angle):\n    g = 9.81  # Earth's gravity (m/s¬≤)\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\ndef time_of_flight_rocket(initial_velocity, launch_angle):\n    g = 9.81\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\ndef time_of_flight_satellite(initial_velocity, launch_angle):\n    g = 9.81\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\n\ntime_of_flight_ball &lt;- function(initial_velocity, launch_angle) {\n  g &lt;- 9.81  # Earth's gravity (m/s¬≤)\n  return((2 * initial_velocity * sin(launch_angle)) / g)\n}\n\ntime_of_flight_rocket &lt;- function(initial_velocity, launch_angle) {\n  g &lt;- 9.81\n  return((2 * initial_velocity * sin(launch_angle)) / g)\n}\n\ntime_of_flight_satellite &lt;- function(initial_velocity, launch_angle) {\n  g &lt;- 9.81\n  return((2 * initial_velocity * sin(launch_angle)) / g)\n}\n\n\n\n\nSolution\n\nRefactor the code to accept parameters as arguments, instead of hard-coding them.\nExtract common functionality into functions or methods.\nRefactor duplicated code into higher-level abstractions.\nMake use of utility functions to centralize common code and avoid duplication.\n\n\nPythonR\n\n\ndef time_of_flight(initial_velocity, launch_angle, gravity=9.81):\n    \"\"\"Compute time of flight for any projectile.\"\"\"\n    return (2 * initial_velocity * np.sin(launch_angle)) / gravity\n\n# Usage\ntof_ball = time_of_flight(30, np.pi/4)       # Time of flight for a ball\ntof_rocket = time_of_flight(100, np.pi/3)    # Time of flight for a rocket\ntof_mars_probe = time_of_flight(300, np.pi/6, gravity=3.71)  # Gravity adjusted for Mars\n\n\ntime_of_flight &lt;- function(initial_velocity, launch_angle, gravity = 9.81) {\n  # Compute time of flight for any projectile\n  return((2 * initial_velocity * sin(launch_angle)) / gravity)\n}\n\n# Usage\ntof_ball &lt;- time_of_flight(30, pi/4)       # Time of flight for a ball\ntof_rocket &lt;- time_of_flight(100, pi/3)    # Time of flight for a rocket\ntof_mars_probe &lt;- time_of_flight(300, pi/6, gravity = 3.71)  # Gravity adjusted for Mars",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/duplication.html#key-takeaways",
    "title": "Duplicated code",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nExtracting common functionality into functions or methods can help reduce duplication and improve code reuse.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html",
    "href": "docs/software/code_quality/code_smells/dead_code.html",
    "title": "Dead code",
    "section": "",
    "text": "Dead code refers to unused or unreachable code that remains in the codebase, but serves no functional purpose. Commented-out code consists of inactive code blocks that developers have disabled rather than deleting. Both contribute to clutter and reduce maintainability.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Dead code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#symptoms",
    "href": "docs/software/code_quality/code_smells/dead_code.html#symptoms",
    "title": "Dead code",
    "section": "Symptoms",
    "text": "Symptoms\n\nUnused variables or functions.\nConditional blocks that never execute.\nLarge blocks of commented-out code.\nUsing comments to disable code to change the behavior of the code.\n\n\n\n\n\n\n\nTip\n\n\n\nDo not use comments to change the behavior of the code. Instead, make use of input parameters or configuration settings to control the behavior of the code.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Dead code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#example---dead-code",
    "href": "docs/software/code_quality/code_smells/dead_code.html#example---dead-code",
    "title": "Dead code",
    "section": "Example - Dead code",
    "text": "Example - Dead code\nUnreachable or unused code that remains in the codebase but serves no purpose.\n\nPythonR\n\n\ndef calculate_discount(price, customer_type):\n    if customer_type == \"premium\":\n        discount = 0.20\n    elif customer_type == \"standard\":\n        discount = 0.10\n    else:\n        discount = 0\n\n    final_price = price * (1 - discount)\n\n    # Dead code - this variable is never used\n    unused_result = final_price * 2\n\n    # Dead code - this function is never called\n    def log_transaction():\n        print(\"Transaction logged\")\n\n    return final_price\n\n\ncalculate_discount &lt;- function(price, customer_type) {\n  if (customer_type == \"premium\") {\n    discount &lt;- 0.20\n  } else if (customer_type == \"standard\") {\n    discount &lt;- 0.10\n  } else {\n    discount &lt;- 0\n  }\n\n  final_price &lt;- price * (1 - discount)\n\n  # Dead code - this variable is never used\n  unused_result &lt;- final_price * 2\n\n  # Dead code - this function is never called\n  log_transaction &lt;- function() {\n    print(\"Transaction logged\")\n  }\n\n  return(final_price)\n}\n\n\n\n\nIssues\n\nunused_result is computed but never returned or used, wasting memory and computation.\nlog_transaction() is defined but never called, adding unnecessary clutter.\nDead code confuses readers about the actual functionality of the function.\nMakes refactoring and maintenance more difficult.\n\n\n\nSolution - Dead code\nRemove unused code completely. Use version control (e.g., Git) to restore it if needed later.\n\nPythonR\n\n\ndef calculate_discount(price, customer_type):\n    if customer_type == \"premium\":\n        discount = 0.20\n    elif customer_type == \"standard\":\n        discount = 0.10\n    else:\n        discount = 0\n\n    final_price = price * (1 - discount)\n    return final_price\n\n\ncalculate_discount &lt;- function(price, customer_type) {\n  if (customer_type == \"premium\") {\n    discount &lt;- 0.20\n  } else if (customer_type == \"standard\") {\n    discount &lt;- 0.10\n  } else {\n    discount &lt;- 0\n  }\n\n  final_price &lt;- price * (1 - discount)\n  return(final_price)\n}",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Dead code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#example---commented-out-code",
    "href": "docs/software/code_quality/code_smells/dead_code.html#example---commented-out-code",
    "title": "Dead code",
    "section": "Example - Commented-out code",
    "text": "Example - Commented-out code\nDisabling code with comments instead of deleting it or using configuration settings to control behavior.\n\nPythonR\n\n\ndef process_data(data, debug=False):\n    # Debug logging is controlled by comments instead of a parameter\n    # print(\"Processing started\")  # Commented-out for production\n\n    result = []\n    for item in data:\n        value = item * 2\n        # print(f\"Processing: {item} -&gt; {value}\")  # Commented-out debug statement\n        result.append(value)\n\n    # Entire feature disabled via commenting - no way to re-enable without editing code\n    # if debug:\n    #     with open(\"log.txt\", \"w\") as f:\n    #         f.write(str(result))\n\n    return result\n\n\nprocess_data &lt;- function(data, debug = FALSE) {\n  # Debug logging is controlled by comments instead of a parameter\n  # cat(\"Processing started\\n\")  # Commented-out for production\n\n  result &lt;- c()\n  for (item in data) {\n    value &lt;- item * 2\n    # cat(sprintf(\"Processing: %d -&gt; %d\\n\", item, value))  # Commented-out debug statement\n    result &lt;- c(result, value)\n  }\n\n  # Entire feature disabled via commenting - no way to re-enable without editing code\n  # if (debug) {\n  #   write.table(result, file = \"log.txt\", quote = FALSE, row.names = FALSE)\n  # }\n\n  return(result)\n}\n\n\n\n\nIssues\n\nCommented-out code is confusing: is it there for a reason or was it forgotten?\nIt blocks maintenance - developers won‚Äôt refactor around dead code they don‚Äôt understand.\nThe debug parameter exists but is never used because the logging is disabled via comments.\nMakes version history harder to follow with unclear code blocks.\n\n\n\nSolution - Commented-out code\nUse input parameters or configuration settings to control behavior (execution) instead of commenting out code.\n\nPythonR\n\n\ndef process_data(data, debug=False):\n    \"\"\"\n    Process data with optional debug logging.\n\n    Parameters:\n    -----------\n    data : list\n        The data to process\n    debug : bool\n        If True, enable debug output\n    \"\"\"\n    if debug:\n        print(\"Processing started\")\n\n    result = []\n    for item in data:\n        value = item * 2\n        if debug:\n            print(f\"Processing: {item} -&gt; {value}\")\n        result.append(value)\n\n    if debug:\n        with open(\"log.txt\", \"w\") as f:\n            f.write(str(result))\n\n    return result\n\n# Usage\nprocess_data([1, 2, 3], debug=False)      # Production mode\nprocess_data([1, 2, 3], debug=True)       # Debug mode\n\n\nprocess_data &lt;- function(data, debug = FALSE) {\n  # Process data with optional debug logging\n  #\n  # Parameters:\n  # -----------\n  # data : vector\n  #   The data to process\n  # debug : logical\n  #   If TRUE, enable debug output\n\n  if (debug) {\n    cat(\"Processing started\\n\")\n  }\n\n  result &lt;- c()\n  for (item in data) {\n    value &lt;- item * 2\n    if (debug) {\n      cat(sprintf(\"Processing: %d -&gt; %d\\n\", item, value))\n    }\n    result &lt;- c(result, value)\n  }\n\n  if (debug) {\n    write.table(result, file = \"log.txt\", quote = FALSE, row.names = FALSE)\n  }\n\n  return(result)\n}\n\n# Usage\nprocess_data(c(1, 2, 3), debug = FALSE)   # Production mode\nprocess_data(c(1, 2, 3), debug = TRUE)    # Debug mode",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Dead code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/dead_code.html#key-takeaways",
    "title": "Dead code",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nDelete dead code immediately - use version control to recover it if needed later.\nDo not use comments to disable code - use parameters, configuration settings, or conditional logic instead.\nCommit removals meaningfully - explain in the commit message why code was removed.\nKeep code clean - cluttered code with dead sections is harder to understand and maintain.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Dead code"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab_ci_cd.html",
    "href": "docs/software/automation/gitlab_ci_cd.html",
    "title": "GitLab pipelines",
    "section": "",
    "text": "As previously mentioned in the Project management section, TU Delft has its own GitLab instance. However, it does not (yet) have preconfigured servers available to run GitLab pipelines, the equivalent of GitHub Actions.\nIn order to set up a pipeline, you will need to request a TU Delft Virtual Private Server and configure a GitLab runner there. The DCC has developed a step-by-step guide (see below), along with guidance on setting up a CI pipeline for a MATLAB environment.\n\n\n\n Continuous Integration with GitLab\nSet up CI/CD for your project in TU Delft GitLab.\n\nLearn more ¬ª\n\n\n\n Setting up a GitLab runner for MATLAB\nCreate a CI pipeline for a MATLAB environment in TU Delft GitLab.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html",
    "href": "docs/software/automation/gitlab/gitlab_docker.html",
    "title": "CI with GitLab",
    "section": "",
    "text": "If you are using a TU Delft GitLab instance and you want to implement DevOps or CI/CD pipelines, you will need to install a GitLab Runner yourself. This runner must be set up on a server, where it will respond to repository events such as commits or merge requests in your GitLab project. This guide will help you deploy a GitLab Runner in a Docker container on a server. Once set up, the runner will automatically execute CI/CD tests and store artifacts whenever a new commit is pushed to your GitLab repository.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html#background",
    "href": "docs/software/automation/gitlab/gitlab_docker.html#background",
    "title": "CI with GitLab",
    "section": "",
    "text": "If you are using a TU Delft GitLab instance and you want to implement DevOps or CI/CD pipelines, you will need to install a GitLab Runner yourself. This runner must be set up on a server, where it will respond to repository events such as commits or merge requests in your GitLab project. This guide will help you deploy a GitLab Runner in a Docker container on a server. Once set up, the runner will automatically execute CI/CD tests and store artifacts whenever a new commit is pushed to your GitLab repository.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html#quick-overview-of-how-it-works",
    "href": "docs/software/automation/gitlab/gitlab_docker.html#quick-overview-of-how-it-works",
    "title": "CI with GitLab",
    "section": "Quick overview of how it works",
    "text": "Quick overview of how it works\nTo run a CI/CD pipeline, a gitlab-runner Docker container runs continuously on the server. When a new commit is pushed to the GitLab repository, it triggers the CI/CD process. The pipeline, defined in the .gitlab-ci.yml file, specifies the jobs to run (e.g., unit tests). The Docker image used for running the CI/CD jobs can be specified in the first line of the .gitlab-ci.yml file. In the below example, we define image:python:3.12.3, a new container based on the python:3.12.3 Docker image is spawned each time a commit is made. This container executes the tests on your Python scripts and generates artifacts, as outlined in the .gitlab-ci.yml file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html#prerequisites",
    "href": "docs/software/automation/gitlab/gitlab_docker.html#prerequisites",
    "title": "CI with GitLab",
    "section": "Prerequisites",
    "text": "Prerequisites\nServer: This guide assumes you have access to a server to host the GitLab Runner. You can request a server from TU Delft ICT Services by following the instructions here. It is useful to set this up on a server so that Docker can be running continuously, and be ready to run CI/CD tests whenever a new commit occurs in the repository.\nDocker: A Docker container is used to run the GitLab Runner and initialize the CI/CD pipeline.\nGitLab Runner: ‚ÄúRunners are the agents that run the CI/CD jobs that come from GitLab. When you register a runner, you are setting up communication between your GitLab instance and the machine where GitLab Runner is installed. Runners usually process jobs on the same machine where you installed GitLab Runner.‚Äù - GitLab documentation\nGitLab repository: A remote GitLab repository stores your project code and keeps track of its development. You‚Äôre on one right now! :) If you haven‚Äôt already, log in to TU Delft‚Äôs GitLab instance at gitlab.tudelft.nl using your NetID and password, and create a repository for your project.\nCI/CD pipeline: ‚ÄúA CI/CD pipeline automates your software delivery process. The pipeline builds code, runs tests (CI), and safely deploys a new version of the application (CD)‚Äù.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html#toolssoftware",
    "href": "docs/software/automation/gitlab/gitlab_docker.html#toolssoftware",
    "title": "CI with GitLab",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nGitLab (TU Delft instance)\nDocker\ngitlab-runner Docker image",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/gitlab_docker.html#steps",
    "href": "docs/software/automation/gitlab/gitlab_docker.html#steps",
    "title": "CI with GitLab",
    "section": "Steps",
    "text": "Steps\n\nRequest the server\nConnect to the server via ssh\nInstall Docker on the server\nPull in the gitlab-runner image\nCreate a unit test function stored as a file in the repository\nMake the .gitlab-ci.yml file\nSet up the GitLab Runner\nDeploy the GitLab Runner in a Docker container\nRegister the runner\nTest the CI/CD pipeline\n\n\nStep 1. Request server running Ubuntu\nIf you don‚Äôt have a VPS already, you can request one from TU Delft ICT. Instructions for requesting a server and storage are available here.\nRecommended Configuration for a GitLab Runner:\n\nBasic Configuration 4 (Ubuntu)\nNo additional ports need to be configured for deploying a GitLab Runner with Docker.\nAdditional space if your Docker images exceed ~10Gb.\n\n\n\nStep 2. Connect to the server via ssh\nThe email response from Sysadmin@TUDelft.nl will include instructions for connecting to your server via SSH. The default login process involves:\n\nConnecting to the Bastion host (an intermediary server).\nConnecting to your assigned server (so it is a two-step process).\n\nRefer to your email provided by ICT admin for these steps.\nFor Windows, you can use PuTTY to connect. For Mac/Linux, you can configure one-step access by storing SSH keys between your local machine and the server and setting up an alias.\nUpon successful connection, your terminal/command prompt should display something like this:\n\n\n\nStep 3. Install Docker on the server\nServers often do not come with Docker installed, so you may need to do it by yourself. To check whether Docker is installed, run docker --version. If you get an error message, you can install it using the following commands:\n# From https://docs.docker.com/engine/install/ubuntu\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install the latest Docker packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\n# Verify that the installation is successful by running the hello-world image\nsudo docker run hello-world\nNow that you‚Äôve installed Docker, you can check its installation with docker --version from the terminal. The result should show the version of Docker you just installed.\n\n\nStep 4. Pull the gitlab-runner Docker image\nIn order to run CI/CD jobs for your repository, you need to install GitLab Runner. GitLab Runner is an application that works with GitLab CI/CD to run jobs in a pipeline. Rather than install GitLab Runner directly on the server, we will run a lightweight version of it as a Docker container. To do so, we first need to pull the gitlab-runner Docker image by running:\ndocker pull gitlab/gitlab-runner\nYou can check whether it was successful by running docker images - you should see the gitlab/gitlab-runner image listed in the output.\n\n\nStep 5. Create a unit test function stored as a file in the repository\n\n\nStep 6. Set up the CI by configuration of the .gitlab-ci.yml file\nThis file, located in the root of your repository, defines the CI/CD pipeline. It specifies the docker container to run, what scripts to run inside the container, and what to store as artifacts after the job completion.\nIn the first line of the file, specify the Docker image using: image: &lt;image_name&gt;:&lt;tag&gt; to indicate you want to run the runner in a Docker container. Replace  with the name of the Docker image (e.g., python, image names can be found on DockerHub). Replace  with the image version (e.g., 3.12.3). If omitted, the latest tag is used by default.\n\n\n\n\n\n\nNote Important\n\n\n\n\nThis file must be named .gitlab-ci.yml for the GitLab Runner to recognize it. Below is an example .gitlab-ci.yml file. Only use spaces to indent your .yml configuration.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\nCheck Atlassian guides for more examples.\n\n\n# Sample CI/CD configuration for Python\n# -----\ntest:\n  image: python:3.12.3 # Docker image for the build environment\n  cache:\n    paths:\n      - .cache/pip\n      - venv/\n  script: # Modify the commands below to build your repository.\n    - pip install -r requirements.txt  # Install dependencies\n    - pytest\n  artifacts:\n    paths:\n      - cover/ # Store coverage reports as artifacts\nOptional: You can add tags to the .gitlab-ci.yml file to assign specific runners to jobs. If you use tags, ensure they match exactly when registering the runner.\n\n\nStep 7. Setup the GitLab Runner\n\nFollow the instructions here up to step 7 to create a GitLab Runner for your repository\nChoose Linux under operating systems\nCopy the authentication token generated during the process. You will need it for Step 9.\n\n\n\nStep 8. Deploy GitLab runner in a Docker container\nRun the following command to deploy the GitLab Runner as a Docker container:\ndocker run -d --name gitlab-runner --restart always \\\n-v /srv/gitlab-runner/config:/etc/gitlab-runner \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\ngitlab/gitlab-runner:latest\n\n\n\n\n\n\nWarning Warning\n\n\n\nMounting /var/run/docker.sock lets any CI job control the host Docker daemon, effectively giving root access on the server. Use only on trusted projects, or consider alternatives!\n\n\nCheck that gitlab-runner container is running with\ndocker ps -a\n\n\nStep 9. Register the runner using authentication token\nRun the following command to register your runner and configure it to deploy in a Docker container on your server.\n\n\n\n\n\n\nNote Note\n\n\n\nGitLab has migrated from registration tokens to authentication tokens. The new authentication tokens use the glrt- prefix and are created through the GitLab UI first, then used in the registration command.\n\n\ndocker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register\nYou will be prompted to answer the following questions:\nEnter the GitLab instance URL (for example, https://gitlab.com/):\nhttps://gitlab.tudelft.nl\n\nEnter the authentication token:\nglrt-xxxxxxxxxxxxxxx\n\nVerifying runner... is valid                        runner=xxxxxxx\n\nEnter a name for the runner. This is stored only in the local config.toml file:\n[xxxxxxx]: example-runner\n\nEnter an executor: instance, kubernetes, docker-windows, docker-autoscaler, parallels, shell, ssh, virtualbox, docker+machine, custom, docker:\ndocker\n\nEnter the default Docker image (for example, ruby:2.7):\npython:3.12.3\n\nRunner registered successfully. \n\nFeel free to start it, but if it is running already the config should be automatically reloaded!\n\nConfiguration (with the authentication token) was saved in \"/etc/gitlab-runner/config.toml\"\n\n\nStep 10. Verify the CI/CD Pipeline\n\n1. Check Runner Status:\n\nGo to your repository‚Äôs Settings ‚áæ CI/CD.\nUnder Runners, expand the section to confirm that your runner is active (indicated by a green dot). This means the runner is ready to execute jobs, but it requires a trigger to start.\n\n\n\n2. Trigger the Pipeline:\n\nMake a new commit to your GitLab repository. This will automatically trigger the pipeline to run.\n\n\n\n3. Monitor Pipeline Status:\n\nAfter the new commit, navigate to CI/CD ‚áæ Pipelines in your project.\nCheck the status of your pipeline:\n\nA green ‚Äúpassed‚Äù status with a checkmark means your pipeline ran successfully. Congratulations!\nA red ‚Äúfailed‚Äù status indicates an error. Review the error message to troubleshoot. Common issues include incorrect formatting in the .gitlab-ci.yml file or misconfigured test definitions.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\nThis guide was created using multiple references on GitLab runner and Docker. If you would like more information or details, please refer to the links below. - Hostinger - how to install docker on Ubuntu - Cloudclone - How to install docker on Ubuntu - Digitalocean- How to install docker on Ubuntu - GitLab - Use CI/CD to build your application - GitLab - Docker executor - TU Delft - GitLab CI/CD YAML syntax reference - GitLab - CI/CD jobs in Docker containers - Semaphore - Introduction to CI/CD pipeline",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/software/automation/ci_cd.html",
    "href": "docs/software/automation/ci_cd.html",
    "title": "Automation for software development",
    "section": "",
    "text": "Continuous Integration (CI) refers to the build and unit testing stages of the software release process. Every committed revision can trigger an automated build and test. With Continuous Delivery (CD), code changes are automatically built, tested, and prepared for a release to production.\n\n\n\nSource: Solidstudio (solidstudio.io). Asset path.\n\n\nYou can implement Continuous Integration and Continuous Delivery (CI/CD) workflows for your research software using either GitHub Actions or GitLab pipelines. Choose the platform that fits your project needs based on factors such as privacy and security needs, collaboration requirements, and available features.\n\n\n\n GitHub Actions\nGitHub Actions workflows and concepts.\n\nLearn more ¬ª\n\n\n\n GitLab pipelines\nGitLab pipelines in TU Delft GitLab.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation"
    ]
  },
  {
    "objectID": "docs/resources/courses.html",
    "href": "docs/resources/courses.html",
    "title": "Courses and workshops",
    "section": "",
    "text": "The TU Delft organises training for researchers on data management, research software development, and open science. For an overview of available courses, please visit the TU Delft Library website.\n\n\n\nThe Carpentries teaches foundational coding and data science skills to researchers worldwide.\n Software Carpentry Lesson  Data Carpentry Lesson\n\nCourses overview\n\n\n\n\nThe Delft Institute for Computational Science and Engineering offers courses on supercomputing through the Delft High Performance Computing Center.\n\nCourses overview\n\n\n\n\nCodeRefinery teaches good practices for writing and maintaining research software, focussed on open source software. Their lessons cover version control, testing, continuous integration, documentation, and more.\n\nUpcoming workshops\n\n\n\n\nThe Research Engineering and Infrastructure Team offers support on software engineering, data science, and high-performance computing. They offer courses on Rust for Research, Python best practices, and Working with a cluster.\n\nCourses overview\n\n\n\n\nThe TU Delft offers a variety of Massive Open Online Courses.\n Open Science  Automated Software Testing  AI, Data & Digitalization  Unix Tools\n\nMOOC overview"
  },
  {
    "objectID": "docs/resources/courses.html#training-for-researchers-at-the-tu-delft",
    "href": "docs/resources/courses.html#training-for-researchers-at-the-tu-delft",
    "title": "Courses and workshops",
    "section": "",
    "text": "The TU Delft organises training for researchers on data management, research software development, and open science. For an overview of available courses, please visit the TU Delft Library website.\n\n\n\nThe Carpentries teaches foundational coding and data science skills to researchers worldwide.\n Software Carpentry Lesson  Data Carpentry Lesson\n\nCourses overview\n\n\n\n\nThe Delft Institute for Computational Science and Engineering offers courses on supercomputing through the Delft High Performance Computing Center.\n\nCourses overview\n\n\n\n\nCodeRefinery teaches good practices for writing and maintaining research software, focussed on open source software. Their lessons cover version control, testing, continuous integration, documentation, and more.\n\nUpcoming workshops\n\n\n\n\nThe Research Engineering and Infrastructure Team offers support on software engineering, data science, and high-performance computing. They offer courses on Rust for Research, Python best practices, and Working with a cluster.\n\nCourses overview\n\n\n\n\nThe TU Delft offers a variety of Massive Open Online Courses.\n Open Science  Automated Software Testing  AI, Data & Digitalization  Unix Tools\n\nMOOC overview"
  },
  {
    "objectID": "docs/resources/courses.html#external-training-opportunities",
    "href": "docs/resources/courses.html#external-training-opportunities",
    "title": "Courses and workshops",
    "section": "External training opportunities",
    "text": "External training opportunities\n\n\n\nThe eScience Center offers regular workshops on good practices for research software development and on intermediate-level topics such as GPU programming, Deep Learning, and Image Processing. Check out the overview of the training materials.\n\nUpcoming workshops\n\n\n\n\nSURF is the IT cooperative of Dutch education and research institutions and offers various workshops on using the supercomputers and storage solutions.\n\nUpcoming events\n\n\n\n\nThe Software Sustainability Institute in the UK provides training and resources to improve the quality of research software.\n\nTraining Hub\n\n\n\n\nTaxila provides an overview of training, learning, and teaching materials for trainers and trainees in the Netherlands.\n\nUpcoming workshops"
  },
  {
    "objectID": "docs/resources/courses.html#community-training-opportunities",
    "href": "docs/resources/courses.html#community-training-opportunities",
    "title": "Courses and workshops",
    "section": "Community training opportunities",
    "text": "Community training opportunities\n\n\n\nAre you learning R or looking for a friendly community to practice with? The R Caf√© is a welcoming space for R users of all levels - from absolute beginners to experienced programmers.\n\nUpcoming events\n\n\n\n\n4TU.ResearchData offers training resources and community engagement to research and research-support professionals working to make their research data FAIR.\n\nUpcoming events"
  },
  {
    "objectID": "docs/infrastructure/vps_ssh.html",
    "href": "docs/infrastructure/vps_ssh.html",
    "title": "Set up SSH tunneling for a VPS",
    "section": "",
    "text": "This guide explains how to set up a secure, single-step Secure Shell (SSH) connection to a Virtual Private Server (VPS) at TU Delft. SSH is a protocol that allows secure remote access to servers over an unsecured network.\nCommonly, connecting to a VPS requires first accessing a Bastion Host (an intermediary server controlling access). Faculty-managed VPS setups at TU Delft can be accessed by two types of bastion hosts: linux-bastion-ex.tudelft.nl having a local /home directory, and linux-bastion.tudelft.nl having access to your own central /home directory. In this sense, linux-bastion-ex.tudelft.nl is a more secure option recommended for the steps below.\nConnecting to a VPS via a bastion host is a two-step process. However, by using SSH tunneling and SSH keys, you will be able to connect directly from your local machine to your VPS. This setup bypasses the need to log in to the bastion host separately, for example, simplifies secure file transfers between your local machine and the VPS.\n\n\n\n\n\n\nImportant Accessing a VPS\n\n\n\nDepending on your geographical location, access to a VPS via SSH may be blocked by the TU Delft firewall. For example, access is usually blocked if you are connecting from your home network. Therefore, before going through the following steps, please check the TU Delft manuals for working remotely for eduVPN and bastion host.\n\n\n\nPrerequisites\nBefore starting, you need:\n\nA TU Delft NetID.\nAccess to a VPS provided by TU Delft ICT. Check this guide for more information on how to request a VPS.\nAn SSH client installed on your local machine. This is usually included in most Linux and macOS distributions via a terminal or shell. For Windows, you can use a third-party SSH client like PuTTY or a Windows Subsystem for Linux (WSL).\n\n\n\nSteps for Linux (including WSL) and macOS\n\n\n\n\n\n\nTip Summary of steps\n\n\n\n\nCreate SSH keys.\nCopy SSH keys to bastion host and remote server.\nCreate a new host for SSH connection.\nTest connection.\n\n\n\n\nSet up SSH tunneling\n\nIf you do not have an SSH key-pair, create one on the local machine. Go to the terminal and enter the following command. Replace &lt;my-keyname&gt; with a name of your choice for the SSH key, e.g., id_rsa or id_ed25519.\n\n$  ssh-keygen -t ed25519 -f ~/.ssh/&lt;my-keyname&gt;\nYou will be prompted to create a passphrase. We recommend you to add one to make the connection more secure. The passphrase will be asked every time you connect to the VPS. To skip the passphrase, press Enter when prompted. You should see something like this:\nGenerating public/private ed25519 key pair.\nEnter passphrase for \"ed25519\" (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in ~/.ssh/&lt;my-keyname&gt;\nYour public key has been saved in ~/.ssh/&lt;my-keyname&gt;.pub\nThe key fingerprint is:\nSHA256:6j06srvun06gJ5UCmD+MVq6RsPuytCO5mF4hTELnWTg root@local-machine\nThe key's randomart image is:\n+--[ED25519 256]--+\n&lt;image cut for security reasons&gt;\n+----[SHA256]-----+\nA private and public key pair will be created in ~/.ssh.\n\n\n\n\n\n\nImportant Accessing .ssh\n\n\n\n~/ in ~/.ssh refers to your /home directory. As stated above, this is different for the two bastion hosts at TU Delft. For linux-bastion-ex.tudelft.nl, it refers to the local /home directory of the bastion host, while for linux-bastion.tudelft.nl, it refers to your central /home directory. . in ~/.ssh refers to a hidden directory. To view hidden files and directories in the terminal, you can use the command ls -a.\n\n\nThe public key is the file with the .pub extension, e.g., &lt;my-keyname&gt;.pub\n\n\n\n\n\n\nTip Tip\n\n\n\nSimilar to passwords, it is advised to rotate your SSH keys regularly (e.g., every 6 months). You can do this by generating a new key pair and replacing the old one on your local machine and VPS.\n\n\n\nLog in to your VPS and, copy the content of your public key into the VPS ~/.ssh/authorized_keys file. You can achieve this by copying the content of the public key file to the clipboard and pasting it into the authorized_keys file on the VPS. Be mindful not to remove anything from this file, as other SSH connections might stop working. Finally, save the file.\nCreate a new host for SSH connection. On your local machine, edit the ~/.ssh/config file and add the following configuration. If the file does not exist, create it.\n\nHost &lt;host-nickname&gt;\n    HostName &lt;target-host&gt;\n    User &lt;target-username&gt;\n    ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\n    IdentityFile ~/.ssh/&lt;my-keyname&gt;\nReplace:\n\n&lt;host-nickname&gt;: a name of your choice for the target host, e.g., my-server.\n&lt;target-host&gt;: the actual name of the target host (FQDM), e.g., server.tudelft.nl.\n&lt;target-username&gt;: the username used to log in to the target host, usually your NetID.\n&lt;bastion-username&gt;: the username used to log in to the bastion server (often same as NetID, but keep separate in case it differs).\n&lt;my-keyname&gt;: the name of the SSH private key you created, e.g., id_rsa. If your private key is stored in a different location, replace the path accordingly.\n\n\nTest the SSH tunneling connection. Connect to the VPS using SSH tunneling by typing the command below. Use your bastion-password when asked. This is usually the password associated with your NetID.\n\n$ ssh &lt;host-nickname&gt;\nIf you encounter problems with the connection, use the debug mode ssh -vvv &lt;host-nickname&gt; to find out what might have gone wrong. This command will provide detailed information about the connection process and can help you troubleshoot any issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Server connection"
    ]
  },
  {
    "objectID": "docs/infrastructure/ssl_certificates.html",
    "href": "docs/infrastructure/ssl_certificates.html",
    "title": "Configure SSL/TLS certificates",
    "section": "",
    "text": "In web servers, SSL/TLS certificates are used to encrypt the communication between a web server and the client (e.g.¬†web browser). In a web server, this is what makes it possible to have HTTPS connections instead of HTTP connections. HTTPS connections are secure, meaning that the data exchanged between the server and the client is encrypted and cannot be intercepted or read by third parties. In order to do this, you need an SSL certificate from a certificate signing authority.\nWhen a web server certificate expires or becomes invalid, a web browser will show you a warning saying it is risky to access a website. This is because the browser cannot verify the identity of the web server, and therefore cannot ensure that the connection is secure. If you are a website owner, it is important to ensure that your SSL certificate is valid and up-to-date to avoid these warnings and maintain the trust of your users.\nThis guide shows how to request and prepare an SSL/TLS certificate from HARICA, a Certification Authority trusted by TU Delft ICT. TU Delft staff can request SSL/TLS certificates using the Academic login option on the HARICA website. You can opt to use Let‚Äôs Encrypt certificates via Certbot instead, which also provides free SSL certificates, but these certificates need to be renewed every 90 days. The HARICA certificates are valid for up to one year.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Web servers",
      "SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/ssl_certificates.html#request-certificate",
    "href": "docs/infrastructure/ssl_certificates.html#request-certificate",
    "title": "Configure SSL/TLS certificates",
    "section": "Request Certificate",
    "text": "Request Certificate\nTo request SSL/TLS certificates via HARICA, follow these steps:\n\nLog in to the HARICA website using your TU Delft credentials.\nFollow the instructions on the HARICA website to create an SSL certificate. You need to do so for the qualified domain name associated with your server (e.g.¬†my-site.tudelft.nl).\n\n\n\n\n\n\n\nImportant Private key and passphrase\n\n\n\nWhen creating an SSL/TLS certificate, HARICA will prompt you to set passphrase and download a private key file. This file is essential for using the SSL/TLS certificate. Ensure that you securely store this private key file and do not forget the passphrase. If either the private key file or the passphrase is lost, you will need to request a new SSL/TLS certificate.\n\n\n\nOnce the new certificates are issued, you will be notified by email. Log in to the HARICA website and download the certificate as a PEM bundle file.\n\n\nPrepare Certificate on the Server\nFor configuring the web server, you typically need two specific files: fullchain.pem and privkey.pem (the private key file you downloaded when requesting the certificate). You need to upload these files to the server, decrypt the private key file, and place them in the appropriate directory with the correct permissions. The instructions below assume you have a remote server running a Linux-based OS, you have SSH access to it, and have set a host nickname.\n\nRename the PEM bundle file as fullchain.pem, and the private key file as privkey.pem. These exact names are not required, but they are common conventions.\nUpload the new fullchain.pem and the privkey.pem using scp to your home directory on the server.\n# From the directory where you downloaded the certificate and private key files:\nscp ./fullchain.pem &lt;host-nickname&gt;:~/ \n\nscp ./privkey.pem &lt;host-nickname&gt;:~/\nLog in to the server via SSH, and check the files have been uploaded correctly:\nssh &lt;host-nickname&gt;\n\nls ~/\n# You should see the files 'fullchain.pem' and 'privkey.pem' listed.\nDecrypt the private key file. Before using it on the web server, you need a decrypted copy of it.\nopenssl rsa -in ~/privkey.pem -out ~/decrypted_privkey.pem \n# You will be prompted to enter the passphrase you set during the initial certificate creation.\n\n\n\n\n\n\n\nNote Encryption algorithms\n\n\n\nThe command above uses the openssl rsa tool to decrypt the private key file. This command works for RSA-encrypted private keys. If your private key uses a different encryption algorithm, consult the OpenSSL documentation. The encryption algorithm is decided when creating the private key during the certificate request.\n\n\n\nCopy the certificate and private key files to the appropriate directory for your web server. This is dictated by the web server you are using.\nsudo cp ~/fullchain.pem &lt;web-server-directory&gt;/fullchain.pem\nsudo cp ~/decrypted_privkey.pem &lt;web-server-directory&gt;/privkey.pem\nSet the correct permissions for the certificate files. This ensures that only the root user can read the private key file, while the fullchain file can be read by others as needed:\nsudo chown root:maintainers &lt;web-server-directory&gt;/fullchain.pem &lt;web-server-directory&gt;/privkey.pem\nsudo chmod 644 &lt;web-server-directory&gt;/fullchain.pem\nsudo chmod 600 &lt;web-server-directory&gt;/privkey.pem\nConfigure the web server to use HTTPS, and restart the web server. The configuration steps depend on the web server you are using (e.g., Apache, Nginx). Consult the documentation for your specific web server for instructions on how to enable HTTPS.\nDelete the temporary certificate files and decrypted private key from your home directory to maintain security:\nrm ~/decrypted_privkey.pem  ~/fullchain.pem ~/privkey.pem",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Web servers",
      "SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/ssl_certificates.html#renew-certificate",
    "href": "docs/infrastructure/ssl_certificates.html#renew-certificate",
    "title": "Configure SSL/TLS certificates",
    "section": "Renew Certificate",
    "text": "Renew Certificate\nSSL/TLS certificates have a validity period, after which they expire and need to be renewed. HARICA certificates are valid for up to one year. To renew your certificate, you need to follow the same steps as requesting a new certificate. Make sure to do this before the current certificate expires to avoid any disruptions in your website‚Äôs HTTPS availability.\nLet‚Äôs Encrypt certificates, on the other hand, are valid for 90 days. If you are using Let‚Äôs Encrypt certificates, consider setting up automatic renewal using tools like Certbot to ensure your certificates are always up-to-date.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nDomain validation and certificate issuance by Let‚Äôs Encrypt\nSSL security protocol by GeeksforGeeks",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Web servers",
      "SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/moving_data.html",
    "href": "docs/infrastructure/moving_data.html",
    "title": "Moving data to remote servers",
    "section": "",
    "text": "This section describes how to transfer data to and from a TU Delft virtual server. The procedure is different depending on whether the server runs a Windows or Linux-based operating system. Although there are many ways to transfer data from one machine to another, TU Delft servers only support a few of them.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Data transfer"
    ]
  },
  {
    "objectID": "docs/infrastructure/moving_data.html#linux-servers",
    "href": "docs/infrastructure/moving_data.html#linux-servers",
    "title": "Moving data to remote servers",
    "section": "Linux servers",
    "text": "Linux servers\nThe scp command is a secure file transfer utility that allows you to copy files between Linux-based hosts (this includes macOS) on a network. It uses SSH for data transfer, providing the same authentication and security as SSH. Common scenarios in which you might need to transfer data between hosts include uploading scripts, downloading results, or transferring configuration files.\n\n\n\n\n\n\nTip Moving large data batches\n\n\n\nIf you need to transfer large files or a multitude of small files, consider using tools like rclone or rsync instead. The instructions below are for transferring small files or a few files at a time.\n\n\n\n\n\n\n\n\nImportant Overwriting files\n\n\n\nNotice that scp will overwrite files in the destination directory without prompting if they already exist. Always double-check the paths and filenames to avoid accidental data loss.\n\n\n\nPrerequisites\nBefore starting, you need:\n\nSCP (Secure Copy Protocol) installed on your local machine. SCP is a command-line utility that allows you to securely transfer files between hosts on a network.\nSSH access to the remote host (e.g., VPS) you want to connect to.\n\n\n\nMoving data via SCP\nTo copy data to and from a remote host using the scp command, you can use the following syntax:\n# Copy TO Remote Host\nscp &lt;path-my-local-file&gt; &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/remote-directory&gt;/\n# Copy FROM Remote Host\nscp &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/my-remote-file&gt; &lt;path-my-local-directory&gt;/\n\n\n\n\n\n\nTip Moving files to restricted directories\n\n\n\nSome directories on the remote host may require elevated permissions to write files. If you encounter a ‚ÄúPermission denied‚Äù error, you may need to use sudo to copy files to those directories. However, using scp with sudo directly is not supported. Instead, you can copy the file to a temporary directory /tmp where you have write access and then move it to the desired location using sudo after connecting to the remote host.\n\n\n\n\nTransferring files using ProxyJump\nIn the case of a VPS hosted by TU Delft, you need to copy data to a remote host via a bastion host (an intermediary server). Therefore, you must use the -o option of scp to specify a ProxyJump that will connect to the bastion host first. Alternatively, you can choose to transfer files using SSH tunneling.\n\nTransfer to remote host\n# If using default SSH key name, for example, id_ed25519 or id_rsa\nscp -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" &lt;path-my-local-file&gt; \\ \n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/remote-directory&gt;/\n\n# If using a custom SSH key name, for example, my_custom_key\nscp  -i &lt;path-to-custom-private-ssh-key&gt; -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\ \n&lt;path-my-local-file&gt;  &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-directory&gt;/\n\n\nTransfer from remote host\n# If using default SSH key name, for example, id_ed25519 or id_rsa\nscp -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\ \n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/\n\n# If using a custom SSH key name, for example, my_custom_key\nscp -i &lt;path-to-custom-private-ssh-key&gt; -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\\n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/\n\n\n\nTransferring files using SSH tunneling\nIf SSH tunneling has been configured correctly for the remote host, you can copy files to and from a remote host as follows:\n# Copy TO remote host\n$ &lt;path-my-local-file&gt; &lt;host-nickname&gt;:&lt;full-path-remote-directory&gt;/\n# Copy FROM remote host\n$ scp &lt;host-nickname&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Data transfer"
    ]
  },
  {
    "objectID": "docs/infrastructure/moving_data.html#windows-servers",
    "href": "docs/infrastructure/moving_data.html#windows-servers",
    "title": "Moving data to remote servers",
    "section": "Windows servers",
    "text": "Windows servers\nTransferring data to and from a TU Delft Windows server is done via the Citrix platform, using the app‚Äôs built-in menu shown in the image below.\n\n\n\nCitrix Menu. Buttons from left to right: Download, Upload, Multimonitor, Clipboard, and Settings.\n\n\n\nTransferring files\n\nOpen a web browser and log into your Windows server as usual via the Citrix portal.\nOpen the Citrix menu located at the center-top of the server window and click on the Upload or Download button, as shown in the image above.\nA pop-up window will open on where you can select the files you wish to transfer to the server.\n\nIt is not possible to directly transfer files to or from the server‚Äôs C: or D: drives. Instead, you upload or download files to your personal TU Delft drive which is connected to the server. Using the Windows File Explorer and standard copy/paste or drag-and-drop operations, you can transfer the data from your personal drive to the server‚Äôs C: or D: drives.\n\n\n\n\n\n\nWarning Warning\n\n\n\nTU Delft Windows servers have a limited amount of disk space in the C: drive. ICT instructs users to install applications and store data in the D: drive of the server to avoid running out of memory. Alternatively, you can use your personal TU Delft drive, which is also connected to the server. More information can be found at the bottom of the TOPdesk form to request a new VPS.\n\n\n\n\nUsing the clipboard\nTU Delft Windows servers do not allow to directly copy or paste text from the clipboard. Instead, you must use Citrix‚Äôs clipboard functionality. To do so, open the Citrix‚Äôs menu located at the center-top of the window and click on the Clipboard button. A pop-up window will open on which you can copy or paste the text you wish to transfer. Note that it is only possible to transfer text via the clipboard; images or files are not supported.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Data transfer"
    ]
  },
  {
    "objectID": "docs/img/licences.html",
    "href": "docs/img/licences.html",
    "title": "",
    "section": "",
    "text": "testing-pyramid.jpg\n\n\n\n\ntesting-pyramid.jpg\n\n\nPermission: ‚ÄúAll I ask is that you provide attribution to me (e.g., credit @SketchingDev and link back to the original source).‚Äù See https://github.com/SketchingDev/sketchingdev.github.io/issues/20#issuecomment-2690689502"
  },
  {
    "objectID": "docs/data/planning/planning.html",
    "href": "docs/data/planning/planning.html",
    "title": "Planning",
    "section": "",
    "text": "When planning your research, it is important to consider how you will manage your data. This is the very first step in the research data lifecycle; you define your research question, and pinpoint relevant data sources. This includes planning how you will collect, store, and share your data, as well as how you will ensure its security and privacy. You formulate a plan for your data management, which is a living document that you can update as your research progresses.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/data/planning/planning.html#data-management-plan",
    "href": "docs/data/planning/planning.html#data-management-plan",
    "title": "Planning",
    "section": "Data management plan",
    "text": "Data management plan\nTU Delft emphasizes the importance of data management planning and asks researchers to create a data management plan (DMP) at the start of their research project. In many cases this is also a formal requirement. TU Delft provides a dedicated tool called DMPonline to help you write a DMP.\n\n\n\n\n\n\nImportant Important\n\n\n\nPlease consult your faculty data steward for more information regarding DMPs and the specific requirements for your faculty.\nAlso, you can find more information on DMPs:\n\nPhD Supervisors guide - Data management plan\nTU Delft RDM page - ‚ÄúData Management Plans‚Äù\nTU Delft RDM page - ‚ÄúDMPonline‚Äù\nRDM 101 course - Module 5 - DMP",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/data/planning/planning.html#what-to-do-if-you-have-sensitive-data",
    "href": "docs/data/planning/planning.html#what-to-do-if-you-have-sensitive-data",
    "title": "Planning",
    "section": "What to do if you have sensitive data?",
    "text": "What to do if you have sensitive data?\nIf your datasets contains personal, confidential, or otherwise sensitive information, you need to consider how to handle this data appropriately.\n\n\n\n Privacy\nHandle sensitive data.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/data/fair_data/fair.html",
    "href": "docs/data/fair_data/fair.html",
    "title": "FAIR data",
    "section": "",
    "text": "The goal of the FAIR principles is to improve research transparency, reproducibility and reusability. FAIR data enhances research reliability, impact, and visibility, creating new collaboration opportunities for researchers. The acronym FAIR stands for:\n\nFindable: Data should be findable by users. A straightforward and reliable way to achieve this is by depositing data in a repository with appropriate metadata, tags, and identifiers to improve searchability.\nAccessible: Data should be accessible to authorized users. This does not mean all data must be publicly available; rather, it should be ‚Äúas open as possible, as closed as necessary‚Äù. If data cannot be made publicly available due to sensitive information or commercial restrictions, the metadata should still be made public to indicate where and how the data can be accessed if needed.\nInteroperable: Data should be in standardized formats with a clear structure to allow it to be interoperable across different systems, enabling data to be used in various applications by both the data owners and other users.\nReusable: For data to be reusable, users must understand what the data represents, the information it contains, and how to interpret its structure and format. Good documentation and an appropriate license are key in reusability, as these enable others to understand and work with your dataset, especially upon publication.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "FAIR Data"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage.html",
    "href": "docs/data/data_storage/storage.html",
    "title": "Data storage",
    "section": "",
    "text": "In the context of research data management, ‚ÄúData Storage‚Äù refers to securely housing research data on various digital media or infrastructure, having it readily accessible for ongoing work, analysis, and collaboration throughout the active phase of a research project. It involves the systems and practices used to maintain the availability, integrity, and accessibility of data while it is actively being used, processed, or shared by the research team. This includes both local and networked solutions, serving as the persistent layer for active research operations involving collaborators internal to TU Delft, or external.\nData storage, rather than being confined to a single step, functions as a persistent foundation, supporting activities from initial data acquisition through active analysis. As a researcher, you may frequently move between phases, such as returning to data collection after initial analysis reveals gaps, or refining processing methods based on analytical outcomes. This interconnected and iterative nature is crucial for effective research data management, as it means that planning for later stages, like archiving and publishing, must occur early, and ongoing management activities, such as storage and security, are not isolated tasks but integral to every step of active research.\nThis section details the recommended storage solutions available to TU Delft researchers, emphasizing institutional and national (Dutch/European) services and providing guidance on their appropriate use. It also highlights the importance of data security, sharing, and backup as essential components of effective data storage practices.\n\n\n\n Storage options\nLocal and networked, institutional and national.\n\nLearn more ¬ª\n\n\n\n Data security\nEnsuring confidentiality, integrity, and availability.\n\nLearn more ¬ª\n\n\n\n Data sharing\nMaking active data available to internal and/or external collaborators.\n\nLearn more ¬ª\n\n\n\n Data backup\nStrategies to prevent data loss.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage"
    ]
  },
  {
    "objectID": "docs/data/data_storage/security.html",
    "href": "docs/data/data_storage/security.html",
    "title": "Data security",
    "section": "",
    "text": "Data security is a critical aspect of research data management, ensuring that sensitive information is protected from unauthorized access, loss, or corruption. It involves implementing measures to safeguard data throughout its lifecycle, from collection and storage to sharing and archiving.\nTU Delft implements robust authentication and authorization mechanisms, such as strong passwords and two-factor authentication, to restrict data access only to approved individuals and systems.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data security"
    ]
  },
  {
    "objectID": "docs/data/data_storage/security.html#core-principles-of-research-data-security",
    "href": "docs/data/data_storage/security.html#core-principles-of-research-data-security",
    "title": "Data security",
    "section": "Core principles of research data security",
    "text": "Core principles of research data security\nKey principles include:\n\nAvailability: Ensuring that authorized users can access the data and associated systems when needed for their research activities.\nConfidentiality: Ensuring that data is accessible only to individuals who are explicitly authorized to view or use it. ¬†\nIntegrity: Maintaining the accuracy, completeness, and consistency of data throughout its entire lifecycle, preventing unauthorized modification. Regularly backing up data to prevent loss due to accidental deletion, hardware failure, or cyberattacks, and having a recovery plan in place to restore data in case of incidents.\n\n\n\n\n\n\n\n Tip: Explore our Data backup guide for more information.\n\n\n\n\nVerifying data integrity\nA checksum is a small-sized block of data derived from another block of digital data. By comparing the checksum of a file you have downloaded, backed-up, or transferred with the one provided by the source, you can verify if the file is an exact and untampered copy.\n\nStep 1: Obtain an original checksum\nBefore you can verify a file, you need an original checksum value. This is typically provided by a website or source from where you downloaded the file. It could also have been generated by yourself from Step 2. It usually is a long string of alphanumeric characters and is often labeled as MD5, SHA-1, or SHA-256.\nFor example, you might see something like this on a download page:\nSHA-256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\n\nStep 2: Generate a checksum for your file\nYou can use different command-line tools to calculate the checksum of the file you have downloaded, transferred or backed-up.\n\nWindows PowerShellmacOS terminalLinux terminal\n\n\nOpen PowerShell (press Win + X and select ‚ÄúPowerShell‚Äù).\nUse the Get-FileHash command. By default, it uses SHA-256.\nGet-FileHash C:\\path\\to\\your\\file\nTo use a different algorithm like MD5, specify it:\nGet-FileHash C:\\path\\to\\your\\file -Algorithm MD5\n\n\nOpen your terminal (you can find it in Applications &gt; Utilities).\nFor SHA-256, use the shasum -a 256 command:\nshasum -a 256 /path/to/your/file\nFor MD5, use the md5 command:\nmd5 /path/to/your/file\n\n\nOpen your terminal.\nFor SHA-256, use the sha256sum command:\nsha256sum /path/to/your/file\nFor MD5, use the md5sum command:\nmd5sum /path/to/your/file\n\n\n\n\n\nStep 3: Compare the checksums\nFinally, compare the checksum you generated in Step 2 with the original checksum from Step 1. There might be cases where you need to do Step 2 in two different systems, such as a local machine and a remote server.\nIf they match exactly, your file is a perfect copy.\nIf they do not match, the file was likely corrupted during download or has been tampered with. You should delete the file and download it again.\n\n\n\nOther considerations\nOur Data privacy guide has a list of TU Delft privacy related resources that should be helpful if you are working with highly sensitive data, such as personal information, health data, or copyrighted material.\n\n\n\n\n\n\nImportant Important\n\n\n\n\nReach out to your faculty data steward for more information.\nExplore TU Delft security and privacy related resources:\n\nTU Delft Security and Privacy SharePoint site",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data security"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_mounting.html",
    "href": "docs/data/data_storage/project_drive_mounting.html",
    "title": "Mount Project Drive on server",
    "section": "",
    "text": "Project drive storage can be mounted and made accessible in your TU Delft Virtual Private Server (VPS).\n\nPrerequisites\n\nA TU Delft VPS\nA TU Delft Project Data Storage drive\n\n\n\nSteps\n\nLocate the URL of your project storage.\nConnect to your TU Delft VPS via SSH.\nCreate a new directory as a mounting point.\nRetrieve your Linux user and group details.\nEdit the fstab file to include project storage technical details.\nMount the project drive.\n\n\nStep 1. Locate the URL of your project storage\nThe URL for your project drive can be obtained from either:\n\nthe email from TU Delft ICT with the confirmation of your project drive request, or\nby using a web browser to navigate into https://webdata.tudelft.nl/\n\n\n\n\n\n\n\nNote Note\n\n\n\nhttps://webdata.tudelft.nl/ can only be accessed on campus or using eduVPN\n\n\n\nChrome/Brave/Firefox/Vivaldi/SafariEdge\n\n\n\nNavigate into https://webdata.tudelft.nl/staff-umbrella/\n\nA pop-up should appear asking for your username and password.\n\nProvide your NetID in the username field, provide your password accordingly.\n\nYou should now see a list of your project drives.\n\nClick on the project drive of your choice.\n\nCopy everything after ‚Äúhttps://webdata.tudelft.nl/‚Äù\n\n\n\nContent within webdata is under password protection. Typing your username and password is only possible with a pop-up message which is disabled in an Edge browser on systems managed by TU Delft.\n\n\n\n\n\nStep 2. Connect to your TU Delft VPS via SSH\nFollow instructions in the TU Delft ICT email from initial server setup or follow our guide to configure via SSH.\n\n\nStep 3. Create a new directory as the mounting point\nThe convention is to create mounting points in the folder /media. Navigate to the directory and create a new folder with:\ncd /media\nmkdir &lt;server_mount_point&gt;\nReplace &lt;server_mount_point&gt; with the name of your choice. This will be the name of the folder where your project drive will be mounted.\n\n\nStep 4. Find and save your user and group details\nIn the terminal, you can retrieve your local user and group details with:\nid -u &lt;your_netID&gt; # User ID\nid -g &lt;your_netID&gt; # Group ID\nYou may need the values for uid and gid for step 5.\n\n\n\n\n\n\nNote Note\n\n\n\nThese commands are server-specific, so make sure to execute them on the server where the project drives will be mounted.\n\n\n\n\nStep 5. Edit the fstab file to include project storage technical details\nThe fstab file contains a list of the addresses of external file systems. In this file, the details of your project drive will need to be added in a single line. This line consists of four parts:\n\nfilesystem - the address of the project drive\nmount point - the location in the VPS where you want to mount the project drive\ntype - the type of the filesystem\noptions - additional option such as user privileges\n\nThe fstab file must be in the /etc/ directory and can be opened with the vi or nano editor:\n\nvinano\n\n\nIn the terminal, enter the following command to open the fstab file in the vi editor:\nsudo vi /etc/fstab\nThen, switch to the insert mode (hit ‚Äúi‚Äù to switch to insert mode and be able to type)\n\n\nIn the terminal, enter the following command to open the fstab file in the nano editor:\nsudo nano /etc/fstab\n\n\n\nAdd the following line to the file:\n&lt;your_netID&gt;@sftp.tudelft.nl:/staff-umbrella/&lt;project_drive_name&gt;  /media/&lt;server_mount_point&gt; fuse.sshfs  rw,noauto,users,_netdev  0  0\nreplacing the values between &lt; and &gt; with your NetID, the name of your project drive, and the name of the folder you created in step 3.\n\n\n\n\n\n\nNote Note\n\n\n\nIf this configuration throws a permission error during mounting, try:\n//tudelft.net/staff-umbrella/&lt;project_drive_name&gt;/ /media/&lt;server_mount_point&gt; cifs username=&lt;your_netID&gt;,noauto,uid=&lt;your_uid&gt;,gid=&lt;your_gid&gt;,forcegid,rw,_netdev\nUse the values for uid and gid from step 4.\n\n\nClose the file editor and save the changes:\n\nvinano\n\n\nUse Control+C followed by :wq to save the file and close it to get back to your terminal.\n\n\nAs indicated by the nano interface, use Control+O to write out the file. Then, confirm your choice of filename by hitting Enter. Finally, exit the file with Control+X\n\n\n\n\n\nStep 6. Mount the project drive\nTo mount the project drive, execute the command:\nsudo mount /media/&lt;server_mount_point&gt;\nYou can also unmount the drive with:\nfusermount -u /media/&lt;server_mount_point&gt;\nThe project drive will not mount automatically, so you will need to remount it manually each time you restart the server.\n\n\n\n\n\n\nNote Note\n\n\n\nIf the step above does not work, it probably means that the packages for mounting cifs-type filesystems haven‚Äôt been installed. Depending on your Linux flavour you will need to install them using:\n\nUbuntu/DebianRedhat/Centos/Fedora\n\n\nsudo apt install cifs-utils\n\n\nsudo yum install cifs-utils\n\n\n\n\n\n\n\n\nNotes and next steps\nThe steps above can also be used to mount any storage offered by TU Delft with a WebDav link (staff-homes, staff-groups, staff-bulk, student-homes, student-groups and apps). Simply use the latter half of the URL from the WebDav web link of your storage drive, which will change from staff-umbrella (Project Data Storage drive) to something else, depending on the storage drive you would like to mount.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Mount *Project Drive* on server"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html",
    "href": "docs/data/data_publishing/publishing.html",
    "title": "Publishing data",
    "section": "",
    "text": "Data repositories are online locations for storing research data long term. When data is published in a repository, it is (often) assigned a persistent digital object identifier (DOI), which ensures that the data is discoverable and accessible over time. When published, the data should be accompanied by an appropriate license and metadata describing your data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html#tu.researchdata",
    "href": "docs/data/data_publishing/publishing.html#tu.researchdata",
    "title": "Publishing data",
    "section": "4TU.ResearchData",
    "text": "4TU.ResearchData\nChoosing a repository for publishing your data depends on factors such as storage costs, funding or publication requirements, and security needs. TU Delft provides an in-house data repository, 4TU.ResearchData, supporting versioning and a GitHub remote as part of their services. Additionally, when publishing your data, your submission is reviewed to ensure that repository requirements are met. You will receive a DOI for your dataset, which can be used to cite your data in publications. All data is stored in the Netherlands, with two locations in Delft and a backup in Leiden.\n\n\n\n\n\n\nNote Learn more\n\n\n\nGetting started with 4TU.ResearchData",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html#zenodo",
    "href": "docs/data/data_publishing/publishing.html#zenodo",
    "title": "Publishing data",
    "section": "Zenodo",
    "text": "Zenodo\nDeveloped by CERN, Zenodo is an open repository that allows researchers to upload and share their research outputs, including datasets, publications, and software. Similarly to 4TU.ResearchData you will receive a DOI for your dataset. Zenodo supports versioning and has GitHub integration.\n\n\n\nSelecting a data repository. The Turing Way Community. This illustration is created by Scriberia with The Turing Way community, used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\nRegardless of the repository you choose, it is important to ensure that your data is properly documented. This includes describing the content, structure, and context of your data, as well as any relevant documentation that is needed to understand and use the data.\n\n\n\n\n\n\nNote Learn more\n\n\n\nFor a more comprehensive list of repositories and how to select one, you can check the Turing Way book or the TU Delft Library Guidelines",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html",
    "href": "docs/data/data_publishing/licensing_data.html",
    "title": "Data licensing",
    "section": "",
    "text": "When depositing data, selecting an appropriate license cannot be overlooked, as it defines the permissions and restrictions for others using your data. Clear labelling of licensing terms ensures that data can be shared and reused legally and ethically.\nSimilarly to software licenses, data licenses can be divided depending on their restrictiveness. The most common licenses are Creative Commons (CC) licenses, which are widely used for data and other creative works. These licenses can be categorized into permissive and restrictive types.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html#cc-licenses",
    "href": "docs/data/data_publishing/licensing_data.html#cc-licenses",
    "title": "Data licensing",
    "section": "CC licenses",
    "text": "CC licenses\nPermissive licenses allow for broad reuse with minimal restrictions, while restrictive licenses impose limitations on how the data can be used. CC licenses are further divided into the following categories:\n\nPublic Domain (CC0): No restrictions, data can be used for any purpose.\nAttribution (CC-BY): Requires attribution to the original creator, allows for sharing and adaptation with appropriate citations.\nAttribution-ShareAlike (CC-BY-SA): Requires attribution and allows for derivatives under the same license.\nNon-Commercial (CC-BY-NC): Allows use for non-commercial purposes only.\nNo Derivatives (CC-BY-ND): No derivatives allowed.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCreative Commons Licenses",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html#other-data-licenses",
    "href": "docs/data/data_publishing/licensing_data.html#other-data-licenses",
    "title": "Data licensing",
    "section": "Other data licenses",
    "text": "Other data licenses\nIn addition to CC licenses, Open Data Commons provides a set of licenses that are specifically designed for data. These licenses are similar to CC licenses but are tailored for data and databases.\n\nPublic Domain Dedication and License (PDDL): Similar to CC0, it allows for unrestricted use of the data.\nOpen Data Commons Attribution License (ODC-BY): Requires attribution to the original creator, allows for sharing and adaptation with appropriate citations.\nOpen Data Commons Open Database License (ODbL): Allows for sharing, modification, and use of the database, but requires attribution and share-alike for derivatives. Similar to CC-BY-SA, but specifically designed for databases.\nRestrictive Licenses: Limits usage, often prohibiting commercial use or modifications.\n\nWhile Creative Commons and Open Data Commons license might seem similar, the difference between them is that CC licenses are more general and can be applied to any type of work, while Open Data Commons licenses are specifically designed for data and databases and therefore cover different rights.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nTuring Way - Data Licenses",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_processing.html",
    "href": "docs/data/data_processing.html",
    "title": "Data processing",
    "section": "",
    "text": "Data processing is a broad term and is often conflated with related terms. It typically includes steps like data cleaning and data transformation. Some workflows may extend this to preliminary analysis, though visualizations are generally considered a separate stage. The term data wrangling is frequently used interchangeably with early processing stages. The specific steps involved can vary depending on the type of data and the goals of the analysis.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Data processing"
    ]
  },
  {
    "objectID": "docs/data/data_processing.html#data-versioning",
    "href": "docs/data/data_processing.html#data-versioning",
    "title": "Data processing",
    "section": "Data versioning",
    "text": "Data versioning\nData versioning is the process of assigning unique version numbers to different iterations of a dataset. This practice is important when working with machine learning models, as it allows to track changes made to the data over time and ensure that the correct version of the data is used for training and testing. Data versioning can also help with reproducibility, because it allows to recreate previous versions of the data and compare results across different iterations.\nThere are a couple of techniques and tools for data versioning. For example, Git is typically used to track source code and software versions, but is not well-suited for large datasets. However, Git LFS (Large File Storage) can be used to manage large files in a Git repository. Other tools like DVC (Data Version Control) are specifically designed for data versioning and can be used to track changes to datasets and models over time. DVC integrates with Git and allows to version control data files, models, and experiments in a way that is similar to how Git works for source code.\n\nDVC\nDVC has very comprehensive documentation, tutorials and videos available, most of the resources are available on their homepage. It covers how to get started with data versioning, how to integrate it with Git, and how to use DVC with different cloud storage providers (e.g.¬†AWS S3).",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Data processing"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html",
    "href": "docs/data/data_collection/data_conventions.html",
    "title": "Data conventions",
    "section": "",
    "text": "By adhering to domain-aligned data conventions, you can enhance the quality and reproducibility of your work, promote collaboration, data sharing and reuse. While flexibility exists to accommodate the specific needs of your research project, it is important to establish a consistent framework for data collection and management. This section outlines some key considerations for data conventions, including naming conventions, folder structure, and file standards.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#data-conventions",
    "href": "docs/data/data_collection/data_conventions.html#data-conventions",
    "title": "Data conventions",
    "section": "Data conventions",
    "text": "Data conventions\nEstablishing consistent naming conventions and metadata practices for your data files is essential for long-term project maintainability. Similarly, as mentioned in the Project structure guide in the Research Software section, some essential principles apply, like:\n\nuse descriptive, concise and consistent names\navoid special characters and spaces (instead use underscores _ or hyphens -)\nfollow ISO 8601 date formatting (YYYY-MM-DD)\nthe order matters - order elements in the name from least to most specific\ninclude versioning in the name (where applicable)\n\nHowever, the specific conventions you choose should align with the needs of your research project, collaborators, and established practices within your field.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#folder-structure",
    "href": "docs/data/data_collection/data_conventions.html#folder-structure",
    "title": "Data conventions",
    "section": "Folder structure",
    "text": "Folder structure\nMaintaining a consistent folder structure across your research group and project phases prevents confusion, reduces errors in data analysis, and significantly improves research reproducibility. Remember that folder structure should complement your file naming (data) conventions to create a cohesive data organization system. Some practices to consider include:\n\nUse a consistent and descriptive naming convention for folders, similar to file naming conventions\nAvoid using spaces or special characters in folder names\nCreate a hierarchical structure that moves from general to specific, making navigation intuitive\nEstablish consistent naming patterns across all levels of your folder hierarchy\nSeparate raw and processed data into distinct folders to preserve original data integrity\nLimit folder nesting to 3-5 levels to prevent excessive complexity\nGroup related files into logical categories rather than creating folders for individual files\nDocument your folder structure in your data management plan for reference\nFor time-series or versioned data, consider incorporating date-based folder organization\nConsider adding a README file in your data/ folder to provide an overview of the folder structure and its contents",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#file-standards",
    "href": "docs/data/data_collection/data_conventions.html#file-standards",
    "title": "Data conventions",
    "section": "File standards",
    "text": "File standards\nFiles commonly use standardised formats depending on their knowledge domain, and/or the platfrom through which they are made available. For example, FASTA format files are widely used in bioinformatics workflows for sequence alignment, database searches, genome assembly and more. Several major biological databases and tooling use and support the FASTA format, like NCBI, EMBL-EBI, and UniProt.\nIn the field of geosciences, the NetCDF format is widely used for array-oriented scientific data. It is particularly useful for storing multidimensional data such as temperature, pressure, and humidity in a single file. NetCDF files are self-describing and can be compressed to save space, making them ideal for large datasets.\nIn the field of engineering, data is often stored in formats such as HDF5. It is widely used for storing large amounts of numerical data and are supported by many programming languages and software tools.\nIn addition, 4TU.ResearchData has a list of preferred file formats, and they have a data collection policy that outlines the preferred file formats for data submission.\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nThe FAIR Guiding Principles for scientific data management and stewardship",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/access_reuse.html",
    "href": "docs/data/data_collection/access_reuse.html",
    "title": "Access and reuse",
    "section": "",
    "text": "Access and reuse depends on a variety of factors, including the format in which your data is stored, the tools used to create and manage it, and the policies governing its use. The key consideration is planning who, and under what conditions, can access your data.\n\n\n\n\n\n\nImportant Learn more\n\n\n\nAgain, we would like to refer you to modules from the RDM 101 course and links from the PhD Supervisors guide for more information on this topic:\n\nRDM 101 - Module 3 - Access to Data\nRDM 101 - Module 4 - Data access and Data publication\nPhD Supervisors guide - Discover & Reuse\n\n\n\nTo ensure that your data can be accessed and reused in the future, consider the following practices:\n\nChoose the right format: Opting for widely accepted, non-proprietary, and open formats ensures that your data remains accessible and usable over time, even as technology evolves. Formats such as CSV for tabular data, TXT or Markdown for text, and PNG for images are examples of formats that are more likely to be supported in the long term. This increases the likelihood that your data will be accessible in the future, regardless of changes in technology or software.\nDocument your data: Documenting the context and structure of your data to enhance its reusability. Providing metadata, clear file naming conventions, and a README file explaining the dataset can significantly improve the chances of your data being understood and reused by others, including your future self.\nFollow data management best practices: Implement data management best practices, such as version control, regular backups, and secure storage. This will help ensure that your data remains accessible and usable over time.\nConsider data sharing policies: Be aware of any data sharing policies or regulations that may apply to your data. This includes understanding the rights and permissions associated with your data, as well as any restrictions on its use or distribution.\n\n\n\n\n\n\n\n Tip: Explore our Data sharing guide for more information.\n\n\n\n\nPlan for long-term storage: Consider how your data will be stored and accessed in the long term. This may involve using cloud storage solutions, institutional repositories, or other platforms that provide reliable access to your data over time.\n\n\n\n\n\n\n\n Tip: Explore our section Data storage for more information on data storage options.\n\n\n\n\nArchive in a trusted repository: Deposit your data in a trusted data repository. Repositories like 4TU.ResearchData provide long-term storage, preservation, and access services, making it easier for others to find and reuse your data.\n\n\n\n\n\n\n\n Tip: Explore our section Archive and publish for information on archiving your research data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Access and reuse"
    ]
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About these guides",
    "section": "",
    "text": "The DCC Guides are a curated, well-maintained online resource tailored for research support staff and researchers at TU Delft. They provide guidance on data management, research software development, and computing practices. By incorporating best practices from both internal and external sources, the guides will offer a centralized, comprehensive collection of solutions and resources specific to the TU Delft research environment.\nThe DCC Guides support research support staff by:\n\nEnabling the collection, curation, and sharing of developed solutions and relevant resources.\nPromoting a consistent, unified approach to delivering research support.\n\nThe DCC Guides support researchers by:\n\nHelping them understand the key challenges and requirements of digital research.\nGuiding researchers towards compliance with TU Delft policies on data management and research software.\nProviding standalone documentation for resolving digital research challenges independently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "docs/about.html#purpose-and-benefits",
    "href": "docs/about.html#purpose-and-benefits",
    "title": "About these guides",
    "section": "",
    "text": "The DCC Guides are a curated, well-maintained online resource tailored for research support staff and researchers at TU Delft. They provide guidance on data management, research software development, and computing practices. By incorporating best practices from both internal and external sources, the guides will offer a centralized, comprehensive collection of solutions and resources specific to the TU Delft research environment.\nThe DCC Guides support research support staff by:\n\nEnabling the collection, curation, and sharing of developed solutions and relevant resources.\nPromoting a consistent, unified approach to delivering research support.\n\nThe DCC Guides support researchers by:\n\nHelping them understand the key challenges and requirements of digital research.\nGuiding researchers towards compliance with TU Delft policies on data management and research software.\nProviding standalone documentation for resolving digital research challenges independently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "docs/about.html#audience",
    "href": "docs/about.html#audience",
    "title": "About these guides",
    "section": "Audience",
    "text": "Audience\nThe DCC Guides are primarily designed for the DCC and TU Delft research support staff, with researchers as a secondary audience. Usability for support processes is a core focus. Success is assessed through increased user engagement (measured by feedback and site analytics) and improved support outcomes (evaluated internally).\n\n1. DCC team members\nAs the main contributors and maintainers, the DCC team is the primary audience. A key goal is to document and preserve developed solutions for reuse, avoiding redundant effort. The team is responsible for regularly updating the guides, integrating user feedback, and ensuring content quality.\n\n\n2. TU Delft research support staff\nThis group includes Faculty Data Stewards, ICT Innovation, and the Library‚Äôs Research and Data Services team. These staff members can use the guides as a trusted resource to support researchers more effectively and stay up to date with best practices.\n\n\n3. TU Delft researchers\nResearchers can benefit from easy access to current, reliable information to support their research workflows. The guides are not intended as a complete training pathway and will link to trustworthy external resources where appropriate.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have tips, ideas, or questions related to Research Computing, Research Data, and Research Software at TU Delft?\nAre you a researcher interested in these topics?\nDo you work and collaborate with researchers on these topics?\n\nWe use GitHub Discussions as a place to connect with other members of our community. We hope that you:\n\nAsk questions you‚Äôre wondering about.\nShare ideas (and solutions).\nEngage with other community members.\n\n\n\n\nPlease be welcoming and open‚Äëminded. This is a community we build together. See our Code of Conduct for more information.\n\n\n\nDo you have questions you‚Äôd like to ask? Would you like to point to specific resources and potential solutions or ideas? Would you like to contribute to the development of these guides? Select the Discussions category that best matches your situation:\n\nUse the Q&A to ask a question on a specific topic.\nUse Ideas to propose new features, topics, or improvements.\nUse Solutions to share worked examples, guidelines, or solutions. Strong posts in this category may be integrated into the DCC guides by maintainers.\nYou can also use Polls to gather opinions or vote on a change or idea.\nLastly, you can use the section Docs draft box to simply share plain‚Äëtext drafts or outlines for new or updated guides. Maintainers can review, edit, and integrate these into the DCC guides. Any contribution is appreciated.\n\nConsider watching or subscribing to categories of interest in Discussions to follow updates and replies.\n\n\nIf you want to propose a change to the content of the guides, you can do so by either:\n\nOpening a new issue describing the change you would like to see.\n\nThis is the preferred way for small changes, typos, or if you are unsure about how to proceed.\nMaintainers will either implement the change themselves or help you to integrate your proposed changes. Maintainers may convert an issue to a Discussion if that‚Äôs a better fit.\n\nOpening a discussion in a relevant category (see above).\n\nMaintainers can help you to integrate your proposed changes.\n\nCreating a pull request with the proposed changes.\n\nThis would be the ideal way to propose changes (and if you are familiar with the steps described below).\nInclude a short summary and links to any related discussion/issue, and keep the pull request focused on a single topic.\nOpen it as a draft if you want early feedback.\n\n\n\n\n\n\nIf you wish to contribute to the development of these guides directly, please follow the instructions below.\n\n\n\nFork the repository to your own GitHub profile.\nClone the repository.\nNavigate to the root of this repository in your terminal.\nInstall Quarto if you don‚Äôt already have it installed on your machine. You can find the installation instructions here.\n\nAlternatively, install Quarto within a virtual environment using the environment.yml file by:\n\nRunning conda env create -f environment.yml in the terminal to create a conda environment with Quarto pre-installed.\nActivating the environment by running conda activate dcc_guides.\n\n\n\nRun quarto preview in your terminal.\nYou will see the rendered version in a browser window.\n\n\n\n\n\nIn your forked repository, either commit a new change to the repository to trigger the build action or manually trigger the action.\n\nTo manually trigger the action, go to Actions ‚Üí Quarto Publish Guides and press Run workflow and again Run workflow.\n\nIn your forked repository, under Settings ‚Üí Pages set Source to gh-pages and /(root) and press Save.\n\n\n\n\n\nMake sure your changes are committed and pushed to your forked repository.\n\nNote: While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes regularly.\n\nGo to the original repository where you want to propose the changes.\nClick on the ‚ÄúPull requests‚Äù tab.\nClick on the ‚ÄúNew pull request‚Äù button.\nSelect the branch from your forked repository that contains your changes.\nReview the changes and add a descriptive title and comment for your pull request.\nClick on the ‚ÄúCreate pull request‚Äù.\nWait for maintainers to review your pull request. They may provide feedback or request changes before merging it.\n\n\n\n\n\nFor new guide pages, please copy the Quarto front matter template into the top of your .md file.\nMaintainers will add it if omitted.\n\n\n\n\n\n\nMaintainers will review contributions for scope and fit with the site‚Äôs content\nContent may be edited for style, structure, and metadata to ensure consistency across the guides."
  },
  {
    "objectID": "CONTRIBUTING.html#welcome",
    "href": "CONTRIBUTING.html#welcome",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have tips, ideas, or questions related to Research Computing, Research Data, and Research Software at TU Delft?\nAre you a researcher interested in these topics?\nDo you work and collaborate with researchers on these topics?\n\nWe use GitHub Discussions as a place to connect with other members of our community. We hope that you:\n\nAsk questions you‚Äôre wondering about.\nShare ideas (and solutions).\nEngage with other community members."
  },
  {
    "objectID": "CONTRIBUTING.html#code-of-conduct",
    "href": "CONTRIBUTING.html#code-of-conduct",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Please be welcoming and open‚Äëminded. This is a community we build together. See our Code of Conduct for more information."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-participate",
    "href": "CONTRIBUTING.html#how-to-participate",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have questions you‚Äôd like to ask? Would you like to point to specific resources and potential solutions or ideas? Would you like to contribute to the development of these guides? Select the Discussions category that best matches your situation:\n\nUse the Q&A to ask a question on a specific topic.\nUse Ideas to propose new features, topics, or improvements.\nUse Solutions to share worked examples, guidelines, or solutions. Strong posts in this category may be integrated into the DCC guides by maintainers.\nYou can also use Polls to gather opinions or vote on a change or idea.\nLastly, you can use the section Docs draft box to simply share plain‚Äëtext drafts or outlines for new or updated guides. Maintainers can review, edit, and integrate these into the DCC guides. Any contribution is appreciated.\n\nConsider watching or subscribing to categories of interest in Discussions to follow updates and replies.\n\n\nIf you want to propose a change to the content of the guides, you can do so by either:\n\nOpening a new issue describing the change you would like to see.\n\nThis is the preferred way for small changes, typos, or if you are unsure about how to proceed.\nMaintainers will either implement the change themselves or help you to integrate your proposed changes. Maintainers may convert an issue to a Discussion if that‚Äôs a better fit.\n\nOpening a discussion in a relevant category (see above).\n\nMaintainers can help you to integrate your proposed changes.\n\nCreating a pull request with the proposed changes.\n\nThis would be the ideal way to propose changes (and if you are familiar with the steps described below).\nInclude a short summary and links to any related discussion/issue, and keep the pull request focused on a single topic.\nOpen it as a draft if you want early feedback."
  },
  {
    "objectID": "CONTRIBUTING.html#for-developers",
    "href": "CONTRIBUTING.html#for-developers",
    "title": "Contributing guidelines",
    "section": "",
    "text": "If you wish to contribute to the development of these guides directly, please follow the instructions below.\n\n\n\nFork the repository to your own GitHub profile.\nClone the repository.\nNavigate to the root of this repository in your terminal.\nInstall Quarto if you don‚Äôt already have it installed on your machine. You can find the installation instructions here.\n\nAlternatively, install Quarto within a virtual environment using the environment.yml file by:\n\nRunning conda env create -f environment.yml in the terminal to create a conda environment with Quarto pre-installed.\nActivating the environment by running conda activate dcc_guides.\n\n\n\nRun quarto preview in your terminal.\nYou will see the rendered version in a browser window.\n\n\n\n\n\nIn your forked repository, either commit a new change to the repository to trigger the build action or manually trigger the action.\n\nTo manually trigger the action, go to Actions ‚Üí Quarto Publish Guides and press Run workflow and again Run workflow.\n\nIn your forked repository, under Settings ‚Üí Pages set Source to gh-pages and /(root) and press Save.\n\n\n\n\n\nMake sure your changes are committed and pushed to your forked repository.\n\nNote: While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes regularly.\n\nGo to the original repository where you want to propose the changes.\nClick on the ‚ÄúPull requests‚Äù tab.\nClick on the ‚ÄúNew pull request‚Äù button.\nSelect the branch from your forked repository that contains your changes.\nReview the changes and add a descriptive title and comment for your pull request.\nClick on the ‚ÄúCreate pull request‚Äù.\nWait for maintainers to review your pull request. They may provide feedback or request changes before merging it.\n\n\n\n\n\nFor new guide pages, please copy the Quarto front matter template into the top of your .md file.\nMaintainers will add it if omitted."
  },
  {
    "objectID": "CONTRIBUTING.html#notes-on-review-and-integration",
    "href": "CONTRIBUTING.html#notes-on-review-and-integration",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Maintainers will review contributions for scope and fit with the site‚Äôs content\nContent may be edited for style, structure, and metadata to ensure consistency across the guides."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the DCC Guides!",
    "section": "",
    "text": "These guides provide a starting point for Research Software, Research Data, and Research Computing at TU Delft. This is an initiative from the TU Delft Digital Competence Centre.\n Shared solutions for research support staff  Practical guides for researchers\n\n\n   Research Software \n   Data Management \n   Computing Infrastructure \n   Courses and Workshops \n\n\n\n\n\n\n\n\n Want to get involved? \n Join the community We welcome anyone to join us in improving our guides! Find out how in our contributing guide.\n Join the discussion We welcome community discussions, ideas, and general questions to develop solutions and receive feedback in our community forum.\n Open an issue We track topic requests and bug-reports via GitHub issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "<span style=\"filter:grayscale(100%);\">‚åÇ</span> **Home**"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dcc@tudelft.nl. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dcc@tudelft.nl. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "docs/community.html",
    "href": "docs/community.html",
    "title": "Community",
    "section": "",
    "text": "Teams channel - DCC Community\nData Stewards\nOpen Science Program\nOpen Science Community\nDelft HPC\nData Champions\nICT Innovation\nStatistical Helpdesk\n\n\n\n\n4TU.ResearchData is an international data repository for science, engineering and design. Its services include curation, sharing, long-term access and preservation of research datasets. These services are available to anyone around the world. In addition, 4TU.ResearchData also offers training and resources to researchers to support them in making research data findable, accessible, interoperable and reproducible (FAIR).\n\n\n\nFounded in 2012 as an independent foundation by NWO and SURF, the Netherlands eScience Center is the national centre with the digital skills to create innovative software solutions in academic research. They award research projects based on calls for proposals, and train researchers in the use of research software. They offer our expertise in the form of research software engineers (RSEs), the technology specialists with expert digital skills who work with us at the Center.\n\n\n\nNL-RSE brings together the community of people writing and contributing to research software from Dutch universities, knowledge institutes, companies and other organizations to share knowledge, to organize meetings, and raise awareness for the scientific recognition of research software."
  },
  {
    "objectID": "docs/community.html#partners-within-the-tu-delft",
    "href": "docs/community.html#partners-within-the-tu-delft",
    "title": "Community",
    "section": "",
    "text": "Teams channel - DCC Community\nData Stewards\nOpen Science Program\nOpen Science Community\nDelft HPC\nData Champions\nICT Innovation\nStatistical Helpdesk"
  },
  {
    "objectID": "docs/community.html#tu.researchdata",
    "href": "docs/community.html#tu.researchdata",
    "title": "Community",
    "section": "",
    "text": "4TU.ResearchData is an international data repository for science, engineering and design. Its services include curation, sharing, long-term access and preservation of research datasets. These services are available to anyone around the world. In addition, 4TU.ResearchData also offers training and resources to researchers to support them in making research data findable, accessible, interoperable and reproducible (FAIR)."
  },
  {
    "objectID": "docs/community.html#escience-center",
    "href": "docs/community.html#escience-center",
    "title": "Community",
    "section": "",
    "text": "Founded in 2012 as an independent foundation by NWO and SURF, the Netherlands eScience Center is the national centre with the digital skills to create innovative software solutions in academic research. They award research projects based on calls for proposals, and train researchers in the use of research software. They offer our expertise in the form of research software engineers (RSEs), the technology specialists with expert digital skills who work with us at the Center."
  },
  {
    "objectID": "docs/community.html#nl-rse",
    "href": "docs/community.html#nl-rse",
    "title": "Community",
    "section": "",
    "text": "NL-RSE brings together the community of people writing and contributing to research software from Dutch universities, knowledge institutes, companies and other organizations to share knowledge, to organize meetings, and raise awareness for the scientific recognition of research software."
  },
  {
    "objectID": "docs/data/data_collection/collection.html",
    "href": "docs/data/data_collection/collection.html",
    "title": "Data collection",
    "section": "",
    "text": "Data collection is one of the first steps in the data lifecycle. It involves gathering and recording data from various sources, which can include (but are not limited to) experiments, observations, simulations or existing datasets. These can come from various sources being both public and private. The data collected can be quantitative or qualitative, and it can be structured or unstructured. The choice of data collection methods and tools depends on the research objectives, the type of data needed, and the resources available.\nProper data collection practices are essential for ensuring the integrity and reliability of the data, as well as for its use later analysis and interpretation. This section provides an overview of important aspects of data collection, including data conventions, data access and reuse, and the use of eLabJournal and RSpace. These elements are essential for maintaining the quality and usability of research data, and they play a significant role in the overall research process.\n\n\n\n\n\n\nImportant Learn more\n\n\n\n\nWe would like to refer you to the Discover & Reuse and Gather/Create & Analyse and sections of the TU Delft Navigating Research Data and Software: A Practical Guide for PhD Supervisors guide for more information.\nAdditionally, we would like to refer you to the RDM 101 book for more information on this topic.\n\n\n\n\n\n\n Data conventions\nData standards and types.\n\nLearn more ¬ª\n\n\n\n Data access and reuse\nMaking your data accessible and reusable.\n\nLearn more ¬ª\n\n\n\n eLabJournal and RSpace\nTools made available by TU Delft to document the research process.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection"
    ]
  },
  {
    "objectID": "docs/data/data_collection/elab_rspace.html",
    "href": "docs/data/data_collection/elab_rspace.html",
    "title": "eLabJournal and RSpace",
    "section": "",
    "text": "Jones recounts her frustration with relying on paper notebooks to record experiments. ‚ÄúIt becomes problematic when you need to replicate experiments or continue a research project conducted by a researcher who has since left your institution without documenting their work properly.‚Äù Jones identifies some difficulties of understanding handwritten information (including her own!) ‚ÄúLocating and interpreting data from past experiments can be challenging using paper notebooks‚Äù, she admits. ‚ÄúFinally, after rifling through pages of scribbled diagrams, photocopies and post-it notes you find the experiment you‚Äôve been searching for and it can be impossible to decipher the handwriting and bridge the gaps of missing information‚Äù. These problems sound all too familiar to most lab-based scientists and have increased the demand for digital solutions, such as ELNs, that can improve the rigour, robustness and reproducibility of scientific research.\nDr.¬†Si√¢n Jones, quote taken from Keep calm and go paperless: Electronic lab notebooks can improve your research. CC-BY-4.0\n\nAn electronic laboratory notebook (ELN), also called a digital lab notebook, offers a text editor for writing notes similar to a paper notebook, along with spreadsheet tools for calculations and formatting tables and graphs. They include protocol templates to record standard procedures and laboratory inventories to keep track of samples, reagents, and equipment. Additionally, ELNs provide collaboration tools to share experimental information with others, making them a straightforward solution to research documentation.\nTU Delft has a subscription to eLABJournal and RSpace ELNs, and both offer similar functionalities.\n\n\n\n\n\n\n\nImportant Learn more\n\n\n\nFor instructions on getting started with RSpace and eLabJournal, along with other essential information about ELNs, visit the TU Delft Library website‚Äôs Electronic Lab Notebook page.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "eLabJournal and RSpace"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/archival_publishing_index.html",
    "href": "docs/data/data_publishing/archival_publishing_index.html",
    "title": "Archiving and publishing data",
    "section": "",
    "text": "Towards the end of the research cycle, data archiving (and publishing, when possible) is an essential step for the longevity and accessibility of your research. Archiving data means that it is preserved and remains accessible for future use. This process not only helps with the visibility of your work but also contributes to the advancement of knowledge in your field. Other researchers have the data available and can build upon your work. Learning how to effectively preserve and share your research isn‚Äôt just good scientific practice - it‚Äôs how your work makes a lasting impact.\n\n\n\n Licensing\nChoose a license for your data.\n\nLearn more ¬ª\n\n\n\n Publishing data\nBroaden the reach of your research.\n\nLearn more ¬ª\n\n\n\n Offboarding and ownership\nEnsures accountability and longevity.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/offboarding.html",
    "href": "docs/data/data_publishing/offboarding.html",
    "title": "Offboarding and ownership",
    "section": "",
    "text": "Offboarding procedures are necessary for ensuring the continuity and integrity of research data when team members leave or transition roles. Establishing clear ownership protocols and conducting thorough offboarding processes help maintain data accessibility, prevent loss, and ensure compliance with institutional policies. To achieve effective offboarding, document data responsibilities, transfer access rights, and conduct exit interviews to capture essential knowledge and insights.\nThis guide serves primarily as a reminder that there should be a plan in place for offboarding. Different research groups will have different practices, and there is no single definitive checklist or procedure. However, we would like to refer you to the Data Management Offboarding by Harvard RDM site for ideas on how to approach offboarding and preserve knowledge when colleagues leave.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Offboarding and ownership"
    ]
  },
  {
    "objectID": "docs/data/data_storage/backup.html",
    "href": "docs/data/data_storage/backup.html",
    "title": "Data backup",
    "section": "",
    "text": "An effective system for backing up research data is crucial to the success and reproducibility of any research project. Backups ensure that irreplaceable files are not lost due to hardware failure, accidental deletion, or other unforeseen events.\nA widely recommended strategy for data backup is the 3-2-1 Rule:\nWhen deciding on backup frequency, you should consider how often the data changes, the amount of work that would be lost between backups, the cost (time and money) to replace lost work, and the effort required for the backup process. Automated backup solutions are often more efficient than manual processes. Please refer to the section on Storage options for more information on which options involve automated backup or not.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data backup"
    ]
  },
  {
    "objectID": "docs/data/data_storage/backup.html#using-rsync-in-the-command-line-for-data-backup",
    "href": "docs/data/data_storage/backup.html#using-rsync-in-the-command-line-for-data-backup",
    "title": "Data backup",
    "section": "Using rsync in the command line for data backup",
    "text": "Using rsync in the command line for data backup\nIf you are comfortable with the command line, rsync is a powerful and versatile utility available on Linux (including Windows Subsystem for Linux: WSL) and macOS systems that can efficiently synchronize and back up files and directories. It is particularly useful for incremental backups, as it only transfers the differences between the source and destination, saving time and bandwidth.\nHere are simple steps to use rsync for data backup:\n\nBasic Local Backup: To copy files from a source directory to a destination directory on the same machine (e.g., to an external hard drive), use the following command:\n\n   rsync -av /path/to/your/source_data/ /path/to/your/backup_destination/\n\n-a (archive mode): This is a composite flag that preserves permissions, timestamps, symbolic links, and recursively copies directories, making it ideal for backups.\n-v (verbose): This flag provides detailed output, showing which files are being copied or skipped.\n\n\nIncremental Backups: To ensure that only new or changed files are copied, and to remove files from the backup destination if they‚Äôve been deleted from the source, add the --delete flag:\n\n   rsync -av --delete /path/to/your/source_data/ /path/to/your/backup_destination/\nThis makes the backup an exact mirror of the source, reflecting deletions as well as additions and modifications.\n\nRemote Backups via SSH: For secure backups to a remote server (e.g., a research server or another machine), rsync can use SSH. Ensure the remote machine has an SSH server running and you have SSH access.\n\n   rsync -avz /path/to/your/source_data/ username@remote_host:/path/to/remote/backup_destination/\n\n-z (compress): This flag compresses file data during transfer, which can speed up transfers over a network connection.\n\n\nExcluding Specific Files or Directories: If you need to exclude certain files or folders from your backup (e.g., temporary files or large datasets that are not critical to back up frequently), use the --exclude option:\n\n   rsync -av --exclude 'temp_files/' --exclude 'logs/*.log' /path/to/your/source_data/ /path/to/your/backup_destination/\nYou can specify multiple --exclude options for different patterns or paths.\n\n\n\n\n\n\nImportant Important\n\n\n\nrsync is a robust tool for maintaining up-to-date copies of your research data, supporting both local and remote backup strategies as part of a comprehensive data management plan. For more advanced usage, refer to the rsync manual by running man rsync in your terminal or consult the rsync documentation.\n\n\n\n\n\n\n\n\nImportant Important\n\n\n\nThese steps are similar to those recommended in Moving data to your server and in the Transfer Data section of the DelftBlue documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data backup"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html",
    "href": "docs/data/data_storage/project_drive_request.html",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "",
    "text": "TU Delft network drives are automatically mounted on TU Delft-managed computers (running Windows) when connected to the TU Delft network (dastud, eduVPN).\nOn macOS and Linux, there are a few additional steps needed to access Project Drives. The instructions can be found here. (Last updated: August 2024)\nIt can also be accessed through webdata.tudelft.nl using a WebDAV web link staff-umbrella. To mount on a TU Delft Virtual Private Server, first follow the instructions here, and then the instructions in the next guide Mount Project Drive on server.\n\n\n\n\n\n\nNote Reminder\n\n\n\nIf you are accessing the Project Data Storage (U:) drive from outside the TU Delft network (e.g., from home), you need to connect to eduVPN beforehand.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html#accessing-the-project-data-storage-u-drive",
    "href": "docs/data/data_storage/project_drive_request.html#accessing-the-project-data-storage-u-drive",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "",
    "text": "TU Delft network drives are automatically mounted on TU Delft-managed computers (running Windows) when connected to the TU Delft network (dastud, eduVPN).\nOn macOS and Linux, there are a few additional steps needed to access Project Drives. The instructions can be found here. (Last updated: August 2024)\nIt can also be accessed through webdata.tudelft.nl using a WebDAV web link staff-umbrella. To mount on a TU Delft Virtual Private Server, first follow the instructions here, and then the instructions in the next guide Mount Project Drive on server.\n\n\n\n\n\n\nNote Reminder\n\n\n\nIf you are accessing the Project Data Storage (U:) drive from outside the TU Delft network (e.g., from home), you need to connect to eduVPN beforehand.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html#requesting-project-data-storage-u-space",
    "href": "docs/data/data_storage/project_drive_request.html#requesting-project-data-storage-u-space",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "Requesting Project Data Storage (U:) space",
    "text": "Requesting Project Data Storage (U:) space\n\nPrerequisites\n\nTU Delft NetID\n(Optional) A list of TU Delft collaborators who should have read/write access to it\n\n\n\nSteps\n\nRequest the storage via the TU Delft ICT form on TopDesk.\nFill out and send the form according to your data storage preferences and requirements.\nAccess your data storage.\n\n\nStep 1. Request storage via the TU Delft ICT form on TopDesk\nYou can make a request for data storage via a form available on the TopDesk self-service portal (requires NetID sign-in); navigate to:\n\nHOME ‚áæ ICT SERVICES ‚áæ IT WORKSPACE ‚áæ DATA STORAGE ‚áæ PROJECT DATA STORAGE - APPLICATION FORM\n\n\n\nStep 2. Fill out and send the form according to your data storage preferences and requirements\nThe form has six main sections: ‚ÄúCaller‚Äù, ‚ÄúApplication‚Äù, ‚ÄúAccess for third parties‚Äù, ‚ÄúStorage type‚Äù, ‚ÄúStorage Requirements‚Äù, and ‚ÄúBackup Retention‚Äù.\nThe Caller section should be auto-populated with your name, building, phone number (if you have a work phone number), email, department/program, organizational unit, and (sometimes) room. You will then be considered as main administrator of this server.\nIn the next part, Application, you can input:\n\n(Short) Description of the research project for which you are requesting the storage. This will help ICT to understand your needs and provide you with the best possible service.\nA proposed name for the Project Data Storage. This name will be used as the folder name for your storage (e.g., U:/your_project_name). Even if not required, we strongly advise to keep it as short as possible, yet descriptive, and without spaces or special characters. For example, use underscores ( _ ) or hyphens ( - ) to separate words, or use CamelCase (dividing words using capital letters).\nWho should have access. Here you can add the full name, NetID, and email address of your collaborators and which type of access they should have (read or write). You can always add or remove users later by contacting the ICT Servicedesk. You can also choose to have no other users with access apart from yourself. If you have a few collaborators, you can also add their details using an attachment file using the box below.\n\nIn Access for third parties, you can indicate if you want to allow external users (i.e., non-TU Delft users) to access the data. If you choose ‚ÄúYes‚Äù, you will need to provide the names and email addresses of the external users in the text box below (referred in step 3 above, only difference is the lack of NetID for external users).\nStorage type allows you to ask for high availability. This choice depends on your I/O (Input/Output) operations. For example, large/long simulations, and real time data processing may require high availability, especially if you mount it to a server as described in the section Mount Project Drive on server. If you are unsure about the availability needed, contact your faculty Data Steward for advice before submitting you application.\nStorage Requirements allows you to specify the amount of storage you need. The minimum (upper limit) is 250 GB, and the maximum is 5 TB. If you need 5 TB or more, your request will be forwarded to the Faculty IT Manager (FIM) for further review.\nFinally, in Backup Retention, you can choose how long you want your data to be backed up. For the default option, a backup is made on a daily basis and is stored for two weeks. This means a data loss of a maximum of one day can occur. After two weeks, a back-up is made every week and saved for a year. This means a data loss of maximum one day can occur. An extended option is also available to keep data stored from 53 weeks.\n\n\nStep 3. Access your data storage\nAfter submitting the request, you will receive a response from ICT. Once approval is granted, you can access your Project Data Storage as described above.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html",
    "href": "docs/data/data_storage/sharing.html",
    "title": "Data sharing",
    "section": "",
    "text": "This section covers aspects of data sharing, for ongoing work, throughout the active phase of a research project. The use of data repositories, the importance of data licenses, and the significance of metadata in sharing completed work are discussed in the section on Archiving and publishing data.\nBoth sections are closely related, as data sharing is a key component of the broader data management lifecycle. As such, these sections include shared concepts of security and privacy also covered in sections of their own.\nAs shown in the overview of storage options section, TU Delft provides several options for data storage, which can be used to share data with internal and external partners. The choice of storage solution depends on the nature of the data, the level of security required, and the intended audience for the shared data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html#tu-delft-internal",
    "href": "docs/data/data_storage/sharing.html#tu-delft-internal",
    "title": "Data sharing",
    "section": "TU Delft internal",
    "text": "TU Delft internal\n\nNetworked storage\n\nPersonal Data Storage (H:)\nThis is the default storage location for personal data, such as documents, spreadsheets, and presentations. It is accessible only to the user and is not suitable for sharing with others. However, it enables you to share your data across TU Delft-managed devices connected to the TU Delft network.\n\n\nGroup Data Storage (M:)\nThe folder structure on this network drive follows a similar structure to that of the faculties, departments, and research groups at TU Delft. It is typically accessible to members of a research group and can be used to share data with colleagues within the same department and faculty. Access to this storage is managed by your department secretary and/or the Faculty ICT Manager.\n\n\nProject Data Storage (U:)\nAccess to this storage is requested by a contact person ‚Äúcaller/owner‚Äù which can in turn provide access via a NetID. Its data can be accessed through CIFS and NFS (with kerberos authentication), meaning it can be mounted in different systems. Please visit the project drive request and project drive mounting sections for more information.\n\n\n\nCloud-based storage\nCloud services are not recommended as primary storage locations for research data. A critical drawback is that access to data stored on these platforms can be lost upon the creator‚Äôs departure from TU Delft, posing a significant risk to data continuity and ownership. Furthermore, they ‚Äúshould not be used for highly confidential data such as state secrets, sensitive personal data or highly sensitive IP material‚Äù.\nTU Delft provides a few collaboration tools through the Microsoft Office 365 platform, including Microsoft Teams, SharePoint and OneDrive.\n\nMicrosoft Teams\nThis platform is primarily used for communication and collaboration within research groups and projects. It allows for file sharing, real-time collaboration, and integration with other Microsoft 365 applications. However, it is not a dedicated data storage solution and should be used in conjunction with other storage options.\n\n\n\n\n\n\nTip Find more details on TopDesk Home ‚áæ ICT services ‚áæ Collaboration Tools ‚áæ Microsoft Teams.\n\n\n\n\n\n\n\n\nMicrosoft SharePoint\nConsiderations for this platform are similar to those for Microsoft Teams. It is primarily used for document management and collaboration within research groups and projects. SharePoint allows for file sharing, version control, and integration with other Microsoft 365 applications.\n\n\n\n\n\n\nTip Find more details on TopDesk Home ‚áæ ICT services ‚áæ Collaboration Tools ‚áæ SharePoint 2016 support",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html#external",
    "href": "docs/data/data_storage/sharing.html#external",
    "title": "Data sharing",
    "section": "External",
    "text": "External\n\nMicrosoft OneDrive for Business\nOneDrive is installed by default on the laptops and desktops supplied by TU Delft. TU Delft OneDrive is recognisable by the name: ‚ÄúOneDrive - Delft University of Technology‚Äù. Web based access within and outside TUDelft, sharing and working together is possible with TU Delft colleagues and also external users. OneDrive is suitable for sharing data with external partners, as it allows for controlled access and collaboration. However, it is important to ensure that sensitive data is not shared without proper security measures in place.\n\n\n\n\n\n\nTip Find more details on TopDesk Home ‚áæ OneDrive for Business\n\n\n\n\n\n\n\n\nSURFdrive\nSURFdrive is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. SURFdrive offers staff, researchers and students an easy way to share and synchronise files within a secure community cloud with ample storage capacity.\n\n\n\n\n\n\nTip Find more details on SURFdrive.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html",
    "href": "docs/data/data_storage/storage_options.html",
    "title": "Storage options",
    "section": "",
    "text": "Storing your data in a secure location is a key element of a successful project with a data component. As a TU Delft researcher, you have several options. Below you can find an overview of the available storage options, depending on whether they are local, networked, or cloud-based, how secure they are, whether they can be shared with internal or external partners, and whether they are automatically backed up or not. Please visit the sections on Data Security, Data Sharing and Data Backup for more information on these topics.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html#overview-of-storage-options",
    "href": "docs/data/data_storage/storage_options.html#overview-of-storage-options",
    "title": "Storage options",
    "section": "Overview of storage options",
    "text": "Overview of storage options\n\n\n\n\n\n\nImportant Important\n\n\n\nThese options have been curated from:\n\nThe intranet section on Data Storage\nThe TOPdesk ‚ÄúOverview of data storage and file sharing‚Äù (Home &gt; ICT services &gt; IT for Research &gt; Overview of data storage and file sharing)\nAnd the Storage Finder tool\nSURF\n\nPlease note that the information is subject to change, and you should always refer to the official TU Delft documentation for the most up-to-date information.\n\n\n\n\n\nStorage Option\nLocation\nSecurity\nSharing\nBackup\nSize\nAditional info\n\n\n\n\nPrivate laptop\nLocal\nLow\nDefined by owner\nDefined by owner\nCheck device specifications\nSecurity listed as low as the device is prone to loss and theft, among other incidents\n\n\nPrivate desktop\nLocal\nMedium\nDefined by owner\nDefined by owner\nCheck device specifications\nSame as above, perhaps more secure than a laptop as it is not necessarily mobile\n\n\nTU Delft managed laptop\nTU Delft network\nMedium\nInternal to TU Delft\nDefined by owner (unless using options listed below)\nCheck device specifications\nAll TU Delft managed devices are password protected. Security is therefore slightly higer than private counterparts\n\n\nTU Delft managed desktop\nTU Delft network\nMedium\nInternal to TU Delft\nDefined by owner (unless using options listed below)\nCheck device specifications\nAll TU Delft managed devices are password protected. Security is therefore slightly higer than private counterparts\n\n\nTU Delft Personal Data Storage (H:)\nTU Delft network\nHigh\nInternal to TU Delft\nYes\n8 GB\nStorage Finder\n\n\nTU Delft Group Data Storage (M:)\nTU Delft network\nHigh\nInternal to TU Delft\nYes\n50 GB\nStorage Finder\n\n\nTU Delft Project Data Storage (U:)\nTU Delft network\nHigh\nInternal to TU Delft. External access can be enabled with a TU Delft guest account.\nYes\n&gt; 250 GB\nStorage Finder\n\n\nMicrosoft Teams\nCloud\nHigh\nInternal to TU Delft\nYes\n&lt; 250 GB\nStorage Finder\n\n\nMicrosoft Sharepoint\nCloud\nHigh\nInternal to TU Delft\nYes\n&lt; 250 GB\nStorage Finder\n\n\nMicrosoft OneDrive for Business\nCloud\nHigh\nExternal access can be enabled\nYes\n300 GB\nStorage Finder\n\n\nSURFdrive\nCloud\nHigh\nExternal access can be enabled\nYes\n1 TB\nSURFdrive documentation",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html#project-drive-operations",
    "href": "docs/data/data_storage/storage_options.html#project-drive-operations",
    "title": "Storage options",
    "section": "Project Drive operations",
    "text": "Project Drive operations\nA Project Drive is a TU Delft managed network drive that allows you to store and share data with your project team members. It is a secure and reliable option for storing research data, and it is automatically backed up.\n\n\n\n\n\n\nTip For most situations, the Project Data Storage (U:) drive is the optimal choice for storing research data.\n\n\n\n\n\n\nBelow you can see how to access a Project Data Storage (U:) drive, request space for it, and how to mount it on a server (please refer to the section on Remote Servers for more details).\n\n\n\n Access and request space for Project Data Storage\n\nLearn more ¬ª\n\n\n\n Mount Project Drive on server\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/getting_started.html",
    "href": "docs/data/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This section provides resources to support data owners and users at various stages of the data lifecycle. It highlights important information and available tooling at TU Delft that can help you implement the Findable, Accessible, Interoperable, and Reproducible (FAIR) principles in your research data management practices.\nTU Delft emphasizes researchers‚Äô adherence to best practices in Research Data and Software Management (RDSM) as crucial to research quality. The university has maintained policies for a structured framework clarifying roles, responsibilities, and procedural standards for RDSM.\nResearch data includes any information observed, generated or created for use in research projects. Though often tied to specific research projects, research data follows its own lifecycle, which is distinct from the project itself. After initial data collection, data may be processed, analyzed, published, shared, archived, reused or destroyed.\nEffective research data management by following FAIR principles ensures efficiency and reproducibility at every step of the data lifecycle. At TU Delft, various tools and resources are available to meet diverse data management needs at any stage in its lifecycle.\n\n\n\n\n\n\nImportant Important note\n\n\n\nPlease note that useful and practical information on research data management (RDM) is covered by other TU Delft resources, such as the RDM 101 book, PhD Supervisors guide, the RDM and TU Delft library pages. We want to refer you to these resources throughout this section, and where applicable, an admonition (like this one) will be placed at the beginning of a page.\n\n\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nTU Delft Research Data Management landing page\nTU Delft - The goal of data management\nTU Delft Research Data framework policy\nResearch data management policies per faculty",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/data/planning/privacy.html",
    "href": "docs/data/planning/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "In case you are working with sensitive data, it is important to consider the privacy of the individuals involved. This includes ensuring that personal data is collected, stored, and processed in compliance with relevant laws and regulations.\nYou might need to obtain informed consent from participants before collecting their data, and you should also need to anonymize or pseudonymize data to protect the privacy of individuals. Additionally, you should be aware of the ethical implications of your research and ensure that you are conducting it in a responsible manner.\n\n\n\n\n\n\nImportant Important\n\n\n\n\nReach out to your faculty data steward for more information.\nExplore TU Delft privacy related resources:\n\nTU Delft Personal Data page\nTU Delft Personal Research Data Workflow Guide\nTU Delft Privacy SharePoint site\nPhD Supervisors guide - Personal data and human subjects in research\n\nAdditionally, Utrecht University has a Data Privacy handbook that covers and explains many privacy topics in detail!",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning",
      "Privacy"
    ]
  },
  {
    "objectID": "docs/infrastructure/getting_started.html",
    "href": "docs/infrastructure/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This section guides you through the essential systems and services needed to manage computational resources and collaborate effectively within TU Delft supported environments.\nComputing infrastructure includes the servers and tools that enable you to handle data processing tasks, host research applications, and maintain secure access to resources. TU Delft provides managed infrastructure options, and the guides in this section address practical aspects of working with TU Delft infrastructure. Topics covered include requesting and accessing virtual private servers, establishing secure connections, transferring data to remote systems, configuring secure communications and web servers.\nYou can use these resources to set up computing environments suited to your project requirements. This includes running computational analyses, hosting databases, and deploying research software and applications with secure, managed infrastructure backing your work.\nMost of these resources are private to the TU Delft network. However, you can also find a few details on how to enable external access when required, for example, in the case of a web server.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/infrastructure/remote_servers.html",
    "href": "docs/infrastructure/remote_servers.html",
    "title": "Remote Servers",
    "section": "",
    "text": "A server is a computer that handles requests (data, services, or programs) from other computers, known as clients, over a network. Servers are often a critical component of architectural solutions for data management and software deployment. There are many reasons why you as a researcher may need to use a server, for example:\n\nYou need a machine to handle large amounts of requests\nYou want to outsource the maintenance of a server to TU Delft ICT\nYou would like to rely on safety and security administrated by the university, including backups\nA part of your analysis should be running continuously, and cannot do it with your own machine\n\n\n\n\n\n\n\nNote Note\n\n\n\nIt is not recommended to use servers to set up services that are already provided by the ICT department. When in doubt, you can always contact your Faculty ICT Manager (FIM) and/or Faculty Data Steward.\n\n\n\nVirtual vs physical servers\nAll servers are a physical computer sitting somewhere. However, there is a common distiction between virtual and physical servers, where a virtual server is an independent instance provided by a larger physical server. Virtual servers are provided free of charge and can be requested via TOPdesk. Physical server placement can be requested by contacting the faculty‚Äôs IT manager, and any associated costs are paid by the purchasing researcher and/or department.\nIn most cases, a virtual server is the most suitable option. However, a physical server may be necessary when it is intended for specific use cases (e.g., laboratory equipment, sensor data acquisition, image processing).\nTU Delft offers its employees the use of physical or virtual servers, these servers are referred as faculty managed servers which are therefore private to the university network by default. Virtual Private Servers (VPS) can be requested as described in the next guide Request a VPS, whereas physical server placement follow a different procedure for which we strongly encourage you to consult your FIM.\n\n\nRelevant considerations\nHaving a Faculty Managed Server (either virtual or physical) can poise several advantages:\n\nICT provides the server, operating system, and network access.\nICT provides daily backups, restoration services, and virus scanning for Windows servers.\nICT ensures that the server operating system remains up to date (e.g., security patches), except for Linux systems.\nAccess can be granted to both TU Delft members and external users.\nUsers are granted administrator privileges, allowing them to install any required software, provided it complies with the conditions specified in the request form.\n\n\n\n\n\n\n\nTip Tip\n\n\n\nDetailed information on managing the server, including network and firewall settings, is provided in the TOPdesk application form.\n\n\n\n\nExample use cases\n\nPerforming computational or data processing tasks that require a dedicated server environment.\nRunning an instance of a service, application, or other specialized tools for a lab or research group not currently centrally provided by the university.\nHosting a static website, a web application, or an API for a project.\nHosting databases, such as MySQL, PostgreSQL, MongoDB, or other database management systems.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html",
    "href": "docs/infrastructure/vps_request.html",
    "title": "Request a VPS",
    "section": "",
    "text": "This guide describes the essentials for requesting and setting up a TU Delft managed Virtual Private Server (VPS).\nCommon use cases for VPSs covered in these guides include:\n\nSet up a web server\nSet up runners for TU Delft GitLab\nMounting and handling storage drives such as a Project Drive",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#background",
    "href": "docs/infrastructure/vps_request.html#background",
    "title": "Request a VPS",
    "section": "",
    "text": "This guide describes the essentials for requesting and setting up a TU Delft managed Virtual Private Server (VPS).\nCommon use cases for VPSs covered in these guides include:\n\nSet up a web server\nSet up runners for TU Delft GitLab\nMounting and handling storage drives such as a Project Drive",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#prerequisites",
    "href": "docs/infrastructure/vps_request.html#prerequisites",
    "title": "Request a VPS",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft NetID\nBasic knowledge of Linux (if requesting a Linux server)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#toolssoftware",
    "href": "docs/infrastructure/vps_request.html#toolssoftware",
    "title": "Request a VPS",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nFor Windows users, you will need a programming and runtime environment like Windows Subsystem for Linux (WSL) or an SSH client like PuTTY.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#steps",
    "href": "docs/infrastructure/vps_request.html#steps",
    "title": "Request a VPS",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the TU Delft server request form\nFill and send the form according to your preferences and needs\nReceive confirmation of server deployment from TU Delft ICT\nLogin to your server for the first time\n\n\nStep 1: Navigate to the TU Delft server request form\nYou can make a request for a server via the TopDesk self service portal. If the link does not work, you can also navigate to the TopDesk portal homepage and type ‚ÄúFaculty managed Servers‚Äù in the search box. Alternatively, you can navigate to the form via:\n\nICT SERVICES ‚áæ IT FOR RESEARCH ‚áæ FACULTY MANAGED SERVERS.\n\n\n\nStep 2: Fill and send the form according to your preferences and needs\nThe form is divided into three sections: ‚ÄúCaller‚Äù, ‚ÄúGeneral Questions‚Äù, and ‚ÄúTechnical Questions‚Äù.\nCaller should contain the contact information of the main administrator of this server. If you select your name, the fields below should be auto-populated with your building, phone number, email, department/program, organizational unit, and (sometimes) room. If you are requesting the server on behalf of someone else, you can fill in their details instead.\nThe last question in the Caller Details section access to the server by external users. Generally speaking, granting access to TU Delft-managed servers is not recommended, but if it is necessary you can add the contact details of the external party and the reason(s) for which they should have access. You will need to provide a company-affiliated email address for the external user, and the request may or may not be granted by ICT.\n\n\n\n\n\n\nNote Note\n\n\n\nKeep in mind that a server provides access to the backend of your application. If for example you want to deploy a web server to share your data widely, users do not need direct access to the server in order to access the data itself.\n\n\nThe next section contains General Questions about the name and purpose of your server. If you plan to use this server ongoing into the future, you can either leave the field ‚ÄúExpiration Date‚Äù blank or add a date in 10+ years. TU Delft ICT will alert you when the expiration date you select is nearing. There you can also add the netIDs of your collaborators who should have access to the server\nThe Technical Questions section asks you to specify an operating system and some other technical details about your server configuration.\nWhen requesting a VPS, users can choose from a range of predefined hardware and operating system configurations. The following operating systems are available: Windows Server 2019, Windows Server 2022, Red Hat Enterprise Linux (latest supported version), and Ubuntu (latest LTS version). If you are new to working with servers, generally the best choice may be ‚ÄúBasic configuration 4‚Äù (Ubuntu).\nAll configurations have rather limited internal memory, so if you need more memory you can always mounting additional storage drive(s) such as a Project Drive. If additional processing capacity is needed, it can be requested via the ‚ÄòICT malfunction‚Äô or ‚ÄòRequest ICT service‚Äô forms in TOPdesk. This includes options such as increasing the number of processors, cores per processor, RAM, or disk storage. It may also reveal a reason to consider a physical server instead of a VPS.\nThe next question deals with opening ports to the server through the TU Delft firewall. Ports are essentially gateways to the server that are specific to different purposes. For example, port 80 is opened to handle HTTP requests, port 20 is opened to handle SSH requests, port 3306 is opened to allow access to a MySQL database, and port 443 is opened to handle HTTPS requests. If you are planning to use your VPS as a webserver, ports 80 and 443 should be open. You can use this space to ask ICT to do so.\nThe next section, FQDN, is the way you can refer to your server on the internet. The recommendation is a format like &lt;servername&gt;.&lt;facultyabbreviation&gt;.tudelt.nl. In general, it‚Äôs best to keep names relatively short and informative to make it easy to reference and remember.\n\n\n\n\n\n\nTip Tip\n\n\n\nYou should also be sure to check the instructions in the form and contact your faculty Data Steward or Faculty IT Manager if you need further explanation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#initial-configuration-of-your-vps",
    "href": "docs/infrastructure/vps_request.html#initial-configuration-of-your-vps",
    "title": "Request a VPS",
    "section": "Initial Configuration of your VPS",
    "text": "Initial Configuration of your VPS\nA few days after submitting the request, you will receive an email from ICT with login details. You can connect to your VPS via SSH (secure shell) using a command line interface (CLI). If you are in a Windows environment, it is recommended to install WSL or PuTTY to be able to use the ssh command in a CLI. The Unix based systems (e.g., Mac, Ubuntu) contain SSH by default in their ‚ÄúTerminal‚Äù application. In order to login to your VPS, you need to first SSH to the bastion server with ssh &lt;username&gt;@linux-bastion-ex.tudelft.nl and then from there login to your server ssh &lt;servername&gt;. The first thing we recommend to do after logging into the server is to update the pre-installed packages:\n\nDebian (Ubuntu)RedHat\n\n\nsudo apt-get update && sudo apt-get upgrade\n\n\nsudo yum update\n\n\n\nIt would be also useful to set a password for the VPS when you log in. You can do that by passwd command.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/vps_request.html#next-steps",
    "href": "docs/infrastructure/vps_request.html#next-steps",
    "title": "Request a VPS",
    "section": "Next Steps",
    "text": "Next Steps\n\nSet up a web server\nSet up runners for TU Delft GitLab\nMounting and handling storage drives such as a Project Drive\n\nWe will add more documentation when other common cases come to our attention, so please reach out to us with your questions or suggestions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/web_servers.html",
    "href": "docs/infrastructure/web_servers.html",
    "title": "Web Servers",
    "section": "",
    "text": "If you want to host a website or a web application, you will need to work with a web server. The job of a web server is to serve content on the internet by acting as a middleman between client machines and server machines. When a client requests content, the web server processes that request and delivers the appropriate response. Popular web server software includes Apache and Nginx.\nThis guide provides general considerations to take into account when setting up a web server, along with links to resources for the most commonly used web server platforms. For specific implementation guidance, consult the official documentation for your chosen web server software and operating system.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Web servers"
    ]
  },
  {
    "objectID": "docs/infrastructure/web_servers.html#considerations-when-setting-up-a-web-server",
    "href": "docs/infrastructure/web_servers.html#considerations-when-setting-up-a-web-server",
    "title": "Web Servers",
    "section": "Considerations when setting up a web server",
    "text": "Considerations when setting up a web server\n\nOperating system: Choose an operating system that provides appropriate performance, security, and compatibility for your needs.\nHardware requirements: Ensure that the server hardware meets the requirements for your web application and expected traffic levels.\nWeb server software: Select web server software that aligns with your application requirements (e.g.¬†Apache or Nginx).\nSecurity: Implement security measures to protect your server from vulnerabilities.\nMaintenance: Regularly update your web server software and operating system to ensure security and performance. Monitor server logs for any unusual activity.\n\n\nOperating system\nThe choice of operating system significantly impacts your web server‚Äôs performance, security, and maintenance requirements. Server-grade operating systems are designed for continuous operation and provide robust performance features needed for web hosting environments.\nLinux-based systems are widely used for web servers due to their stability, security features, and cost-effectiveness. Popular distributions include:\n\nUbuntu Server: Offers long-term support (LTS) versions with regular security updates and extensive community resources\nDebian: Known for stability and security\nEnterprise Linux distributions: Include commercially supported options like Red Hat Enterprise Linux (as well as community-driven alternatives)\n\nWindows Server provides an alternative for environments requiring Windows-specific technologies or where teams have existing Windows expertise. Commercial licensing is required for Windows Server installations.\nAt TU Delft, you can request a VPS preinstalled with Ubuntu Server or Windows Server. Which one you choose depends on your familiarity with the operating system and the specific requirements of your web application. However, Linux-based servers are generally preferred for web hosting due to their lower resource requirements, free licensing, and strong community support.\n\n\nHardware requirements\nHardware requirements depend on multiple factors including expected traffic volume, application complexity, data processing needs, and performance requirements. Without performance testing specific to your application, exact requirements can be difficult to determine.\nGeneral guidelines for basic web server hardware:\n\nCPU: Multi-core processors handle concurrent requests more effectively. Current-generation processors with adequate cores are recommended for most applications.\nRAM: Memory requirements vary significantly based on type and expected load. Start with sufficient RAM for your base operating system and application stack, then monitor and adjust based on actual usage patterns.\nStorage: SSDs provide better performance than traditional hard drives. Storage capacity should account for your application files, data, logs, and future requirements.\n\n\n\nWeb server software\nDifferent web servers excel in different scenarios. The two most popular web servers are Apache and Nginx. Sometimes, the choice of a web server software is determined by your web application. For example, PHP applications can be served by either Apache (using mod_php) or Nginx (using PHP-FPM), while Node.js applications are commonly served by Nginx as a reverse proxy.\nThe following table provides a general comparison of key characteristics:\n\n\n\n\n\n\n\n\nCharacteristic\nApache\nNginx\n\n\n\n\nArchitecture\nProcess/thread-based\nEvent-driven, asynchronous\n\n\nConfiguration\nExtensive options, flexible\nSimpler syntax, centralized\n\n\nStatic Content\nGood performance\nExcellent (very fast & lightweight)\n\n\nDynamic Content\nBuilt-in module support\nTypically uses external processors\n\n\nResource Usage\nHigher memory per connection\nLightweight, better under heavy load\n\n\nLearning Curve\nModerate\nVaries by use case\n\n\nPlatform\nCross-platform\nMostly Linux/Unix (Windows support exists)\n\n\n\nModern web applications often use application servers or runtime environments (such as PHP-FPM, Node.js, or Python WSGI servers) in conjunction with web servers to handle dynamic content generation.\n\n\n\n\n\n\nNote Static vs Dynamic Applications\n\n\n\nStatic web applications serve the same content (HTML, CSS, JavaScript, images) to all users. Dynamic web applications generate content based on user input, database queries, or other variables, requiring server-side processing.\n\n\n\n\nSecurity\nSecurity is a critical aspect of web server management. You should keep in mind that a web server is typically open to anyone on the internet, and therefore, exposed to many cyber attacks. Here are some key security measures to consider:\n\nFirewall: Configure a firewall to restrict access to the web server. Only allow necessary ports (e.g., 80 for HTTP, 443 for HTTPS).\nSSL/TLS: Use SSL/TLS certificates to encrypt data in transit. This ensures that data exchanged between the client and server is secure.\nRegular Updates: Keep the web server software and operating system up to date with the latest security patches.\nAccess Control: Implement strict access controls to limit who can access the server and its resources. Use SSH keys for secure remote access instead of passwords.\nHardening: Follow best practices for hardening your web server, such as disabling unnecessary modules, using secure configurations, and limiting user permissions, such as SSL configurations recommended by the Mozilla organisation\nMonitoring: Set up logging and monitoring to detect unusual activity or potential security issues.\nBackup: Regularly back up your web server data and configurations to recover from potential data loss or security incidents.\n\n\n\nMaintenance\nTo ensure optimal performance and security of a web server, you need to perform regular maintenance tasks. Plan for regular maintenance activities:\n\nMonitoring: Regularly check server logs for errors or unusual activity.\nUpdates and patches: Establish procedures for applying security updates and software patches in a timely manner.\nBackups: Regularly back up server configurations and data to prevent loss.\nRenewal of SSL Certificates: Ensure that SSL/TLS certificates are renewed before they expire to maintain secure connections.\n\n\n\n\n\n\n\nTip Do not overlook maintenance\n\n\n\nThe time, cost and effort of maintaining a web server is often overlooked. While initial setup may be straightforward, maintaining security, availability, and performance requires continuous attention and resources. Unmaintained web servers are vulnerable to security and privacy threats.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Web servers"
    ]
  },
  {
    "objectID": "docs/resources/curriculum.html",
    "href": "docs/resources/curriculum.html",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "These materials represent a curated curriculum designed to help you develop and maintain your repository and code base. All of the resources listed below are free to access and use, and supplementary material like video lessons has been added where possible and relevant.\n\n\n\nWhat is Bash? - of all the shells available, Bash is one of the most popular, the most powerful, and the most friendly\nBash Essentials ‚Äì Bash commands commonly used to navigate file directories in your Terminal/GitBash\nThe Unix Shell lesson from Software Carpentries - use of the shell is fundamental to using a wide range of other powerful tools and computing resources. These lessons will start you on a path towards using these resources effectively\nInstallation Instructions - particularly important for Windows users, as Bash comes pre-installed on Mac. Windows users will need to install GitBash following the instructions at this link\nUsing the Terminal in Mac - The Terminal app allows you to control your Mac using a command prompt\n\n\n\n\n\nWhat is Git?- 2-minute video overview of the technology and how it works\nInstall Git from GitHub Guides - Check if Git is already installed on your machine; if not, follow these instructions to get started. Notes: If you‚Äôve already installed GitBash on Windows OS, you will have Git already. Installing GitHub Desktop will also install the latest version of Git if you don‚Äôt already have it.\nInstalling Git from Software Carpentries - Alternative installation instructions from Software Carpentries, including videos and details per OS.\nIntro to version control with Git from Code Refinery ‚Äì self-paced introductory lesson to version control using Git\nGit Intro video lesson from Code Refinery - Day 1 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 1/2\nGit Intro video lesson from Code Refinery - Day 2 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 2/2\nBranching and merging ‚Äì lesson from Code Refinery on concept of branching in Git (featuring octopus diagram)\nWhat is .gitignore? ‚Äì introduction to how and why to use the .gitignore file to not track some files in a project folder (e.g., because of their size or sensitivity)\nGit command cheat sheet ‚Äì commonly used Git commands in one page that can also be downloaded\n\n\n\n\n\nUnderstanding the GitHub flow ‚Äì guide from GitHub on how and why to work with branches\nCollaborative distributed version control - We have learned how to make a git repository for a single person. What about sharing?\nSSH connection to GitHub ‚Äì instructions to set up SSH connection to GitHub so that you do not need to input your login credentials with every push/pull\nGitlab and SSH keys - instructions to add an SSH key to your (TU Delft) GitLab account for the same reason as above\nGitHub without the Command Line from Code Refinery - practice collaborating and sharing using either the GitHub website or GitHub desktop application\nGitHub Guides: Mastering Markdown - Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\n\n\n\n\n\nIntroduction to Jupyter and JupyterLab - lesson material on the user interface of JupyterLab, how Jupyter notebooks work, and what some common and powerful usecases are\n\n\n\n\n\nAnaconda Installation Guide from Software Carpentries - Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with the latest version of Python and Jupyter Notebooks by default\nIntro to Anaconda Navigator - Anaconda Navigator is a graphical user interface to the conda package and environment manager. This 10-minute guide to Navigator will have you navigating the powerful conda program in a web-like interface without having to learn command line commands\nIntroduction to Conda for (Data) Scientists - Conda is an open source package and environment management system that easily creates, saves, loads, and switches between environments on your local computer\nManaging Conda environments - documentation on performing a range of common tasks with Conda using the command line\n\n\n\n\n\nScientific Computing with Python - a free video course series that teaches the basics of using Python 3\nApplied Data Science with Python Specialization - Coursera course in which you can enroll for free\nLearnPython.org - Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the Python programming language\nProgramming with Python from Software Carpentries - this introduction to Python is built around a common scientific task: data analysis\nPlotting and Programming with Python from Software Carpentries - an introduction to programming in Python for people with little or no previous programming experience using plotting as its motivating example\nData Analysis and Visualization with Python for Social Scientists - basic information about Python syntax, the Jupyter notebook interface, how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python\nCan You Speak Python? - test your knowledge of some important features of the Python programming language and the NumPy and Pandas libraries\n\n\n\n\n\nGetting started with Pandas - documentation and quick start guide for Pandas, an essential Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data\nPandas Tutorial - 14-part tutorial series featuring live code examples and tests of your knowledge\nPandas Data Wrangling Cheat Sheet - a cheat sheet of some of the most used syntax that you probably don‚Äôt want to miss\nPandas Cheat Sheet - Visual - visual, printable 2-page reference guide on commonly performed operations using Pandas\nUltimate Pandas Guide ‚Äî Inspecting Data Like a Pro - Whether you‚Äôre working on a simple analysis or a complex machine learning model, there‚Äôs a lot of value in being able to answer quick, exploratory questions about the nature of your data. This is a walk through of several DataFrame attributes and methods that make data inspection painless and productive\n10 Efficient Ways for Inspecting a Pandas DataFrame Object - A guide to using pandas effectively and efficiently\n\n\n\n\n\nGetting Started with Plotly - The plotly Python library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases\nPlotly Python Open Source Graphing Library - Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts\nHeatmaps with Plotly - How to make Heatmaps in Python with Plotly\n\n\n\n\n\nIpywidgets documentation - ipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.\nIntroduction to ipywidgets - in this tutorial video, learn about ipywidgets, a Python library for building interactive HTML widgets for your Jupyter browser.\nIpywidgets Interact Function | ipywidgets Examples of Slider, Dropdown, Checkbox, Text Box - Video demo on how to make an ipywidgets slider, ipywidgets dropdown, ipywidgets checkbox, or an ipywidgets text box using Python code.\n\n\n\n\n\nInstall R guide from Software Carpentries - R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio, which must also be installed separately from here\nProgramming with R from Software Carpentries - this introduction to R is built around a common scientific task: data analysis\nR for Reproducible Data Analysis from Software Carpentries - write modular code and best practices for using R for data analysis\nR for Social Scientists - basic information about R syntax, the RStudio interface, how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting\n\n\n\n\n\nUsing git with MATLAB - Introduction into using MATLAB and version control with git\nProgramming with MATLAB - Lesson from the Software Carpentries on the basics of programming with MATLAB\n\n\n\n\n\nWriting tests - lesson from CodeRefinery on automated testing\nVideo testing lesson - recording from software testing workshop by Code Refinery\nModular coding - modular code development from Code Refinery\n\n\n\n\n\nInstalling Docker - installation instructions for Windows, macOS, and Linux\nInstall WSL2 update - manual WSL2 update for Windows\nDockerfile reference - information on how to write a Dockerfile\n\n\n\n\n\nSetting up VSCode for Linux - guide to getting started using VSCode with Windows Subsystem for Linux\n\n\n\n\n\nGitHub Actions introduction course - an introductory course from GitHub on how to use GitHub Actions\n\n\n\n\n\nReproducible Research material from Code Refinery - demonstrates how version control, workflows, containers, and package managers can be used to record reproducible environments and computational steps\nReproducible Research video lesson from Code Refinery - Recorded video lesson from Code Refinery workshop in May 2021 on Reproducible Research material\nData + Code + Software = PDF - Slides to an overview on how to integrate data and software into a PDF."
  },
  {
    "objectID": "docs/resources/curriculum.html#bash",
    "href": "docs/resources/curriculum.html#bash",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "What is Bash? - of all the shells available, Bash is one of the most popular, the most powerful, and the most friendly\nBash Essentials ‚Äì Bash commands commonly used to navigate file directories in your Terminal/GitBash\nThe Unix Shell lesson from Software Carpentries - use of the shell is fundamental to using a wide range of other powerful tools and computing resources. These lessons will start you on a path towards using these resources effectively\nInstallation Instructions - particularly important for Windows users, as Bash comes pre-installed on Mac. Windows users will need to install GitBash following the instructions at this link\nUsing the Terminal in Mac - The Terminal app allows you to control your Mac using a command prompt"
  },
  {
    "objectID": "docs/resources/curriculum.html#git",
    "href": "docs/resources/curriculum.html#git",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "What is Git?- 2-minute video overview of the technology and how it works\nInstall Git from GitHub Guides - Check if Git is already installed on your machine; if not, follow these instructions to get started. Notes: If you‚Äôve already installed GitBash on Windows OS, you will have Git already. Installing GitHub Desktop will also install the latest version of Git if you don‚Äôt already have it.\nInstalling Git from Software Carpentries - Alternative installation instructions from Software Carpentries, including videos and details per OS.\nIntro to version control with Git from Code Refinery ‚Äì self-paced introductory lesson to version control using Git\nGit Intro video lesson from Code Refinery - Day 1 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 1/2\nGit Intro video lesson from Code Refinery - Day 2 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 2/2\nBranching and merging ‚Äì lesson from Code Refinery on concept of branching in Git (featuring octopus diagram)\nWhat is .gitignore? ‚Äì introduction to how and why to use the .gitignore file to not track some files in a project folder (e.g., because of their size or sensitivity)\nGit command cheat sheet ‚Äì commonly used Git commands in one page that can also be downloaded"
  },
  {
    "objectID": "docs/resources/curriculum.html#githubgitlab-remote-repositories",
    "href": "docs/resources/curriculum.html#githubgitlab-remote-repositories",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Understanding the GitHub flow ‚Äì guide from GitHub on how and why to work with branches\nCollaborative distributed version control - We have learned how to make a git repository for a single person. What about sharing?\nSSH connection to GitHub ‚Äì instructions to set up SSH connection to GitHub so that you do not need to input your login credentials with every push/pull\nGitlab and SSH keys - instructions to add an SSH key to your (TU Delft) GitLab account for the same reason as above\nGitHub without the Command Line from Code Refinery - practice collaborating and sharing using either the GitHub website or GitHub desktop application\nGitHub Guides: Mastering Markdown - Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform."
  },
  {
    "objectID": "docs/resources/curriculum.html#jupyter-notebooks-and-jupyterlab",
    "href": "docs/resources/curriculum.html#jupyter-notebooks-and-jupyterlab",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Introduction to Jupyter and JupyterLab - lesson material on the user interface of JupyterLab, how Jupyter notebooks work, and what some common and powerful usecases are"
  },
  {
    "objectID": "docs/resources/curriculum.html#anaconda-navigator-and-managing-conda-environments",
    "href": "docs/resources/curriculum.html#anaconda-navigator-and-managing-conda-environments",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Anaconda Installation Guide from Software Carpentries - Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with the latest version of Python and Jupyter Notebooks by default\nIntro to Anaconda Navigator - Anaconda Navigator is a graphical user interface to the conda package and environment manager. This 10-minute guide to Navigator will have you navigating the powerful conda program in a web-like interface without having to learn command line commands\nIntroduction to Conda for (Data) Scientists - Conda is an open source package and environment management system that easily creates, saves, loads, and switches between environments on your local computer\nManaging Conda environments - documentation on performing a range of common tasks with Conda using the command line"
  },
  {
    "objectID": "docs/resources/curriculum.html#python",
    "href": "docs/resources/curriculum.html#python",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Scientific Computing with Python - a free video course series that teaches the basics of using Python 3\nApplied Data Science with Python Specialization - Coursera course in which you can enroll for free\nLearnPython.org - Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the Python programming language\nProgramming with Python from Software Carpentries - this introduction to Python is built around a common scientific task: data analysis\nPlotting and Programming with Python from Software Carpentries - an introduction to programming in Python for people with little or no previous programming experience using plotting as its motivating example\nData Analysis and Visualization with Python for Social Scientists - basic information about Python syntax, the Jupyter notebook interface, how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python\nCan You Speak Python? - test your knowledge of some important features of the Python programming language and the NumPy and Pandas libraries"
  },
  {
    "objectID": "docs/resources/curriculum.html#pandas",
    "href": "docs/resources/curriculum.html#pandas",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Getting started with Pandas - documentation and quick start guide for Pandas, an essential Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data\nPandas Tutorial - 14-part tutorial series featuring live code examples and tests of your knowledge\nPandas Data Wrangling Cheat Sheet - a cheat sheet of some of the most used syntax that you probably don‚Äôt want to miss\nPandas Cheat Sheet - Visual - visual, printable 2-page reference guide on commonly performed operations using Pandas\nUltimate Pandas Guide ‚Äî Inspecting Data Like a Pro - Whether you‚Äôre working on a simple analysis or a complex machine learning model, there‚Äôs a lot of value in being able to answer quick, exploratory questions about the nature of your data. This is a walk through of several DataFrame attributes and methods that make data inspection painless and productive\n10 Efficient Ways for Inspecting a Pandas DataFrame Object - A guide to using pandas effectively and efficiently"
  },
  {
    "objectID": "docs/resources/curriculum.html#plotly",
    "href": "docs/resources/curriculum.html#plotly",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Getting Started with Plotly - The plotly Python library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases\nPlotly Python Open Source Graphing Library - Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts\nHeatmaps with Plotly - How to make Heatmaps in Python with Plotly"
  },
  {
    "objectID": "docs/resources/curriculum.html#ipywidgets",
    "href": "docs/resources/curriculum.html#ipywidgets",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Ipywidgets documentation - ipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.\nIntroduction to ipywidgets - in this tutorial video, learn about ipywidgets, a Python library for building interactive HTML widgets for your Jupyter browser.\nIpywidgets Interact Function | ipywidgets Examples of Slider, Dropdown, Checkbox, Text Box - Video demo on how to make an ipywidgets slider, ipywidgets dropdown, ipywidgets checkbox, or an ipywidgets text box using Python code."
  },
  {
    "objectID": "docs/resources/curriculum.html#r",
    "href": "docs/resources/curriculum.html#r",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Install R guide from Software Carpentries - R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio, which must also be installed separately from here\nProgramming with R from Software Carpentries - this introduction to R is built around a common scientific task: data analysis\nR for Reproducible Data Analysis from Software Carpentries - write modular code and best practices for using R for data analysis\nR for Social Scientists - basic information about R syntax, the RStudio interface, how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting"
  },
  {
    "objectID": "docs/resources/curriculum.html#matlab",
    "href": "docs/resources/curriculum.html#matlab",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Using git with MATLAB - Introduction into using MATLAB and version control with git\nProgramming with MATLAB - Lesson from the Software Carpentries on the basics of programming with MATLAB"
  },
  {
    "objectID": "docs/resources/curriculum.html#modular-code-and-testing",
    "href": "docs/resources/curriculum.html#modular-code-and-testing",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Writing tests - lesson from CodeRefinery on automated testing\nVideo testing lesson - recording from software testing workshop by Code Refinery\nModular coding - modular code development from Code Refinery"
  },
  {
    "objectID": "docs/resources/curriculum.html#docker",
    "href": "docs/resources/curriculum.html#docker",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Installing Docker - installation instructions for Windows, macOS, and Linux\nInstall WSL2 update - manual WSL2 update for Windows\nDockerfile reference - information on how to write a Dockerfile"
  },
  {
    "objectID": "docs/resources/curriculum.html#vscode",
    "href": "docs/resources/curriculum.html#vscode",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Setting up VSCode for Linux - guide to getting started using VSCode with Windows Subsystem for Linux"
  },
  {
    "objectID": "docs/resources/curriculum.html#continuous-integration",
    "href": "docs/resources/curriculum.html#continuous-integration",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "GitHub Actions introduction course - an introductory course from GitHub on how to use GitHub Actions"
  },
  {
    "objectID": "docs/resources/curriculum.html#reproducible-research",
    "href": "docs/resources/curriculum.html#reproducible-research",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Reproducible Research material from Code Refinery - demonstrates how version control, workflows, containers, and package managers can be used to record reproducible environments and computational steps\nReproducible Research video lesson from Code Refinery - Recorded video lesson from Code Refinery workshop in May 2021 on Reproducible Research material\nData + Code + Software = PDF - Slides to an overview on how to integrate data and software into a PDF."
  },
  {
    "objectID": "docs/software/automation/github_ci_cd.html",
    "href": "docs/software/automation/github_ci_cd.html",
    "title": "GitHub Actions",
    "section": "",
    "text": "GitHub Actions is a CI/CD platform that automates building, testing, and deploying code. You can set up workflows that build and test every pull request in your repository, and optionally deploy changes after those pull requests are merged, triggered automatically based on your chosen rules.\n\n\n\n\n\n\nTip Exploring GitHub Actions\n\n\n\nSetting up a demo workflow in GitHub Actions.\n\n\n\n\n\n\n\n\nTip Tips\n\n\n\n\nGitHub has templates available. Go to the Actions tab in your repository and select new workflow for an overview.\nYou can find workflow examples shared in the community.\nAdd workflow_dispatch as a trigger for your GitHub Actions workflow. With this trigger, you can manually run an action, instead of having to rely on external triggers. This is quite useful when testing your workflow.\nTest your workflow in a separate branch to avoid committing many small changes during debugging of the workflow.\n\n\n\n\nAutomating testing\nA common usecase of automation is to trigger automatic testing when pushing changes and creating pull requests.\n\nPythonMATLAB\n\n\nThe example below is taken from the CodeRefinery lesson on Continuous Integration.\n\n\n\n\n\n\nNote Python testing\n\n\n\n\n\nname: Python package testing\n\non:\n  push:\n    branches: [ \"main\", \"develop\" ]\n  pull_request:\n    branches: [ \"main\", \"develop\" ]\n  workflow_dispatch:\n  \njobs:\n  test:\n    permissions:\n      contents: read\n      pull-requests: write\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.10\"\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install flake8 pytest pytest-cov\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n    - name: Lint with flake8\n      run: |\n        # stop the job if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Test with pytest and calculate coverage\n      run: |\n        pytest --cov-report \"xml:coverage.xml\"  --cov=.\n    - name: Create Coverage \n      if: ${{ github.event_name == 'pull_request' }}\n      uses: orgoro/coverage@v3.1\n      with:\n          coverageFile: coverage.xml\n          token: ${{ secrets.GITHUB_TOKEN }}\n\n\n\n\n\nMATLAB has multiple pre-defined GitHub Actions available to use in your workflows.\n\n\n\n\n\n\nNote MATLAB testing\n\n\n\n\n\nname: Generate Test and Coverage Artifacts\non:\n  push:\n    branches: [ \"main\", \"develop\" ]\n  pull_request:\n    branches: [ \"main\", \"develop\" ]\n  workflow_dispatch:\n \njobs:\n  test:\n    name: Run MATLAB Tests\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n      - name: Set up MATLAB\n        uses: matlab-actions/setup-matlab@v2\n        with:\n          release: R2024a\n      - name: Run tests\n        uses: matlab-actions/run-tests@v2\n        with:\n          source-folder: src\n          test-results-junit: test-results/results.xml\n          code-coverage-cobertura: code-coverage/coverage.xml\n      - name: Upload coverage reports to Codecov\n        uses: codecov/codecov-action@v4\n        with:\n          file: code-coverage/coverage.xml\n          token: ${{ secrets.CODECOV_TOKEN }}\n\n\n\nThis workflow uses Codecov to analyse the coverage report (free service for public repositories).\n‚Æï Learn more about Codecov\n\n\n\n\n\nAutomating documentation generation\nGitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.\nIn order to deploy the documentation generated by the workflow below, you need to navigate to Settings -&gt; Pages in your repository and set:\n\nSource: ‚ÄúDeploy from a branch‚Äù\nBranch: gh-pages from root\n\nIt is a best practice to only deploy new documentation to the gh-pages branch upon a Pull Request to the main branch. This avoids mismatches between the available source code and the documentation.\n\n\n\n\n\n\nNote Sphinx building with gh-pages from branch\n\n\n\n\n\nname: Sphinx documentation\n\non: [push, pull_request, workflow_dispatch]\n\npermissions:\n  contents: write\n\njobs:\n  docs:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n      - name: Install dependencies\n        run: |\n          pip install -r docs/requirements.txt\n      - name: Sphinx build\n        run: |\n          sphinx-build docs/ docs/_build/\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v4\n        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}\n        with:\n          publish_branch: gh-pages\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: docs/_build/\n          force_orphan: true\n\n\n\n\n\n\n\n\n\nNote Customizing the workflow\n\n\n\nThis example assumes you have a separate requirements.txt in the /docs folder. Update the location of the requirements.txt if you store the Sphinx dependencies in the root.\n\n\n\n\n\n\n\n\nWarning Warning\n\n\n\n\nWith the GitHub Free plan, Pages cannot be deployed from a private repository.\nGitHub Pages sites are publicly available on the internet, even if the repository for the site is private.\n\n\n\n\n\nWorkflows for building Python packages\nYou can automate publishing a new version of your Python package with a GitHub Action. Notice that in the workflow below, the trigger is the creation of a new release on GitHub.\n\n\n\n\n\n\nNote Building a Python package\n\n\n\n\n\nname: Upload Python Package\n\non:\n  release:\n    types: [published]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install build\n      - name: Build package\n        run: python -m build\n      - name: Publish package\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          password: ${{ secrets.PYPI_API_TOKEN }}\n\n\n\nThis action should only run after all other workflows (such as testing and building) have passed.\n\n\nAdditional concepts\n\nVariables and secrets\nSecrets are variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in GitHub Actions workflows. GitHub Actions can only read a secret if you explicitly include the secret in a workflow.\n‚Æï More information on using secrets in GitHub Actions.\n\n\nMatrix strategy\nA matrix strategy lets you use variables in a single job definition to automatically create multiple job runs that are based on the combinations of the variables. For example, you can use a matrix strategy to test your code in multiple versions of a language or on multiple operating systems.\nA job will run for each possible combination of the variables. In the example below, the workflow will run 12 jobs, one for each combination of the os and python-version variables.\njobs:\n  example_matrix:\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        python-version: ['3.10', '3.11', '3.12', '3.13']       \n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n‚Æï More information on using a matrix for your jobs.\n\n\n\n\n\n\nNote Note\n\n\n\nIf you want deterministic runner environments, specify exact versions (e.g., macos-15) instead of latest, since the latest aliases will migrate over time.\n\n\n\n\nArtifacts\nArtifacts are files or sets of files that are produced during the execution of a workflow and need to be stored or shared between jobs in a workflow.\nTo upload an artifact, you typically add a step in your workflow:\nsteps:\n  - name: Upload build output\n    uses: actions/upload-artifact@v4\n    with:\n      name: build-output\n      path: &lt;path/to/build/output&gt;\nArtifacts can be downloaded in subsequent jobs of the same workflow run using the actions/download-artifact action. This is done with a step like:\nsteps:\n  - name: Download build output\n    uses: actions/download-artifact@v4\n    with:\n      name: build-output\n      path: &lt;path/to/download&gt;\nBy default, artifacts are retained for 90 days, but this can be configured per workflow or at the repository/organization level.\n\n\n\nSonar\nTo automate checking your code quality, you can also make use of a third-party service. SonarQube Cloud (previously known as SonarCloud) is a cloud-based code analysis service designed to detect coding issues in many different programming languages. The free plan allows you to analyze an unlimited number of public repositories. Private projects will not be importable on this plan.\n\nMake your repository public.\nLink your GitHub repository via their login page.\nFollow the instructions to set up the code analysis.\n\nYou can also integrate SonarQube Cloud code analysis in GitHub Actions. Typically, you would create a new workflow file, for example .github/workflows/sonar.yml, and configure triggers to your needs. You will need to set up a token in GitHub Secrets and configure what needs to be analyzed in sonar-project.properties.\n\n\n\n\n\n\nNote Example of SonarQube Cloud GitHub Action\n\n\n\n\n\nname: SonarQube Cloud Workflow\non:\n  push:\n    branches:\n      - main\n  pull_request:\n      types: [opened, synchronize, reopened]\n\njobs:\n  sonarqube:\n    name: SonarQube\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  \n      - name: SonarQubeScan\n        uses: SonarSource/sonarqube-scan-action@v4\n        env: \n          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\n\n\n\nThe basic usage step-by-step is described in their GitHub repository.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nPossible analysis parameters",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitHub Actions"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/runner_matlab.html",
    "href": "docs/software/automation/gitlab/runner_matlab.html",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "",
    "text": "With the continuous method of software development, you continuously build, test, and deploy iterative code changes. This iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.\n\n\nWith this guide, you will create a Continuous Integration Pipeline on a repository within the TU Delft Gitlab to use a Matlab environment.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Setting up a GitLab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/runner_matlab.html#background",
    "href": "docs/software/automation/gitlab/runner_matlab.html#background",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "",
    "text": "With the continuous method of software development, you continuously build, test, and deploy iterative code changes. This iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.\n\n\nWith this guide, you will create a Continuous Integration Pipeline on a repository within the TU Delft Gitlab to use a Matlab environment.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Setting up a GitLab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/runner_matlab.html#prerequisites",
    "href": "docs/software/automation/gitlab/runner_matlab.html#prerequisites",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID\nMATLAB account\nBasic knowledge of Linux (for setting up a server)\nBasic knowledge of Docker (for creating a custom MATLAB image)\n\n\n\n\n\n\n\nTip\n\n\n\nTo learn more about Docker containers, please look at the Reproducible Computational Environments Using Docker lesson from the Software Carpentries.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Setting up a GitLab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/runner_matlab.html#glossary-of-terms",
    "href": "docs/software/automation/gitlab/runner_matlab.html#glossary-of-terms",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Glossary of terms",
    "text": "Glossary of terms\nCI/CD pipeline\nA CI/CD pipeline automates your software delivery process. The pipeline builds code, runs tests (Continuous Intergation), and safely deploys a new version of the application (Continuous Delivery).\nDocker\nWe use a Docker container to run the Gitlab runner and initialise the CI/CD pipeline.\nGitlab runner (from GitLab documentation)\nRunners are the agents that run the CI/CD jobs that come from GitLab. When you register a runner, you are setting up communication between your GitLab instance and the machine where GitLab Runner is installed. Runners usually process jobs on the same machine where you installed GitLab Runner.\nGitlab jobs\nPipeline configuration begins with jobs. Jobs are the most fundamental element of a .gitlab-ci.yml file. Each job is executed by a Gitlab runner. See Gitlab documentation for more info.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Setting up a GitLab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/software/automation/gitlab/runner_matlab.html#steps",
    "href": "docs/software/automation/gitlab/runner_matlab.html#steps",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Steps",
    "text": "Steps\n\nRequest a TU Delft Virtual Private Server\nSet up a Gitlab runner\nCreate a Docker image with a custom Matlab installation\nRegister a gitlab runner for the Matlab container\nObtain a Matlab license file\nConfigure the CI/CD pipeline\nAdd a job to test the pipeline\nOptional: Updating the Matlab version\n\n\nStep 1. Request a TU Delft VPS\nIf you want to work with the TU Delft Gitlab instance and you want to implement CI/CD pipelines, then you need to install a Gitlab runner on your own. Runners are the agents that run the CI/CD jobs that come from GitLab. Currently, the TU Delft instance does not provide this feature out-of-the-box. Therefore, we need a separate (virtual) server to run the Gitlab runners and execute the jobs in the CI/CD pipeline.\nThe TU Delft offers Virtual Private Servers (VPS) for researchers through the TopDesk selfservice portal. If you don‚Äôt have a VPS already, please follow this guide to request a Virtual Private Server)\nVPS requirements\n\n50Gb disk space (the Matlab installation in this guide requires ~10 Gb, but this depends on the size of the installed addons)\n\n\n\nStep 2. Setting up Gitlab runners\nTo set up a gitlab runner on the VPS, please follow this guide for setting up GitLab runners.\nTLDR\n\nInstall docker with\nsudo apt install docker.io\nVerify installation with\nsudo docker --version\nOptional: Move default storage location to larger drive\nIf the file space in the Docker Root directory is not adequate, we must relocate the Docker Root. Please consult this guide for instructions.\nDeploy the gitlab-runner with\ndocker run -d --name gitlab-runner --restart always \\\n-v /srv/gitlab-runner/config:/etc/gitlab-runner \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\ngitlab/gitlab-runner:latest\nVerify deployment with\nsudo docker ps -a\n\n\n\nStep 3. Create a Docker image containing a custom Matlab installation\nIn order for a Gitlab runner to execute MATLAB code, it needs to be able to access a container with MATLAB installed. The aim of this step is to create a Docker image with MATLAB installation that can be used by a Gitlab runner. By building our own Docker image, we can specify the MATLAB version and customize the installed toolboxes.\n\n\n\n\n\n\nNote\n\n\n\nWe have looked into using the Docker images developed by Mathworks. When running these images, you are prompted to supply your MATLAB‚Äôs account username and password to activate the instance. Although it is possible to create a new image from such an activated container and use it on the VPS, we have so far not been able to get this solution working with Gitlab runners. We thus rely on downloading a license file (step 6) and storing it as a Variable on Gitlab (step 7).\n\n\nThis Dockerfile is based on MATLAB‚Äôs Dockerfile template. We will make the following modifications to this template:\n\nset bash as the default run command (Gitlab runners need to access a shell)\nadd additional MATLAB products with the flag --products. In this example, we have added the Parallel Computing Toolbox and the Mapping Toolbox.\n\nIn your user folder on the VPS (/home/username), create a file called Dockerfile\nsudo nano Dockerfile\nand copy the content below in the Dockerfile. Make sure to update the MATLAB release and installed addons to your requirements (see in bold).\n\n\n\n\n\n\n# Copyright 2019 - 2021 The MathWorks, Inc.\n\n# To specify which MATLAB release to install in the container, edit the value of the MATLAB_RELEASE argument.\n# Use lower case to specify the release, for example: ARG MATLAB_RELEASE=r2020a\nARG MATLAB_RELEASE=r2021b\n\n# When you start the build stage, this Dockerfile by default uses the Ubuntu-based matlab-deps image.\n# To check the available matlab-deps images, see: https://hub.docker.com/r/mathworks/matlab-deps\nFROM mathworks/matlab-deps:${MATLAB_RELEASE}\n\n# Declare the global argument to use at the current build stage\nARG MATLAB_RELEASE\n\n# Install mpm dependencies\nRUN export DEBIAN_FRONTEND=noninteractive && apt-get update && \\\n    apt-get install --no-install-recommends --yes \\\n    wget \\\n    unzip \\\n    ca-certificates && \\\n    apt-get clean && apt-get autoremove\n\n# Run mpm to install MATLAB in the target location and delete the mpm installation afterwards\nRUN wget -q https://www.mathworks.com/mpm/glnxa64/mpm && \\ \n    chmod +x mpm && \\\n    ./mpm install \\\n    --release=${MATLAB_RELEASE} \\\n    --destination=/opt/matlab \\\n    --products MATLAB Parallel_Computing_Toolbox Mapping_Toolbox && \\\n    rm -f mpm /tmp/mathworks_root.log && \\\n    ln -s /opt/matlab/bin/matlab /usr/local/bin/matlab\n\n# Add \"matlab\" user and grant sudo permission.\nRUN adduser --shell /bin/bash --disabled-password --gecos \"\" matlab && \\\n    echo \"matlab ALL=(ALL) NOPASSWD: ALL\" &gt; /etc/sudoers.d/matlab && \\\n    chmod 0440 /etc/sudoers.d/matlab\n\n# Set user and work directory\nUSER matlab\nWORKDIR /home/matlab\nCMD [\"bash\"]\n\n\n\n\nTo build a Docker image with the name matlab-gitlab and the version reference r2021b, run the following command in the folder containing the Dockerfile:\nsudo docker build . -t matlab-gitlab:r2021b\nYou can verify the presence of the image with\nsudo docker images\nThis image is now available locally on the VPS.\n\n\n\n\n\n\nTip\n\n\n\nYou can also upload your Docker image to Dockerhub and have it available from there. This removes the need to build the image on the VPS as it can be pulled directly from DockerHub.\n\n\n\n\nStep 4. Register the MATLAB runner\nAfter deploying the gitlab-runner in step 2, we need to register a new runner for our matlab-gitlab image. Run the following command to register your runner and configure it to deploy in a Docker container on your server.\ndocker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register\nIn response to this command you will be prompted to answer a series of questions. You can find the required gitlab-ci token in your Gitlab repository under Settings -&gt; CI/CD -&gt; Runners:\nsudo docker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register \\\n  --non-interactive \\\n  --url \"https://gitlab.tudelft.nl/\" \\\n  --registration-token \"REPOSITORY_TOKEN\" \\\n  --executor \"docker\" \\\n  --docker-image matlab-gitlab:r2021b \\\n  --description \"matlab-runner\" \\\n  --tag-list \"matlab\" \\\n  --docker-privileged=true \\\n  --docker-cap-add \"NET_ADMIN\" \\\n  --docker-pull-policy \"if-not-present\" \\\nFor the changes to take effect, restart the gitlab-runner with\nsudo docker restart gitlab-runner\nThe runner configurations are stored in /srv/gitlab-runner/config/config.toml. If you would like to view or or modify the MATLAB runner, run\nsudo nano /srv/gitlab-runner/config/config.toml\nAfter registering the runner, the configuration file should contain:\n\n\n\n\n\n\nNote\n\n\n\n\n\nconcurrent = 4\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = \"matlab-gitlab\"\n  url = \"https://gitlab.tudelft.nl\"\n  token = \"&lt;token&gt;\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"matlab-gitlab:r2021b\"\n    privileged = true\n    disable_entrypoint_overwrite = false\n    cap_add = [\"NET_ADMIN\"]\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/cache\"]\n    pull_policy = \"if-not-present\"\n    shm_size = 0\n\n\n\n\n\nStep 5. Obtain a MATLAB license file\nEvery TU Delft employee has access to an Individual MATLAB license. Normally, you would activate MATLAB only once after installation through an online activation step. However, this does not work for a Docker container as it is relaunched for each CI trigger.\nThe following steps for activating MATLAB on an offline machine are adapted from the MATLAB Forum:\n\nObtain your Host ID\nObtain your computer login name or username\nActivate the license through the License Center to obtain license file\n\n1. Obtain your Host ID\nThe MATLAB license can only be activated for a specifc computer. In the Docker container, we will set the hostID of the container to 0242ac11ffff.\n\n\n\n\n\n\nNote\n\n\n\nDocker automatically assigns an IP address to each running container, starting from 172.17.0.2 until 172.17.0.255. These IP addresses determine the container‚Äôs MAC address (see here for more details), which in turn needs to match with our license. To prevent the MAC address of the MATLAB container from switching and thereby invalidating the license, we will set it to 02:42:ac:11:ff:ff in the .gitlab-ci.yml file.\n\n\n2. Obtain your computer login name or username\nThe MATLAB license is created for a specific user. In the Docker container, we will set the username to matlab.\n3. Activate the license through the License Center to obtain license file\n\nGo to the License Center: https://www.mathworks.com/mwaccount\nUnder My Software, click the license number you want to activate. If you do not see your license number, in the bottom right hand corner, click View Additional Licenses or Trials.\nClick the Install and Activate tab\nClick Activate to Retrieve License File and/or Activate a Computer\nEnter the following information:\n\nthe release you are activating = r2021b (same version as in the Dockerfile)\nthe operating system = Linux\nthe host ID = 0242ac11ffff\nyour user or login name = matlab\nthe Activation Label = matlab-gitlab\n\n\nDownload the license.lic file\n\n\n\nStep 6. Configure the CI/CD pipeline on Gitlab\nBefore we can run a CI job, we need to configure a few settings in our Gitlab repository\n1. Add tag to MATLAB runner\nUnder Settings -&gt; CI/CD -&gt; Runners we can find the available specific runners. Press the edit button on the matlab-gitlab runner and add the tag matlab-gitlab. With this, we can call more easily call this specific runner within our CI pipeline.\n2. Add license as Variable\nUnder Settings -&gt; CI/CD -&gt; Variables add a new variable called MATLAB_LICENSE, past the content of the downloaded license.lic file and set type to file. Having the license available as a Gitlab variable allows us to update it without having to change the MATLAB image.\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we could have added the license file directly to the Docker image. With the license file in the same folder as the Dockerfile and adding the following command to the Dockerfile, we can build a Docker image with an activated MATLAB:\nCOPY license.lic /opt/matlab/licenses/\nHere, we opted to have it accessible through the Gitlab settings together with the accompanying hostid.\n\n\n\n\n\n\nWarning\n\n\n\nNever share any Docker images that contain license files or other confidential information.\n\n\n\n\n\n\nStep 7. Add a job to test the pipeline\nTo test the pipeline, add the following content to .gitlab-ci.yml via CI/CD -&gt; Editor in your repository.\nvariables:\n  MAC_ADDRESS: 02:42:ac:11:ff:ff\n\ncheck_matlab:\n  tags: \n    - matlab-gitlab\n  before_script:\n    # Change the mac-address to match the MATLAB license\n    - sudo ifconfig eth0 hw ether \"$MAC_ADDRESS\"\n\n    # Add the Matlab license to the Matlab installation in the container\n    - sudo mkdir /opt/matlab/licenses\n    - sudo mv ${MATLAB_LICENSE} /opt/matlab/licenses/license.lic   \n  script:    \n    # Run a MATLAB function/script through the -batch argument\n    - matlab -batch \"disp('hello world!')\"\nAfter commiting, the pipeline should run and execute the job check_matlab. You can check the status of the pipeline via CI/CD -&gt; Pipelines.\nIf all went well, you have successfully setup a Gitlab runner to run MATLAB code. Congrats!\n\n\nStep 8. Optional: Updating the MATLAB version\nIf you need to update the MATLAB version of the Docker container, you will need to go throught the following steps:\n\nUpdate the MATLAB version in the Dockerfile\nBuild the docker image with sudo docker build . -t matlab-gitlab:&lt;version&gt;\nDownload a new license.lic file (see step 5 of this guide)\nUpdate the CI Variable MATLAB_LICENSE with the new license content\nUpdate the image names (not the tags) in .gitlab-ci.yml to use the new image.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to test your code with multiple MATLAB versions to ensure backward compatibility, please look at this example to use multiple docker images.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Automation",
      "GitLab pipelines",
      "Setting up a GitLab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells.html",
    "href": "docs/software/code_quality/code_smells.html",
    "title": "Code smells",
    "section": "",
    "text": "CC-BY-4.0 ¬© 2021 Balaban et al.\n\n\nCode smells are software characteristics that suggest there might be an issue with the code‚Äôs design or implementation. While code smells themselves might not always indicate a bug or malfunction, they can make the code harder to understand and extend, which can lead to bugs and other issues down the line. Code smells are usually noticed and addressed during code reviews, when writing tests, adding new features, fixing bugs, and during automated code analysis.\n\n\n\n\n\n\nTip How to use these cards?\n\n\n\nEach guide provides an overview of a code smell, its symptoms and an example on how to refactor it. We don‚Äôt intend to cover all refactoring techniques, but we aim to provide a starting point for identifying and addressing common code smells.\n\n\n\n\n\nLong Method\nProblem: A function is very long and hard to understand.\n\n Refactor long methods\n\n\n\n\n\nLarge Classes\nProblem: A class contains too many responsibilities or functionalities.\n\n Refactor large classes\n\n\n\n\n\nCode Duplication\nProblem: The same or very similar code appears in multiple places.\n\n Refactor duplicate code\n\n\n\n\n\nHard-Coded Values\nProblem: Literal values (e.g., numeric values or strings) are directly embedded in the code.\n\n Refactor hard-coded values\n\n\n\n\n\nDeep Nesting\nProblem: There are excessive levels of nested for-loops or if-statements.\n\n Refactor nested logic\n\n\n\n\n\nMany Arguments\nProblem: Functions require a long list of parameters.\n\n Refactor argument lists\n\n\n\n\n\nInappropriate Intimacy\nProblem: Two classes or methods depend too much on each other‚Äôs internals.\n\n Refactor coupling\n\n\n\n\n\nSide Effects\nProblem: Changes in one part of the code cause unexpected behavior in another.\n\n Refactor side effects\n\n\n\n\n\nDead Code\nProblem: There is unused or commented-out code.\n\n Refactor unused code\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nTen simple rules for quick and dirty scientific programming\nGood enough practices in scientific computing",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html",
    "title": "Deep Nesting",
    "section": "",
    "text": "‚ÄúCode is like humor. When you have to explain it, it‚Äôs bad.‚Äù\nCory House\nDeep nesting occurs when there are too many levels of indentation in the code, making it harder to understand, maintain, and debug. It can lead to reduced readability, and increases cognitive load for developers.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#symptoms",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#symptoms",
    "title": "Deep Nesting",
    "section": "Symptoms",
    "text": "Symptoms\n\nExcessive indentation makes it hard to track logic.\nMany nested if statements or for loops.\nHard-to-follow branching logic.\nSlow performance due to inefficient code.\nIncreased likelihood of bugs due to complexity.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-conditional-statements",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-conditional-statements",
    "title": "Deep Nesting",
    "section": "Example - Deeply nested conditional statements",
    "text": "Example - Deeply nested conditional statements\n\nPythonR\n\n\ndef validate_model_convergence(model):\n    if model.convergence &gt; 1:\n        if model.convergence &lt; 0.1:\n            if model.secondary_condition == True:\n                return True\n            else:\n                return False\n        else:\n            return False\n    else:\n        return False\n\n\nvalidate_model_convergence &lt;- function(model) {\n  if (model$convergence &gt; 1) {\n    if (model$convergence &lt; 0.1) {\n      if (model$secondary_condition == TRUE) {\n        return(TRUE)\n      } else {\n        return(FALSE)\n      }\n    } else {\n      return(FALSE)\n    }\n  } else {\n    return(FALSE)\n  }\n}\n\n\n\n\nSolutions\nRefactoring deep nesting improves readability and maintainability. Techniques to reduce deep nesting include:\n\nUsing early returns to eliminate unnecessary indentation.\nExtracting complex logic into helper functions for better modularity.\nUsing built-in functions like any and all to simplify conditions.\n\n\nSolution 1: Using early returns\n\nPythonR\n\n\ndef validate_model_convergence(model):\n    if model.convergence &lt;= 1:\n        return False\n    if model.convergence &gt;= 0.1:\n        return False\n    if not model.secondary_condition:\n        return False\n    return True\n\n\nvalidate_model_convergence &lt;- function(model) {\n  if (model$convergence &lt;= 1) {\n    return(FALSE)\n  }\n  if (model$convergence &gt;= 0.1) {\n    return(FALSE)\n  }\n  if (!model$secondary_condition) {\n    return(FALSE)\n  }\n  return(TRUE)\n}\n\n\n\nThis solution uses early returns to reduce the nesting level and make the code more readable. Each condition is checked separately, and if it fails, the function returns immediately, avoiding further nesting and evaluation of unnecessary conditions.\n\n\nSolution 2: Using all for conciseness\nAlternatively, we can use the all function to check multiple conditions in a single line, which can make the code more concise and easier to read.\n\nPythonR\n\n\ndef validate_model_convergence(model):\n    return all([\n        model.convergence &gt; 1,\n        model.convergence &lt; 0.1,\n        model.secondary_condition,\n    ])\n\n\nvalidate_model_convergence &lt;- function(model) {\n  all(\n    model$convergence &gt; 1,\n    model$convergence &lt; 0.1,\n    model$secondary_condition\n  )\n}",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-loops",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-loops",
    "title": "Deep Nesting",
    "section": "Example - Deeply nested loops",
    "text": "Example - Deeply nested loops\nIn this example, we have three nested loops to iterate over three 3D arrays and sum their corresponding elements. This code can be refactored using vectorized operations to improve performance and readability.\n\nPythonR\n\n\nimport numpy as np\n\n# Create three random 10x10x10 arrays\nA = np.random.rand(10, 10, 10)\nB = np.random.rand(10, 10, 10)\nC = np.random.rand(10, 10, 10)\n\n# Using nested loops (inefficient)\nresult = np.zeros((10, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        for k in range(10):\n            result[i, j, k] = A[i, j, k] + B[i, j, k] + C[i, j, k]\n\n\n# Create three random 10x10x10 arrays\nA &lt;- array(runif(1000), dim = c(10, 10, 10))\nB &lt;- array(runif(1000), dim = c(10, 10, 10))\nC &lt;- array(runif(1000), dim = c(10, 10, 10))\n\n# Using nested loops (inefficient)\nresult &lt;- array(0, dim = c(10, 10, 10))\nfor (i in 1:10) {\n  for (j in 1:10) {\n    for (k in 1:10) {\n      result[i, j, k] &lt;- A[i, j, k] + B[i, j, k] + C[i, j, k]\n    }\n  }\n}\n\n\n\n\nSolution\nUsing vectorized operations, we can perform the same operation without nested loops, which is more efficient and easier to read.\n\nPythonR\n\n\nimport numpy as np\n\n# Create three random 10x10x10 arrays\nA = np.random.rand(10, 10, 10)\nB = np.random.rand(10, 10, 10)\nC = np.random.rand(10, 10, 10)\n\n# Vectorized solution (fast & efficient)\nresult = A + B + C\n\n\n# Create three random 10x10x10 arrays\nA &lt;- array(runif(1000), dim = c(10, 10, 10))\nB &lt;- array(runif(1000), dim = c(10, 10, 10))\nC &lt;- array(runif(1000), dim = c(10, 10, 10))\n\n# Vectorized solution (fast & efficient)\nresult &lt;- A + B + C",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#key-takeaways",
    "title": "Deep Nesting",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nDeep nesting makes code harder to read and maintain.\nTechniques like early returns, helper functions, and built-in functions can simplify complex logic.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRealPython - ‚ÄúLook Ma, No For-Loops‚Äù",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html",
    "title": "Hard coding",
    "section": "",
    "text": "Hard-coding variables occurs when constants, configuration values, or logic are directly embedded into the code, making changes difficult. Hard-coding leads to rigid systems that require modifying the source code itself to change behavior, rather than adjusting parameters, settings, or external configurations.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#symptoms",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#symptoms",
    "title": "Hard coding",
    "section": "Symptoms",
    "text": "Symptoms\n\nMagic numbers or string literals appear directly in the code.\nYou find yourself searching the codebase for specific values to tweak behavior for different executions.\nThe same constant value appears multiple times, making updates error-prone.\nThe logic is less readable, since magic numbers don‚Äôt indicate what they represent.\nA small behavior change requires altering the core code, instead of adjusting an input parameters of config file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#example---hard-coding-and-magic-numbers",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#example---hard-coding-and-magic-numbers",
    "title": "Hard coding",
    "section": "Example - Hard coding and magic numbers",
    "text": "Example - Hard coding and magic numbers\n\nPythonR\n\n\ndef calculate_area(radius):\n    # Hard-coded value of pi\n    return 3.14 * radius * radius # What if you need more precision?\n\ndef check_temperature(temperature):\n    # Hard-coded temperature values for thresholding\n    if temperature &gt; 30: # What does 30 represent?\n        print(\"It's too hot!\")\n    elif temperature &lt; 10:\n        print(\"It's too cold!\")\n\n\ncalculate_area &lt;- function(radius) {\n  # Hard-coded value of pi\n  return(3.14 * radius * radius)  # What if you need more precision?\n}\n\ncheck_temperature &lt;- function(temperature) {\n  # Hard-coded temperature values for thresholding\n  if (temperature &gt; 30) {  # What does 30 represent?\n    print(\"It's too hot!\")\n  } else if (temperature &lt; 10) {\n    print(\"It's too cold!\")\n  }\n}\n\n\n\n\nIssues\n\nThe value of pi is hard-coded as 3.14, which can lead to precision issues.\nThe temperature thresholds (30, 10) are buried in the logic, making them difficult to modify.\nThe meaning of 30 and 10 is unclear - are they for a specific region, season, or use case?\n\n\n\nSolution\nUsing named constants and configurable parameters makes the code more readable, maintainable, and flexible.\n\nPythonR\n\n\nimport numpy as np  # Use a library constant\n\nHOT_THRESHOLD = 30  # Define named constant for readability\nCOLD_THRESHOLD = 10  # Define named constant for readability\n\ndef calculate_area(radius):  # Default parameter allows customization\n    return np.pi * radius * radius # Use library constant for pi\n\ndef check_temperature(temperature, hot_threshold=HOT_THRESHOLD, cold_threshold=COLD_THRESHOLD):\n    if temperature &gt; hot_threshold: # Use named constants for readability\n        print(\"It's too hot!\")\n    elif temperature &lt; cold_threshold:\n        print(\"It's too cold!\")\n\n\n# Define constants at the top of the script\nHOT_THRESHOLD &lt;- 30  # Define named constant for readability\nCOLD_THRESHOLD &lt;- 10  # Define named constant for readability\n\ncalculate_area &lt;- function(radius) {\n  # Use pi constant from base R\n  return(pi * radius * radius)  # Use built-in pi constant for precision\n}\n\ncheck_temperature &lt;- function(temperature, hot_threshold = HOT_THRESHOLD, cold_threshold = COLD_THRESHOLD) {\n  if (temperature &gt; hot_threshold) {  # Use named constants for readability\n    print(\"It's too hot!\")\n  } else if (temperature &lt; cold_threshold) {\n    print(\"It's too cold!\")\n  }\n}",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#example---rigid-code",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#example---rigid-code",
    "title": "Hard coding",
    "section": "Example - Rigid code",
    "text": "Example - Rigid code\nThis simulation hard-codes the time step and duration, making it rigid.\n\nPythonR\n\n\ndef run_simulation():\n    step_size = 0.01  # Fixed timestep\n    total_time = 10  # Fixed total duration\n    for t in range(0, total_time, step_size):\n        update_system(t)\n\n\nrun_simulation &lt;- function() {\n  step_size &lt;- 0.01  # Fixed timestep\n  total_time &lt;- 10  # Fixed total duration\n  for (t in seq(0, total_time - step_size, by = step_size)) {\n    update_system(t)\n  }\n}\n\n\n\n\nIssues\n\nChange the step size of total duration required modifying the source code.\nThe code is not reusable across different simulations.\n\n\n\nSolution\nIntroduce function parameters or external configuration files for flexibility and reproducibility.\n\nPythonR\n\n\ndef run_simulation(step_size=0.01, total_time=10):\n    for t in range(0, total_time, step_size):\n        update_system(t)\n\n# Calling with different configurations\nrun_simulation(step_size=0.05, total_time=20)  # Adjust without modifying the underlying code\n\n\nrun_simulation &lt;- function(step_size = 0.01, total_time = 10) {\n  for (t in seq(0, total_time - step_size, by = step_size)) {\n    update_system(t)\n  }\n}\n\n# Calling with different configurations\nrun_simulation(step_size = 0.05, total_time = 20)  # Adjust without modifying the underlying code\n\n\n\nFor larger projects, moving configuration values to a separate file or class can further improve reproducibility and maintainability. Users would then only need to adjust the (text-based) configuration file without touching the core code.\n# config.yaml\nsimulation:\n  step_size: 0.01\n  total_time: 10\n\nPythonR\n\n\nimport yaml\n\ndef load_config(file_path=\"config.yaml\"):\n    with open(file_path, 'r') as file:\n        return yaml.safe_load(file)\n\nconfig = load_config()\nrun_simulation(config['simulation']['step_size'], config['simulation']['total_time'])\n\n\n# Install yaml package if needed: install.packages(\"yaml\")\nlibrary(yaml)\n\nload_config &lt;- function(file_path = \"config.yaml\") {\n  return(yaml::read_yaml(file_path))\n}\n\nconfig &lt;- load_config()\nrun_simulation(\n  step_size = config$simulation$step_size,\n  total_time = config$simulation$total_time\n)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#key-takeaways",
    "title": "Hard coding",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nUse named constants for improved readability.\nExternalize configuration values to allow easy adjustments without modifying the source code.\nConfiguration files or classes can further improve maintainability and reproducibility.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html",
    "href": "docs/software/code_quality/code_smells/large_class.html",
    "title": "Large Classes",
    "section": "",
    "text": "A monolithic design is where an entire system is built as a single, tightly coupled unit without clear separation of responsibilities or modularization. This often leads to large, complex classes that handle multiple responsibilities, making the codebase harder to understand, modify, and maintain.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#symptoms",
    "href": "docs/software/code_quality/code_smells/large_class.html#symptoms",
    "title": "Large Classes",
    "section": "Symptoms",
    "text": "Symptoms\n\nLarge classes that try to handle too many responsibilities.\nCode duplication across multiple parts of the system.\nDifficulties in testing because changes in one part of the code affect others.\nLimited reusability of components due to tight coupling.\nSmall modifications require extensive changes across the codebase.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#example---violating-the-single-responsibility-principle",
    "href": "docs/software/code_quality/code_smells/large_class.html#example---violating-the-single-responsibility-principle",
    "title": "Large Classes",
    "section": "Example - Violating the Single Responsibility Principle",
    "text": "Example - Violating the Single Responsibility Principle\nIn this example, we are writing code for a temperature monitoring system. A bad design would be putting everything inside one big class:\nclass SensorSystem:\n    def __init__(self):\n        self.temperature = 0\n\n    def read_temperature(self):\n        # Simulated temperature reading\n        self.temperature = 25  \n        print(f\"Temperature: {self.temperature}¬∞C\")\n\n    def log_temperature(self):\n        # Simulated logging\n        print(f\"Logging temperature: {self.temperature}¬∞C\")\n\n    def send_alert(self):\n        if self.temperature &gt; 30:\n            print(\"ALERT: High temperature detected!\")\n\ndef main():\n    sensor_system = SensorSystem()\n    sensor_system.read_temperature()\n    sensor_system.log_temperature()\n    sensor_system.send_alert()\n\nif __name__ == \"__main__\":\n    main()\n\nSolution\nWe should split this class into smaller, focused classes:\n\nTemperatureSensor ‚Äì Handles sensor readings.\nLogger ‚Äì Handles logging.\nAlertSystem ‚Äì Handles alerts.\n\n\nFollow the Single Responsibility Principle (SRP) - Ensure that each class has only one job. If a class is doing too much, split its responsibilities into separate classes.\nUse dependency injection: Reduce class coupling by calling dependencies as arguments (injecting dependencies) rather than hard-coding them. This promotes modularity and testability, as well as making it easier to swap out components.\n\nclass TemperatureSensor:\n    def read_temperature(self):\n        # Simulated sensor reading\n        return 25  \n\nclass Logger:\n    def log(self, message):\n        print(f\"LOG: {message}\")\n\nclass AlertSystem:\n    def send_alert(self, temperature):\n        temperature_threshold = 30\n        if temperature &gt; temperature_threshold:\n            print(\"ALERT: High temperature detected!\")\n\nclass SensorSystem:\n    def __init__(self, sensor, logger, alert_system):\n        self.sensor = sensor\n        self.logger = logger\n        self.alert_system = alert_system\n\n    def monitor_temperature(self):\n        temperature = self.sensor.read_temperature()\n        self.logger.log(f\"Temperature: {temperature}¬∞C\")\n        self.alert_system.send_alert(temperature)\n\n# Dependency Injection\ndef main():\n    sensor = TemperatureSensor()\n    logger = Logger()\n    alert_system = AlertSystem()\n    sensor_system = SensorSystem(sensor, logger, alert_system) # dependencies injected\n\n    sensor_system.monitor_temperature()\n\nif __name__ == \"__main__\":\n    main()\nWhy is this better?\n\nNo unnecessary mixing of concerns.\nEasily swap different logging or alerting mechanisms.\nEach component can be tested in isolation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/large_class.html#key-takeaways",
    "title": "Large Classes",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nIf your class is doing too many things, split it into smaller, focused classes.\nUse dependency injection to keep components flexible and testable.\nFollowing modular design makes your code easier to understand, modify, and reuse.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nArjanCodes - Dependency Injection Best Practices",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html",
    "href": "docs/software/code_quality/code_smells/many_arguments.html",
    "title": "Many arguments",
    "section": "",
    "text": "When a function or method takes many parameters (inputs), it can become difficult to understand, maintain, and test. If a function needs a lot of information to work, it might be doing too many things at once, and this can confuse programmers or lead to mistakes. Refactoring the code to reduce the number of parameters or organizing the data in a more logical way can make the code easier to read and work with.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many arguments"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#symptoms",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#symptoms",
    "title": "Many arguments",
    "section": "Symptoms",
    "text": "Symptoms\n\nFunctions or methods with many parameters, especially if some of them are not used within the function.\nFunctions with long and confusing parameter lists, which are hard to remember or use correctly.\nCode that‚Äôs hard to change or update because of too many parameters being passed around.\nFunctions often require the same set of parameters, which can be grouped together logically.\n\n\n\n\n\n\n\nWarningA good rule of thumb\n\n\n\n\n1-3 parameters: Generally fine.\n4-5 parameters: Might be acceptable if needed, but review if they can be grouped.\n6+ parameters: Strongly consider refactoring the function or method.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many arguments"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#example---long-parameter-list",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#example---long-parameter-list",
    "title": "Many arguments",
    "section": "Example - Long parameter list",
    "text": "Example - Long parameter list\nHere‚Äôs an example of a function that takes many parameters:\n\nPythonR\n\n\ndef process_machine_operation(machine_id, temperature, pressure, speed, duration):\n    # Perform machine operation\n    print(\"Machine ID:\", machine_id)\n    print(\"Temperature:\", temperature)\n    print(\"Pressure:\", pressure)\n    print(\"Speed:\", speed)\n    print(\"Operation Duration:\", duration)\n\n# Usage\nprocess_machine_operation(\n    machine_id=\"M001\",\n    temperature=100.5,\n    pressure=200.0,\n    speed=1500.0,\n    duration=5.0)\n\n\nprocess_machine_operation &lt;- function(machine_id, temperature, pressure, speed, duration) {\n  # Perform machine operation\n  print(paste(\"Machine ID:\", machine_id))\n  print(paste(\"Temperature:\", temperature))\n  print(paste(\"Pressure:\", pressure))\n  print(paste(\"Speed:\", speed))\n  print(paste(\"Operation Duration:\", duration))\n}\n\n# Usage\nprocess_machine_operation(\n  machine_id = \"M001\",\n  temperature = 100.5,\n  pressure = 200.0,\n  speed = 1500.0,\n  duration = 5.0\n)\n\n\n\nThis function takes a lot of information at once: the machine ID, temperature, pressure, speed, and duration. If the function grows even more complex, it will become very hard to keep track of what each parameter means, and it could make the code difficult to maintain.\n\nSolutions\nTo solve this problem, we can do one or both of the following:\n\nSimplify the Function: Break the function into smaller parts that do one thing each.\nUse Objects to Group Related Data: Instead of passing many individual pieces of information, we can group them together into one object or structure that holds related information.\n\n\n1. Using a Dataclass (or S3 Classes in R)\n\nPythonR\n\n\nfrom dataclasses import dataclass\n\n# Create a dataclass to group the machine operation parameters\n@dataclass\nclass MachineOperationData:\n    machine_id: str\n    temperature: float\n    pressure: float\n    speed: float\n    duration: float\n\n\n# Create an S3 class to group the machine operation parameters\nMachineOperationData &lt;- function(machine_id, temperature, pressure, speed, duration) {\n  structure(\n    list(\n      machine_id = machine_id,\n      temperature = temperature,\n      pressure = pressure,\n      speed = speed,\n      duration = duration\n    ),\n    class = \"MachineOperationData\"\n  )\n}\n\n\n\nNow, instead of passing five separate parameters to our function, we‚Äôll just pass one object that holds everything. Next, we change our process_machine_operation function to accept this new object, making it simpler and cleaner.\n\nPythonR\n\n\ndef process_machine_operation(operation_data):\n    # Perform machine operation\n    print(\"Machine ID:\", operation_data.machine_id)\n    print(\"Temperature:\", operation_data.temperature)\n    print(\"Pressure:\", operation_data.pressure)\n    print(\"Speed:\", operation_data.speed)\n    print(\"Operation Duration:\", operation_data.duration)\n\n# Usage\noperation_data = MachineOperationData(\n    machine_id=\"M001\",\n    temperature=100.5,\n    pressure=200.0,\n    speed=1500.0,\n    duration=5.0)\n\nprocess_machine_operation(operation_data)\n\n\n\n\n\n\nTip\n\n\n\nPython: You can combine dataclasses with data validation through Pydantic.\n\n\n\n\nprocess_machine_operation &lt;- function(operation_data) {\n  # Perform machine operation\n  print(paste(\"Machine ID:\", operation_data$machine_id))\n  print(paste(\"Temperature:\", operation_data$temperature))\n  print(paste(\"Pressure:\", operation_data$pressure))\n  print(paste(\"Speed:\", operation_data$speed))\n  print(paste(\"Operation Duration:\", operation_data$duration))\n}\n\n# Usage\noperation_data &lt;- MachineOperationData(\n  machine_id = \"M001\",\n  temperature = 100.5,\n  pressure = 200.0,\n  speed = 1500.0,\n  duration = 5.0\n)\n\nprocess_machine_operation(operation_data)\n\n\n\n\n\n\nTip\n\n\n\nR: S3 is R‚Äôs lightweight object-oriented system. For more advanced OOP features and validation, consider using the R6 package or the S7 package.\n\n\n\n\n\n\n\n2. Divide and conquer\nAlthough using a single dataclass is a good start, we don‚Äôt want our data structure to become too big and complicated. If the dataclass starts holding too much data, it can make the code harder to understand. Instead, we can break it into smaller, simpler data classes that work together. For example:\n\nPythonR\n\n\n@dataclass\nclass Machine:\n    machine_id: str\n    manufacturer: str\n\n@dataclass\nclass OperationParameters:\n    temperature: float\n    pressure: float\n    speed: float\n    duration: float\n\n@dataclass\nclass EnvironmentalConditions:\n    humidity: float\n    altitude: float\n\n@dataclass\nclass MachineOperationData:\n    machine: Machine\n    operation_parameters: OperationParameters\n    environmental_conditions: EnvironmentalConditions\n\ndef process_machine_operation(operation_data):\n    print(\"Machine ID:\", operation_data.machine.machine_id)\n\n    # Implement machine operation\n\n\n# Define constructor functions for each component\nMachine &lt;- function(machine_id, manufacturer) {\n  structure(\n    list(machine_id = machine_id, manufacturer = manufacturer),\n    class = \"Machine\"\n  )\n}\n\nOperationParameters &lt;- function(temperature, pressure, speed, duration) {\n  structure(\n    list(temperature = temperature, pressure = pressure,\n         speed = speed, duration = duration),\n    class = \"OperationParameters\"\n  )\n}\n\nEnvironmentalConditions &lt;- function(humidity, altitude) {\n  structure(\n    list(humidity = humidity, altitude = altitude),\n    class = \"EnvironmentalConditions\"\n  )\n}\n\nMachineOperationData &lt;- function(machine, operation_parameters, environmental_conditions) {\n  structure(\n    list(machine = machine,\n         operation_parameters = operation_parameters,\n         environmental_conditions = environmental_conditions),\n    class = \"MachineOperationData\"\n  )\n}\n\n# Usage with composed S3 classes\noperation_data &lt;- MachineOperationData(\n  machine = Machine(\"M001\", \"TechCorp\"),\n  operation_parameters = OperationParameters(100.5, 200.0, 1500.0, 5.0),\n  environmental_conditions = EnvironmentalConditions(65.0, 500.0)\n)\n\nprocess_machine_operation &lt;- function(operation_data) {\n  print(paste(\"Machine ID:\", operation_data$machine$machine_id))\n\n  # Implement machine operation\n}\n\n\n\nHere, instead of having one large MachineOperationData class, we‚Äôve divided it into smaller pieces. Each class now represents a specific part of the data, which can then be used individually or grouped together as needed. This approach keeps everything organized and easy to work with.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many arguments"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#key-takeaways",
    "title": "Many arguments",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nDon‚Äôt pass too many parameters. If a function requires many parameters, it‚Äôs a sign that the function might be doing too much. Group related data together to reduce the number of parameters or break the function into smaller parts.\nUse classes or dataclasses to help organize related data into neat packages that are easy to pass around in your code.\nKeep things simple: Don‚Äôt let your classes become too big. If necessary, break them down into smaller parts, but keep them organized and easy to understand.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRealPython - Data Classes\nR S3 Classes",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many arguments"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html",
    "href": "docs/software/code_quality/code_style.html",
    "title": "Code style and tools",
    "section": "",
    "text": "‚ÄúPrograms must be written for people to read, and only incidentally for machines to execute.‚Äù\nHarold Abelson",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#style-guide",
    "href": "docs/software/code_quality/code_style.html#style-guide",
    "title": "Code style and tools",
    "section": "Style guide",
    "text": "Style guide\nStyle guides are a set of rules and conventions that define how code should be written in a particular programming language. They cover aspects such as naming conventions, indentation, line length, and other formatting rules. Style guides help to ensure that code is consistent, readable, and maintainable, and they are often enforced by static analysis tools and formatters. Many programming languages have official style guides, and there are also community-driven style guides that provide additional recommendations and best practices.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#static-analysis-tools",
    "href": "docs/software/code_quality/code_style.html#static-analysis-tools",
    "title": "Code style and tools",
    "section": "Static analysis tools",
    "text": "Static analysis tools\nStatic analysis tools are used to analyze source code without executing it. They can identify potential issues in the code, such as syntax errors, bugs, and code smells. Static analysis tools can also enforce coding standards and best practices, and help to identify security vulnerabilities. They are often integrated into Continuous Integration workflows to ensure that code quality is maintained throughout the development process.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#formatters",
    "href": "docs/software/code_quality/code_style.html#formatters",
    "title": "Code style and tools",
    "section": "Formatters",
    "text": "Formatters\nFormatters are tools that automatically adjust the formatting of your code to make it consistent and readable according to predefined style guidelines. They do not identify errors in the logic of the code but instead can restructure the whitespace, line breaks, and indentation so that the code is more uniform across a project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#overview-of-programming-languages",
    "href": "docs/software/code_quality/code_style.html#overview-of-programming-languages",
    "title": "Code style and tools",
    "section": "Overview of programming languages",
    "text": "Overview of programming languages\n\n\n\nLanguage\nStyle Guide\nStatic Analysis Tools\nFormatters\n\n\n\n\nPython\nPEP 8\npylint, flake8, prospector\nblack, autopep8, yapf\n\n\nR\nTidyverse Style Guide\nlintr\nstyler\n\n\nMATLAB\nMATLAB Style Guidelines 2.0\ncheckcode\nCode Analyzer\n\n\nC/C++\nGoogle C++ Style Guide\ncppcheck, clang-tidy\nclang-format, astyle\n\n\nJulia\nJulia Style Guide, Blue Style Guide\nJET.jl, Aqua.jl\nJuliaFormatter.jl\n\n\nFortran\nFortran Best Practices\nfortran-linter\nfprettify\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nThe Turing Way - Code Quality\nThe Turing Way - Code Style\nRealPython - Python Code Quality\nRealPython - PEP8",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/online_services.html",
    "href": "docs/software/code_quality/online_services.html",
    "title": "Online services",
    "section": "",
    "text": "Sonar\nSonar is a cloud-based service that provides inspection of code quality to perform automatic reviews with static code analysis to detect bugs, code smells and security vulnerabilities in a project. It supports many programming languages and integrates with GitHub (and GitLab and Bitbucket) as part of the Continuous Integration workflows. Sonar is particularly useful for projects that require compliance with coding standards or need regular feedback on the quality of the code.\n Learn more: SonarQube Cloud documentation\n\n\n\n\n\n\nTipConsideration\n\n\n\nWhile Sonar offers valuable features for code quality analysis, be aware that for non open-source projects it is a paid service, and pricing model depends on how many lines of code you want to check.\n\n\n\n\nGitHub CodeQL\nGitHub CodeQL is a semantic code analysis engine that allows you to write queries to find security vulnerabilities in your codebase. It is particularly useful for identifying security vulnerabilities in open-source projects, and it can be integrated into your GitHub Actions workflow to automatically scan your code for vulnerabilities. CodeQL is available for a variety of programming languages, and it can help you identify and fix security issues before they become a problem.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGitHub CodeQL\nIntroduction to code scanning\n\n\n\n\n\nCode coverage\nCode coverage quantifies the proportion of source code that is run by a software program‚Äôs (unit) test suite. It helps to identify which parts of the codebase have been tested, and achieving a high code coverage generally indicates a lower likelihood of hidden bugs. However, it is important to note that high code coverage does not necessarily translate to high code quality - it simply tells us how much of the codebase is being tested.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nSonar - Test coverage\nCodecov - Test coverage\n\n\n\n\n\nDependabot\nDependabot is a GitHub app that helps you keep your dependencies up to date. It checks for outdated dependencies in your project and automatically creates pull requests to update them. This can help you stay on top of security vulnerabilities and ensure that your project is using the latest features and bug fixes.\n Enabling Dependabot for your repository\n\n\nOpenSSF\nThe Open Source Security Foundation (OpenSSF) Best Practices badge provides a way for Free/Libre and Open Source Software (FLOSS) projects to demonstrate their adherence to best practices. Projects can choose to self-certify for free. Inspired by the numerous badges available on GitHub, the OpenSSF Best Practices Badge allows to quickly identify which FLOSS projects are committed to best practices and are therefore more likely to deliver high-quality and secure software.\nThe criteria for earning the passing badge and additional details about the OpenSSF Best Practices Badging program can be found on GitHub.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nOpenSSF - Best Practices\nGitHub - Best Practices Badge",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Online services"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies.html",
    "title": "Environment and dependency management",
    "section": "",
    "text": "Properly managing dependencies and your environment is a critical aspect of any software project. This ensures that your project can be reliably reproduced, simplifies setup for collaborators, and reduces conflicts between third-party libraries.\n\n\n\n Python\nEnvironment and dependency management in Python.\n\nLearn more ¬ª\n\n\n\n MATLAB\nEnvironment and dependency management in MATLAB.\n\nLearn more ¬ª\n\n\n\n R\nEnvironment and dependency management in R.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/python_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/python_envs_dependencies.html",
    "title": "Environment and dependency management in Python",
    "section": "",
    "text": "When working with Python, managing dependencies and environments is important to ensure your project can be reproduced and shared.\n\n\n\n\n\n\nNote Definitions:\n\n\n\nA dependency is any external library your project needs, and a virtual environment is an isolated workspace where dependencies are installed.\n\n\nThere are several ways to manage dependencies and environments:\n\nConda environments\nVirtual environments (venv/virtualenv)\nDependency management tools (e.g.¬†poetry, pipenv)\n\n\nConda Environments\nConda is a package and environment manager popular in the research and data science community. It allows you to manage both Python and non-Python dependencies.\n\nBasic commands\n# Create a new environment, e.g. with python 3.12\nconda create -n your_env_name python=3.12\n\n# List all environments\nconda env list\n\n# Activate an environment\nconda activate your_env_name\n\n# Install packages in an environment\nconda install package_name\n\n# Remove a package\nconda remove package_name\n\n# Export an environment to a file\nconda env export &gt; environment.yml\n\n# Deactivate an environment\nconda deactivate\n\n# Remove an environment\nconda env remove -n your_env_name\n\n\nConda environment files\nConda environment files (environment.yml) are used to specify the dependencies of a project. They can be used to create an environment from scratch, or to update an existing environment.\n# Export an environment to a file\nconda env export &gt; environment.yml\n\n# Create an environment from a file\nconda env create -f environment.yml\n\n# Update an environment from a file\nconda env update -f environment.yml\n\n\n\nVirtual Environments (venv/virtualenv)\nPython provides venv as a built-in tool for creating virtual environments. virtualenv is a third-party tool that provides similar functionality.\n\nBasic commands\n# Creating a virtual environment\n# Using venv (Python 3.3+ built-in)\npython -m venv your-env-name\n\n# Using virtualenv (must be installed first)\npip install virtualenv\nvirtualenv your-env-name\n\n#Activating the environment\n# Linux/macOS\nsource your-env-name/bin/activate\n# Windows\nyour-env-name\\Scripts\\activate\n\n# Installing a library (package)\npip install lib_name\n\n# Uninstalling a library (package)\npip uninstall lib_name\n\n# To deactivate\ndeactivate\n\n\nManaging dependencies with pip\nA requirements.txt file lists all dependencies with their specific versions.\n# Export requirements.txt from an activated environment\npip freeze &gt; requirements.txt\n\n# Install dependencies from requirements.txt\npip install -r requirements.txt\n\n\n\n\n\n\nTip Tip\n\n\n\nUse pip-chill or pipreqs instead of pip freeze to exclude unnecessary dependencies. pip-chill lists only packages you installed, while pipreqs lists packages your code actually uses.\n\n\n\n\n\nDependency Management Tools\nConsider using tools that offer more sophisticated dependency management by integrating virtual environment creation and dependency resolution. They maintain a project manifest (e.g., pyproject.toml for Poetry) that specifies primary dependencies and generate lockfiles to pin exact versions for reproducibility.\n\nPipenv: Combines pip and virtualenv into a single tool, with a focus on simplicity and ease of use.\nPoetry: Manages dependencies, environments, and package building in a streamlined way.\nPixi: A new tool that aims to provide a more user-friendly experience for managing Python environments and dependencies.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCodeRefinery - Recording dependencies\nThe Turing Way - Package Management Systems\nConda documentation\nvirtualenv documentation\nvirtualenvwrapper extension\n\n\n\n\nUsing uv for environments and dependencies\nBesides the aforementioned tools, uv is becoming increasingly adopted because it manages project dependencies and environments (much like Poetry), provides Python version management, and a lockfile in one tool, and is considerably faster. It can replace pip, virtualenv, pyenv, and pip‚Äëtools, and others in day‚Äëto‚Äëday workflows.\nInstead of relying on a requirements.txt file (as with pip), uv maintains a uv.lock lockfile that records the fully resolved dependency graph.\n# Create/refresh uv.lock\nuv lock                   \n# Install only what is in the lock\nuv sync --locked\nYou can also work with an existing requirements.txt file with uv. Or export the dependencies to a requirements.txt file.\n# Does not remove extra packages already in the env\nuv pip install -r requirements.txt\n\n# Prunes extra packages in your env to match the file exactly\nuv pip sync requirements.txt\n\n# Export the dependencies\nuv export --format requirements-txt -o requirements.txt\nYou can migrate a project to pyproject.toml (import from requirements).\nuv add -r requirements.txt\nuv add --dev -r requirements-dev.txt\nA typical workflow to set up an environment to run Jupyter notebooks.\n# Scaffold a new project, create pyproject.toml\nuv init\n\n# Pin your python version                            \nuv python pin 3.12\n\n# Add notebook dependencies to the project\nuv add --dev ipykernel jupyterlab\n\n# Resolve dependencies and install them exactly\nuv lock && uv sync --locked\n\n# Launch a Jupyter notebook from the project env\nuv run --with jupyter jupyter lab\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nuv documentation",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "Python"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/index.html",
    "href": "docs/software/development_workflow/index.html",
    "title": "Development workflow",
    "section": "",
    "text": "Version control and collaboration\nVersion control ensures transparency, reproducibility, and smooth teamwork. Clear branching strategies and consistent workflows make it easier to track changes, review code, and integrate contributions.\n\n\n\n\n Project Management\nManaging your projects through version control platforms.\n\nLearn more ¬ª\n\n\n\n Branch Management\nChoosing a branching strategy.\n\nLearn more ¬ª\n\n\n\n Collaboration\nCollaborative workflow.\n\nLearn more ¬ª\n\n\n\n\n\nProject organization\nA structured repository with defined folders, managed dependencies, and reusable components keeps your project consistent, scalable, and easier for others to understand and extend.\n\n\n\n\n Project Structure\nStructuring your project.\n\nLearn more ¬ª\n\n\n\n Project Templates and Reusability\nReusing projects and repositories.\n\nLearn more ¬ª\n\n\n\n Environments and Dependencies\nManaging your environments and dependencies.\n\nLearn more ¬ª\n\n\n\n Workflow Management\nTools for writing and managing workflows.\n\nLearn more ¬ª\n\n\n\n Software Design\nPrinciples for designing your software.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_structure.html",
    "href": "docs/software/development_workflow/project_structure.html",
    "title": "Project structure",
    "section": "",
    "text": "In software development, the choices you make at the start will affect your project‚Äôs final outcome. One key decision is how to structure your project, as a well-organised setup is essential for reproducibility and long-term maintainability.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project structure"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_structure.html#repository-structures",
    "href": "docs/software/development_workflow/project_structure.html#repository-structures",
    "title": "Project structure",
    "section": "Repository structures",
    "text": "Repository structures\nThe following are recommendations of how you can structure your project repository for Python, MATLAB, and R projects.\n\nPythonMATLABR\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                     # Documentation directory\n‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks\n‚îú‚îÄ‚îÄ src/                      # Contains your main code\n‚îÇ   ‚îî‚îÄ‚îÄ your_project/            # A folder where your organized code lives\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # A marker file that indicates this folder is for Python code\n‚îÇ       ‚îú‚îÄ‚îÄ module            # A file or folder with specific functions or classes\n‚îÇ       ‚îî‚îÄ‚îÄ extras/           # A folder for additional, related code\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py   # A marker file for the additional code folder\n‚îú‚îÄ‚îÄ tests/                    # Your test directory  \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # Data files used in the project (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # Files from your analysis (if applicable)\n‚îú‚îÄ‚îÄ results/                  # Results (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # Untracked files \n‚îú‚îÄ‚îÄ requirements.txt          # Software dependencies (environment.yml if using Conda)\n‚îÇ                             # ‚Üë Even better to use a build system config (pyproject.toml)\n‚îÇ                             # ‚Üë which is becoming the new standard\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # License information\n Choosing between a src/ layout and a flat layout for Python\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                   # Documentation and user guides\n‚îú‚îÄ‚îÄ src/                    # Main MATLAB code\n‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Helper functions and scripts\n‚îÇ   ‚îú‚îÄ‚îÄ models/             # Core functions or classes implementing models/algorithms\n‚îÇ   ‚îî‚îÄ‚îÄ main_script.m       # Main script/-s or entry point for the project\n‚îÇ\n‚îú‚îÄ‚îÄ scripts/                # Scripts folder (e.g. for analysis and demo scripts)\n‚îú‚îÄ‚îÄ tests/                  # Tests folder (e.g. MATLAB unit tests)\n‚îú‚îÄ‚îÄ data/                   # Raw data files\n‚îú‚îÄ‚îÄ results/                # Output files (figures, processed data, etc.)\n‚îú‚îÄ‚îÄ examples/               # Example usage or tutorials\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore              # Specifies files/folders to ignore in version control\n‚îú‚îÄ‚îÄ README.md               # Project overview and instructions\n‚îî‚îÄ‚îÄ LICENSE                 # License information\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ R/                        # R scripts and functions (can also be called src/)\n‚îÇ   ‚îú‚îÄ‚îÄ function.R            # R functions used across analyses\n‚îÇ   ‚îî‚îÄ‚îÄ other_function.R      \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # raw data files (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # processed data files (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ doc/                      # project documentation\n‚îú‚îÄ‚îÄ man/                      # helper files for package functions generated from roxygen2 (if applicable)\n‚îÇ      \n‚îú‚îÄ‚îÄ vignettes/                # explanatory vignettes for the project (if applicable)\n‚îÇ   ‚îî‚îÄ‚îÄ function_vignette.Rmd # vignettes for each function\n‚îÇ\n‚îú‚îÄ‚îÄ tests/                    # test cases for your functions (highly recommended)\n‚îÇ   ‚îî‚îÄ‚îÄ testthat/             # using the testthat package\n‚îÇ\n‚îú‚îÄ‚îÄ results/                  # output from data analyses etc. (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ scripts/                  # high-level scripts for running analyses\n‚îÇ   ‚îî‚îÄ‚îÄ analysis_script.R     # script running the main analysis\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # gitignore\n‚îú‚îÄ‚îÄ DESCRIPTION               # package description file (if applicable)\n‚îú‚îÄ‚îÄ NAMESPACE                 # namespace file for package (if applicable)\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # license information\n\n\n\nThese structures are a starting point and can be adapted based on the specific needs and practices of your project. Some additional tips:\n\nParticular metadata files are often capitalized, such as README, LICENSE, CONTRIBUTING, CODE_OF_CONDUCT, CHANGELOG, CITATION.cff, NOTICE, and MANIFEST.\nGenerally, all content that is generated upon building or running your code should be added to .gitignore. This likely includes the content of processed_data and results folder.\nGit cannot track empty folders. If you want to add empty folders to enforce a folder structure, e.g., processed_data orresults, the convention is to add the file .gitkeep to the folder.\n\n\n\n\n\n\n\nWarning Managing data\n\n\n\nIf your raw data files or any data assets are large (typically more than a few megabytes), it‚Äôs usually best not to include them directly in the repository. Instead:\n\nKeep such files externally (e.g.¬†cloud storage, Git LFS), and add only a reference or a small sample to the repository.\nAdding placeholder files or instructions in the README for how to obtain the complete datasets.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCode Refinery - Organizing your projects\nArjanCodes guide to structuring Python projects\nA collection of .gitignore templates\nGit LFS",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project structure"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html",
    "href": "docs/software/development_workflow/workflow_management.html",
    "title": "Workflow management",
    "section": "",
    "text": "Workflow management for research software is the practice of organizing series of tasks related to research, such as how scientific code, data and experiments are developed, executed and shared to ensure reproducible, collaborative and efficient research. An example research workflow might include data processing, model training or running simulations, as well as recording and publishing the results. The main objective is not only about making the code run successfully, but about making it reproducible, trackable and adaptable.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#workflow-management-tools",
    "href": "docs/software/development_workflow/workflow_management.html#workflow-management-tools",
    "title": "Workflow management",
    "section": "Workflow management tools",
    "text": "Workflow management tools\nA workflow management tool (or simply a workflow manager) is a tool to help researchers build and manage computational workflows in a structured and reproducible way.\n\nThese tools assist in defining, organizing, executing and tracking sequences of computational steps which are typically represented as a series of commands or scripts.\nBy explicitly recording how data is processed, including the parameters and software environments used, workflow managers make it easier to replicate analyses, debug issues, and share work with collaborators.\nThis level of structure helps avoid the all-too-common situation where a result is produced but the exact steps to generate it are lost or unclear.\n\nThe bioinformatics field is a prominent example where workflow management has become essential since research relies heavily on pipelines to analyze genomic data; usually you perform a sequence of analyses on several files. Hence, there are many workflow managers niche to the field.\n\nBut workflow management is a general need in many other applications involving data to support reproducible analysis.\nAny research field involving data analysis, simulations, or modeling can benefit from adopting workflow managers.\nFields like climate science, neuroscience, physics, and even digital humanities face similar challenges of organizing code, managing data dependencies, and ensuring that computational results are reproducible.\nAs scientific computing becomes more complex and collaborative, workflow management is emerging as a foundational practice to support open science, reproducibility, and long-term maintainability of research software.\n\n\nSome examples of workflow managers\nWorkflow managers come in many forms, from simple scripting approaches to full-featured platforms with graphical user interfaces (GUIs) or domain-specific languages (DSLs). Here are some examples, categorized by interface and complexity:\n\nScript-based or Classic Tools \n\nShell scripts / Python scripts (‚Äúgood old scripts‚Äù)\nMake ‚Äì a traditional build automation tool still used in research for its simplicity\n\nCode-Based Workflow Managers  These systems are primarily driven by writing code in general-purpose languages (like Python), often using functions, decorators, or object-oriented patterns:\n\nSnakemake (Python-based)\nNextflow (DSL based on Groovy)\nAirflow (Python-based)\nRuffus, SciPipe, and many others\n\nSnakemake, Nextflow, and Airflow are examples of DSL-based workflow managers that allow users to define workflows in a concise and human-readable syntax.\nGUI-Based Workflow Managers  These tools provide a visual interface for building and executing workflows, making them ideal for users with little or no programming background:\n\nGalaxy\nKNIME\n\n\n\n\n\n\n\n\nTipChoose the right workflow manager for you\n\n\n\n\nDSLs can make workflows easier to read, document, and debug - especially for complex pipelines.\nGUI-based workflow systems, with drag-and-drop interfaces and visual feedback, are often the most accessible option for beginners or interdisciplinary teams.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#snakemake-a-python-esque-make",
    "href": "docs/software/development_workflow/workflow_management.html#snakemake-a-python-esque-make",
    "title": "Workflow management",
    "section": "Snakemake: a Python-esque make",
    "text": "Snakemake: a Python-esque make\nSnakemake essentially builds on the implicit wildcard rule approach of Make, and it extends its capabilities by allowing the use of Python in a pipeline. Just like Make, its goal is to produce a set of requested output files based on predefined rules and steps.\nAlthough it was originally developed to create scalable bioinformatics and genomics pipelines, it can be generalized to other applications as well. It has become a standard tool in reproducible research; being cited more than 12 times per week in 2023, and has been used extensively in scientific publications in several different fields. Currently, it has over one million downloads on Conda.\n\nNoteworthy features of Snakemake\n\nIf Python and Make were to have a baby.\nYou can describe workflows using a human readable, Python based language.\nIt has built-in caching: if some steps of your workflow have already been run, Snakemake can recognize that and avoid rerunning the same analyses.\nIt can accommodate both serial and parallel jobs since each ‚Äúwork units‚Äù in a workflow can be run independently of one another.\nIt makes debugging easier since it keeps track of all files generated, you can identify which steps in your workflow have failed.\nIntegration with conda allows you to define conda environments for both the whole workflow and individual steps.\nYou can incorporate tools or methods written in different scripting languages.\nWorkflows can be scaled to server, cluster, grid and cloud environments, without modifying the workflow itself.\nIt has an active user and developer base in mainly bioinformatics, and other scientific research community. It‚Äôs development is driven by the needs of scientists and their needs of reproducible research.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#what-about-airflow",
    "href": "docs/software/development_workflow/workflow_management.html#what-about-airflow",
    "title": "Workflow management",
    "section": "What about Airflow?",
    "text": "What about Airflow?\nAirflow has become the go-to tool for building data pipelines, it started at Airbnb and was later adopted by the Apache project. Being backed by the Apache project, and a growing community of contributors, it‚Äôs the most popular workflow manager in software engineering.\n\n Pros:\n\nIt is the most popular choice of workflow management system in software engineering.\nIt has a bigger community support and hence more detailed tutorials and documentation.\nIt offers more features, especially enterprise integrations used in industry.\nIt integrates well with databases, APIs, cloud services and data warehouse like Google BigQuery, AWS S3 and Snowflake.\nIt is more production friendly with built-in scheduling and monitoring features.\n\n Cons:\n\nIt has a complex infrastructure and a steeper learning curve.\nIt is more difficult to share pipelines between different environments.\nIt lacks native support for conda.\nIt has a more convoluted approach to parallel computation.\nIt has limited support for HPC.\n\n\n\nA summary of workflow management tools discussed\n\n\n\n\n\n\n\n\n\nFeature\nSnakemake\nNextflow\nAirflow\n\n\n\n\nTarget Audience\nScientists, researchers\nBioinformaticians, scientists\nData engineers, DevOps\n\n\nEase of Use\nHigh\nMedium\nLow\n\n\nReproducibility Focus\nStrong\nMedium\nLow\n\n\nHPC Support\nExcellent\nGood\nMinimal\n\n\nCloud-Native Support\nModerate\nStrong\nExcellent\n\n\nCommunity\nScientific community\nBioinformatics\nData engineering, cloud-native\n\n\n\n\n\n\n\n\n\nImportantSnakemake vs Airflow\n\n\n\nSnakemake is a gentle introduction to workflow management for most researchers. Airflow produces well defined production level workflows that are meant to be run continually, and hence Snakemake is much better suited for the needs of researchers who want to run reproducible analyses for a particular project or an application.\n\n\n\n\n\n\n\n\nNoteSidenote: What is special about bioinformatics workflows?\n\n\n\n\n\nIn bioinformatics, a workflow is a collection of steps run in series to transform raw data input to processed results (figures, insights, decisions etc.). Each step can be made up of different tools, programs, parameters, databases and dependencies/requirements.\nThere are 3 main reasons why bioinformatics workflows are different than those data engineering workflows in the industry:\n1. Differences in data type, shape and scale\n\nBioinformatics datasets are typically very large and come from various sources (DNA sequences, RNA sequencing, proteomics data, imaging data)\nDifferent data types have different structures and different preprocessing/analysis needs\n\n2. Differences in programs and tooling\n\nBioinformatics pipelines often involve numerous steps with intricate dependencies\nDifferent steps use different, specialized tools\nOpen source, highly specialized tools that are not meant to be integrated natively, there are no software packages or standalone platforms to run analyses from start to finish\nNew algorithms, tools and reference databases are updated frequently, researchers need flexible pipelines that can be adapted easily\nMany bioinformatics tasks and tools are computationally expensive (genome assembly, alignment, sequence search) and require HPC\n\n3. Community support behind bioinformatics workflow managers and open source software\n\nField of bioinformatics has a strong emphasis on open science, reproducible and transparent research. All of which are achieved using workflow managers\n\n\n\n\n\n\nOther fields that already use or can benefit from Snakemake\n\nLarge, parallel deep learning experiments using Snakemake: The USGS demonstrates how Snakemake can be used to perform deep learning experiments on environmental data.\nWorkflow managers in high-energy physics - Enhancing analyses with Snakemake: Physics and high-energy particle research: Large scale HEP experiments with complex dependencies on cloud.\nNeuroscience: Using Snakemake to process and analyze MRI data.\nEcology and evolutionary research: Article outlining the specific benefits of using Snakemake to manage data analysis workflows in ecology and evolutionary research, and demonstrating its use through case studies.\nEarth and Climate Science: Preprocessing large datasets from climate models (NetCDF format) for regional climate projections.\nAutomating preprocessing EEG/MEG data, MRI and fMRI data analysis and predictive modeling for thousands of neuroimaging files.\nChemistry: Running large-scale molecular dynamics simulations with different parameter combinations and collecting results in a reproducible framework.\n\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nAwesome pipeline repository: a curated list of tools for creating pipelines.\nSnakemake documentation: an extensive documentation that includes in depth tutorials and edge cases.\nAn Introduction to Snakemake with R for Economics\nSustainable data analysis with Snakemake: A general overview article about Snakemake published in F1000Research.\nA review of bioinformatic pipeline frameworks.: A review article published in Briefings in Bioinformatics, focused on Bioinformatics pipelines.\nAirflow vs Snakemake: A direct comparison between Airflow and Snakemake for data pipelines, while promoting FlowDeploy.\nWhy are bioinformatics workflows different?",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/code_documentation.html",
    "href": "docs/software/documentation/code_documentation/code_documentation.html",
    "title": "Code documentation",
    "section": "",
    "text": "Good code documentation acts as the bridge between developers and users by clearly explaining the functionality and rationale behind your code. Whether you‚Äôre writing inline comments or structured documentation, the key is to make your code readable, maintainable, and accessible to both current and future contributors.\n\n\n\n Python projects\nCode comments, docstrings, API reference.\n\nLearn more ¬ª\n\n\n\n MATLAB projects\nDocumenting MATLAB projects.\n\nLearn more ¬ª\n\n\n\n R projects\nDocumenting R projects with roxygen2.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html",
    "href": "docs/software/documentation/code_documentation/python_documentation.html",
    "title": "Python code documentation",
    "section": "",
    "text": "Code readability is detailed in a coding style guide.\nCode comments are useful for clarifying complex parts of code, noting why certain decisions were made in specific blocks or lines.\nDocstrings provide a description of the function, class, or module that follows immediately after it is defined, and should contain all the relevant information needed for using them, rather than explaining how the code works. Ideally, every module should have a docstring, and so should every function and class that a module makes available.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html#code-comments",
    "href": "docs/software/documentation/code_documentation/python_documentation.html#code-comments",
    "title": "Python code documentation",
    "section": "Code comments",
    "text": "Code comments\nCode comments are inline annotations meant for developers who read or maintain the source code. They should:\n\nexplain parts that are not intuitive from the code itself\nexplain the purpose of a piece of code (why over how)\nneed to be kept up-to-date as wrong comments are not caught through testing\ndo not replace readable and structured code\ndo not turn old code into commented zombie code (see code smells)\ndo not repeat in natural language what is written in your code, e.g.\n\n# Now we check if the age of a patient is greater than 18\nif age_patient &gt; 18:",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html#docstrings",
    "href": "docs/software/documentation/code_documentation/python_documentation.html#docstrings",
    "title": "Python code documentation",
    "section": "Docstrings",
    "text": "Docstrings\nDocstrings are structured comments, associated with segments (rather than lines) of code which can be used to generate documentation for users (and yourself!) of your project. They allow you to provide documentation to a segment (function, class, method) that is relevant for the user. Docstrings are placed in triple quotes \"\"\" and enable automated generation of API documentation.\nTwo docstring styles are commonly used for their readability:\n\nNumPy styleGoogle style\n\n\ndef func(arg1, arg2):\n    \"\"\"Summary line.\n\n    Extended description of function.\n\n    Parameters\n    ----------\n    arg1 : int\n        Description of arg1\n    arg2 : str\n        Description of arg2\n\n    Returns\n    -------\n    bool\n        Description of return value\n\n    \"\"\"\n    return True\n‚Æï Check out the NumPy style guide or a full example.\n\n\ndef func(arg1, arg2):\n    \"\"\"Summary line.\n\n    Extended description of function.\n\n    Args:\n        arg1 (int): Description of arg1\n        arg2 (str): Description of arg2\n\n    Returns:\n        bool: Description of return value\n\n    \"\"\"\n    return True\n‚Æï Check out the Google style guide or a full example.\n\n\n\n\nDocstring formatting\nPython‚Äôs PEP 257 provides guidelines on how to effectively write docstrings to ensure they are clear, concise, and useful. Some pointers:\n\nThe summary sentence of the docstring should appear on the same line as the opening triple quotes.\nThe closing triple quotes should be placed on a separate line, except for one-line docstrings.\nDocstrings for methods and functions should not have blank lines before or after them.\n\n\n\n\n\n\n\nTip Example\n\n\n\n\n\ndef find_max(numbers):\n    \"\"\"Find the maximum value in a list of numbers.\n\n    Parameters\n    ----------\n    numbers : iterable\n        A collection of numerical values from which the maximum will be determined.\n\n    Returns\n    -------\n    max_value : `float`\n        The highest number in the given list of numbers.\n    \"\"\"\n    pass\n\n\n\n\nDocstrings for classes should immediately follow the class definition without any preceding blank lines. However, a single blank line should follow the docstring, separating it from subsequent code such as class variables or the init method.\n\n\n\n\n\n\n\nTip Example\n\n\n\n\n\nclass Circle(object):\n    \"\"\"A circle defined by its radius.\n\n    Parameters\n    ----------\n    radius : `float`\n        The radius of the circle.\n    \"\"\"\n\n    def __init__(self, radius):\n        self.radius = radius\n\n\n\n\nThe content of a docstring must align with the indentation level of the code it documents.\n\n\n\n\n\n\n\nTip Correct and incorrect examples\n\n\n\n\n\n\n\n\n\ndef get_length(items):\n    \"\"\"Calculate the number of items in the list.\n\n    Parameters\n    ----------\n    items : list\n        A list whose length is to be determined.\n\n    Returns\n    -------\n    length : int\n        The number of items in the list.\n    \"\"\"\n    return len(items)\n\n\ndef get_length(items):\n    \"\"\"Calculate the number of items in the list.\n\nParameters\n----------\nitems : list\n    A list whose length is to be determined.\n\nReturns\n-------\nlength : int\n    The number of items in the list.\n\"\"\"\n    return len(items)\n\n\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nBuild API reference from docstrings\nNumpydoc style guide - best practices for docstrings\n\n\n\n\n\nDocstring contents\nFormatting conventions are important for clarity and readability across different APIs or libraries. Here we adhere to the numpydoc convention.\n\nSummaries\nDocstrings should start with a one-sentence summary and if additional clarification is needed, you could add an extended summary. For functions and methods, use imperative voice, framing its summary as a command or instruction that the user can execute through the API. For classes, the summary should clearly describe what the class represents or its primary responsibility.\n\n\nParameters and arguments\nThe Parameters section lists the input parameters of a class, function, or method. It should include the parameter name, type, and a brief description of what the parameter represents. Parameters are listed in the same order as they appear in the function definition.\n\n\n\n\n\n\nTip Full description and example\n\n\n\n\n\nDescribing parameters\nBasic example:\ndef calcDistance(x, y, x0=0., y0=0., **kwargs):\n    \"\"\"Calculate the distance between two points.\n\n    Parameters\n    ----------\n    x : `float`\n        X-axis coordinate.\n    y : `float`\n        Y-axis coordinate.\n    x0 : `float`, optional\n        X-axis coordinate for the second point (the origin,\n        by default).\n\n        Descriptions can have multiple paragraphs, and lists:\n\n        - First list item.\n        - Second list item.\n    y0 : `float`, optional\n        Y-axis coordinate for the second point (the origin,\n        by default).\n    **kwargs\n        Additional keyword arguments passed to\n        `calcExternalApi`.\n    \"\"\"\n\n\n\n\n\nReturns and Yields\nReturns is an explanation about the returned values and their types, following the same format as Parameters. This is applicable to functions and methods. Use Yields for generators.\n\n\n\n\n\n\nTip Returns and Yields examples\n\n\n\n\n\n\nDocumenting Returns\nDocumenting Yields\n\n\nBasic example for Returns:Basic example for Yields:\n\n\ndef getCoord(self):\n    \"\"\"Get the point's pixel coordinate.\n\n    Returns\n    -------\n    x : `int`\n        X-axis pixel coordinate.\n    y : `int`\n        Y-axis pixel coordinate.\n    \"\"\"\n    return self._x, self._y\n\n\ndef items(self):\n    \"\"\"Iterate over items in the container.\n\n    Yields\n    ------\n    key : `str`\n        An item's key.\n    value : obj\n        An item's value.\n    \"\"\"\n    for key, value in self._data.items():\n        yield key, value\n\n\n\n\n\n\n\n\nRaises\nFor classes, methods, and functions the Raises section is used to describe exceptions that are explicitly raised.\n\n\n\n\n\n\nTip Example\n\n\n\n\n\n\nDocumenting Raises\n\nRaises\n------\nIOError\n    Raised if the input file cannot be read.\nTypeError\n    Raised if parameter ``example`` is an invalid type.\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nDocumenting modules\nDocumenting classes\nDocumenting methods and functions\nDocumenting constants and class attributes\nDocumenting class properties\nComplete example module\nnumpydoc example",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_of_conduct.html",
    "href": "docs/software/documentation/code_of_conduct.html",
    "title": "Code of conduct",
    "section": "",
    "text": "A code of conduct establishes expectations for behavior within your project‚Äôs community, creating an inclusive space that minimizes conflicts and promotes constructive collaboration.\nWhat to include:\n\nPurpose: State the reason for having a code of conduct and its importance for a positive community.\nExpected behavior: Clearly outline the standards of behavior expected from all participants.\nUnacceptable behavior: Specify actions and language that are not tolerated.\nEnforcement: Explain the consequences of violating the code of conduct and how incidents will be handled.\nContact information: Provide contact details for reporting concerns or issues.\n\nYou can add a CODE_OF_CONDUCT.md file at your repository‚Äôs root level. GitHub will automatically detect and display it in your community profile.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nContributor Covenant - A widely adopted template for open source communities.\nGitHub‚Äôs guide to adding a code of conduct",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code of conduct"
    ]
  },
  {
    "objectID": "docs/software/documentation/hosting.html",
    "href": "docs/software/documentation/hosting.html",
    "title": "Hosting",
    "section": "",
    "text": "Once you have created your documentation either in Sphinx, Jupyter Book, MkDocs or Quarto, you can host it online.\n\nGitHub Pages\nGitHub Pages provides a simple way to host your documentation, especially if your project is already on GitHub.\nIt is straightforward to set up GitHub Pages:\n\nWithin your repository, go to the repository settings and find the GitHub Pages section.\nChoose your publishing source (you should have a docs folder or a dedicated branch).\n\nGitHub Pages also supports custom domains, which might be relevant to you. You can configure this by adding a CNAME file to your directory.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGitHub Pages\nConfiguring a custom domain for your GitHub Pages site\nCoderefinery - Deploying Sphinx documentation to GitHub Pages\nCoderefinery - Hosting websites/homepages on GitHub Pages\n\n\n\n\n\nRead the Docs\nRead the Docs is a platform that simplifies the hosting of documentation. It integrates particularly well with Sphinx, allowing for the automatic building and hosting of your project documentation. Read the Docs supports automatic builds and version control, enabling users to switch between different versions of the documentation to match the version of the software they are using. Additionally, it offers support for custom domains.\nIt offers a free service for open-source projects, which includes features like version control and automatic builds. However, for private or commercial projects, Read the Docs requires a paid subscription.\nThe setup is also straightforward:\n\nSign up and import your documentation repository.\nConnect to your GitHub account.\nConfigure your project settings within their dashboard.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRead the Docs: documentation simplified\nRead the Docs tutorial",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Hosting"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html",
    "href": "docs/software/documentation/license.html",
    "title": "Software licenses",
    "section": "",
    "text": "WarningImportant\n\n\n\nFor questions about data and software licenses, please consult your Faculty Data Steward.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html#tu-delft-licensing-policy",
    "href": "docs/software/documentation/license.html#tu-delft-licensing-policy",
    "title": "Software licenses",
    "section": "TU Delft licensing policy",
    "text": "TU Delft licensing policy\nTU Delft, by default, holds the rights to the software created by its employees. In order to apply a pre-approved open source license, you have to follow the guidelines in TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. It states:\n\n\n\n\n\n\nIt is important to remember that in principle TU Delft holds the rights to the software created by its employees (i.e.¬†software developers, researchers and/or staff). So, some formal (legal) steps are needed to arrange matters properly.\nWhen these guidelines are followed and when the software is published, TU Delft disclaims its copyright, allowing software developers, researchers and staff to hold the copyright to their software and thereby having the right to apply one of the pre-approved licences when sharing software. The pre-approved open source software licences at TU Delft are Apache, MIT, BSD, EUPL, AGPL, LGPL, GPL, and CC0.\n\n\n\n\nSteps to apply a pre-approved open-source license\n\nDetermine if it is possible to apply an Open Source Software licence to your project (see diagram below).\nThe waiver of TU Delft should be added to the repository, either as a separate file or within the LICENSE file. The waiver text should be as follows:\nTechnische Universiteit Delft hereby disclaims all copyright interest in the program ‚ÄúName program‚Äù (one line description of the content or function) written by the Author(s). \n\n[Name Dean], Dean of [Name Faculty]\nAssert your own, personal copyright (¬© YEAR, [NAME], [REFERENCE project, grant or study if desired]. Waiving the copyright and having TU Delft staff assert the copyright in their own name facilitates the use of copyleft licences.\nApply one of the TU Delft pre-approved Open Source Software licences in the format and form described in the licence text after stating, ‚ÄúThis work is licensed under a [NAME and VERSION] OSS licence‚Äù.\nMake the software openly available (for instance in an online repository such as GitHub).\nPlease consider acknowledging support from TU Delft and/or your funding provider.\nRegister the software either in 4TU.ResearchData or in PURE. Registries in 4TU.ResearchData are automatically registered in PURE.\n\n\n\n\n\n\n\nThe steps above are mandatory, the TU Delft guidelines state:\nPlease note that if the software is not published, and/or if the guidelines have not been followed correctly and/or if the software is not registered in PURE, then this ‚Äòagreement‚Äô is invalid and the software automatically falls under the legal copyright of TU Delft. This instantly nullifies the right of the software developer or researcher to apply for a licence and thus the open source software licence applied never came into existence. This works retroactively.\n\n\n\n\n\nDecision tree for applying for a license\nTU Delft staff members can apply for an open-source license according to the decision tree found in TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation.\n\n\n\n\n\n\n\nDecision tree to guide software developers, researchers and staff on when they can apply an open source licence to their software (OSS: Open Source Software, IDF: Invention Disclosure Form). From: Bazuine, M. (2021). TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. Zenodo. https://doi.org/10.5281/zenodo.4629635",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html#types-of-open-source-licenses",
    "href": "docs/software/documentation/license.html#types-of-open-source-licenses",
    "title": "Software licenses",
    "section": "Types of open source licenses",
    "text": "Types of open source licenses\n\nPermissive licenses aka copyright (e.g., MIT, BSD, Apache): These licenses allow users to do almost anything with the code, including using it in proprietary software.\nRestrictive licenses aka copyleft (e.g., GPL, AGPL, LGPL, EUPL): These licenses require any derivative works to be open source and distributed under the same license.\n\n\nCopyright licenses\n\nMIT: Very simple and permissive, allowing almost unrestricted reuse. The software can be freely used, modified, distributed, and sublicensed.\nBSD: Similar to the MIT license, but it may include additional attribution requirements.\n\n\n\n\n\n\n\nNote BSD license details\n\n\n\n\n\n\nAttribution: Requires that the copyright notice and list of conditions be included in all copies or substantial portions of the software (except for the BSD 0-Clause License, which does not require any attribution). There are different clause variants of the license. For example, a BSD 3-Clause license adds a clause preventing the use of the names of the project or its contributors to endorse or promote derived products without written permission.\nPatent Protection: Does not include explicit provisions for patent protection.\n\n\n\n\n\nApache: Allows for the use, modification, distribution, and sublicensing of the software under certain conditions. It is often used in open-source projects and is often the choice for its balance between permissiveness and the protection of patents. The Innovation and Impact Center recommends this license for industry collaborations.\n\n\n\n\n\n\n\nNote Apache license details\n\n\n\n\n\n\nAttribution: Requires preservation of the original copyright notice and a notice of any modifications made.\nPatent Protection: Includes a patent retaliation clause, which provides an additional layer of protection. This clause terminates the license if the user initiates patent litigation against any entity regarding the licensed software.\nNotice Requirement: Modifications to the original code must be documented, and a NOTICE file must be included with any substantial portions of the software.\n\n\n\n\n\n\n\n\n\n\nTip Key differences between a BSD and Apache license\n\n\n\n\nThe Apache license includes a patent retaliation clause to protect against patent litigation, but the BSD license does not explicitly address patent rights.\nThe BSD license does not require a specific notice file for modifications, but the Apache License requires a NOTICE file that documents any modifications made to the original code.\n\n\n\n\n\nCopyleft licenses\n\nGPL (GNU General Public License): One of the most widespread copyleft licenses. With the GPL license, any derivative work under this license automatically becomes subject to the same GPL terms, regardless of the size of the contribution. All future modifications and adaptions of code under this license is only compatible with this license and cannot be used in proprietary software.\nDerivatives from GPL (AGPL, LGPL, EUPL): From these, the EUPL license is somewhat more flexible compared to others as it can coexist with other open-source software licenses such as MIT, BSD, and Apache. For instance, if you integrate a portion of software that is licensed under Apache into a project governed by EUPL, that portion can retain its Apache license. In contrast, under the GPL, the entire codebase would need to be licensed under GPL.\n\n\n\n\n\n\n\n There are even instances when GPL licenses are incompatible with each other. For example, GPL-2.0 is incompatible with GPL-3.0. If a project uses GPL-2.0 you are essentially forced to use that license.\n\n\n\n\n\nLicense compatibility\n\n\n\n\n\n\n\nCompatibility of licenses. From: Bazuine, M. (2021). TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. Zenodo. https://doi.org/10.5281/zenodo.4629635\n\n\n\n\n\n\n\n\nTip Tip\n\n\n\nDon‚Äôt forget to check whether your software‚Äôs dependencies have restrictions on re-use.\n\n\nIt is advisable to contact your faculty‚Äôs data steward regarding licensing questions. If your project involves complex legal considerations, particularly regarding intellectual property rights or compliance with licensing agreements, the Innovation and Impact Center should also be involved.\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nTU Delft Research Software Policy\nTU Delft Guidelines on Research Software - the TU Delft Research Software Policy approved by the Executive Board\nChoose a license - a simplified guide on choosing an open-source license\ntl;drLegal - Plain English summaries of software licenses\nThe Turing Way - Licensing\nGitHub - Adding a license to a repository",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/write_a_readme.html",
    "href": "docs/software/documentation/write_a_readme.html",
    "title": "README",
    "section": "",
    "text": "A README file is essential for your software project as it helps users understand the purpose of your project, how to install and use it, and how to contribute. While the specific content of a README can vary from project to project, a good README should always include the following sections:\n\nThe purpose of the project.\nHow to cite the project.\nInstallation and usage instructions.\nThe terms under which the software is distributed (license).\n\n\nProject purpose\nClearly explain the purpose of your project, including its motivation and objectives. This section should serve as an introductory overview, helping users and contributors understand the essence of your project.\nConsider adding:\n\nBackground information\nComparison with alternatives, highlighting what sets your project apart\nLinks to related references or documentation\n\n\n\nHow to cite\nIf you want users to cite your project when they use it, provide a citation in this section, referring to a publication or DOI of the software.\nFor more information about citing the project, see the citation guide.\n\n\nInstallation and usage\nThis section should explain the steps needed to set up and use the software. Before installation, users must be aware of any prerequisites or dependencies that are required. This can include specific versions of programming languages, libraries, operating systems, data, and hardware.\n\nInstallation steps\nProvide a step-by-step guide for installation. This can involve downloading the software from a repository, compiling code, or using a package manager. Consider using package managers, such as pip or conda, to simplify the installation.\nExample: The scikit-learn GitHub repository provides a good example of the Installation section of a README.\n\n\nUsage\nYou can include the simplest possible usage example directly in the README and provide more complex examples in additional files or links.\nExample: The TensorFlow GitHub repository gives a small usage sample after installation instructions and provides a link with additional examples and tutorials.\n\n\n\nLicense\nYour README should specify the licensing terms. For more information, see the License guide.\n\n\n\n\n\n\nTip Tip\n\n\n\nIt is recommended to write README‚Äôs in markdown (README.md) formatting and placed in the project‚Äôs top-level directory. If you find your README is becoming too long, consider incorporating additional documentation files instead of omitting important details.\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nMake a README - README 101\nMaking READMEs readable",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "README"
    ]
  },
  {
    "objectID": "docs/software/fair_software/fair.html",
    "href": "docs/software/fair_software/fair.html",
    "title": "FAIR Software",
    "section": "",
    "text": "While originally targetting data management, the FAIR for Research Software (FAIR4RS) extends the FAIR principles to research software, which, unlike data, is executable and evolves over time. Ensuring the findability of software involves metadata, identifiers, and version control systems, while accessibility includes guidelines for obtaining, installing, and running the software. Interoperability involves adherence to community-driven standards or protocols, and reusability requires detailed documentation and user guides to effectively apply the software in new research projects.\n\n\n\n FAIR Software Checklist\nSet of recommendations for FAIR software. Add FAIR cards to your repository to track progress.\n\nLearn more ¬ª\n\n\n\n Software Management Plan\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nFAIR Guiding Principles for scientific data management and stewardship to research software\nFAIR4RS community in Zenodo\nFAIR Software Checklist - five recommendations for FAIR (scientific) software",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software"
    ]
  },
  {
    "objectID": "docs/software/getting_started.html",
    "href": "docs/software/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "Research software can be thought of as evolving through stages, reflecting how code evolves alongside the research process:\n\nExploratory stage: Code is written to test ideas, prototype methods, or quickly generate results. The focus is flexibility and speed rather than maintainability.\nSharable stage: When research outputs need to be communicated, validated, or reproduced, the software must reach a ‚Äúgood enough‚Äù state. It should be documented, versioned, and structured so that others can rerun or inspect the work.\nSustainable stage: Some research software grows beyond its original project to become a (critical) dependency for others. At this point, higher standards are needed, approaching the quality expectations of production software.\n\nAlongside these levels of maturity, researchers also rely on ‚Äúsoftware in research‚Äù (e.g., operating systems, libraries, dependencies) that may not have been created with research in mind but are essential for computation. For reproducibility, all of these components (exploratory scripts, research software, external dependencies, and documentation) must be identified, described, and made accessible following the FAIR principles for research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/software/getting_started.html#what-is-research-software",
    "href": "docs/software/getting_started.html#what-is-research-software",
    "title": "Getting started",
    "section": "",
    "text": "Research software can be thought of as evolving through stages, reflecting how code evolves alongside the research process:\n\nExploratory stage: Code is written to test ideas, prototype methods, or quickly generate results. The focus is flexibility and speed rather than maintainability.\nSharable stage: When research outputs need to be communicated, validated, or reproduced, the software must reach a ‚Äúgood enough‚Äù state. It should be documented, versioned, and structured so that others can rerun or inspect the work.\nSustainable stage: Some research software grows beyond its original project to become a (critical) dependency for others. At this point, higher standards are needed, approaching the quality expectations of production software.\n\nAlongside these levels of maturity, researchers also rely on ‚Äúsoftware in research‚Äù (e.g., operating systems, libraries, dependencies) that may not have been created with research in mind but are essential for computation. For reproducibility, all of these components (exploratory scripts, research software, external dependencies, and documentation) must be identified, described, and made accessible following the FAIR principles for research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/software/getting_started.html#why-does-research-software-matter",
    "href": "docs/software/getting_started.html#why-does-research-software-matter",
    "title": "Getting started",
    "section": "Why does research software matter?",
    "text": "Why does research software matter?\nCode is now as central to research as data or publications. Sharing and reusing it ensures that scientific work doesn‚Äôt stop at a single experiment but can be built upon, adapted, and tested by others. Even quick, exploratory scripts have value when preserved and explained. And when research software matures into tools used across a field, the benefits multiply by providing a foundation for whole communities.\nJournals and funders are also raising expectations. They increasingly expect code to be documented, reproducible, and accessible, both to validate findings and to support new discoveries. By treating research software as something that can grow from rough sketches into sustainable tools, researchers can improve the reliability of their own work while contributing lasting resources for others.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/software/getting_started.html#what-can-you-find",
    "href": "docs/software/getting_started.html#what-can-you-find",
    "title": "Getting started",
    "section": "What can you find?",
    "text": "What can you find?\nThis series of guides looks at research software through that lens of growth and maturity. You‚Äôll find:\n\nPractical advice on version control, environment management, and modular design.\nCommon pitfalls, including missing documentation, tricky dependencies, and reproducibility issues.\nGuidance on what ‚Äúgood enough‚Äù looks like at different stages, and how to raise the quality of your code when it becomes important to do so.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\nThe DCC guides don‚Äôt provide a full learning path (see About the guides), but the following resources can help you get started with research software development:\n\nThe Turing Way - A handbook to reproducible, ethical and collaborative data science.\nResearch Software Quality Toolkit for Sciences (RSQKit) - Best practices, tools, and resources for improving research software quality.\n\nFor hands-on training in essential skills:\n\nSoftware Carpentry - Entry-level workshops on Python, version control, and the UNIX shell.\nBuilding Better Research Software - Follow-up to Software Carpentry, focusing on research software best practices.\nIntermediate Research Software Development with Python - Covers more advanced Python topics for research software.\n\nFor additional workshops, see Courses & Workshops.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/index.html",
    "href": "docs/software/releases_archiving/index.html",
    "title": "Packaging, releases and archiving",
    "section": "",
    "text": "Eventually your software will be at a sharable and publishable point. One way to distribute your software is to package it and release it on a platform like PyPI (Python Package Index), or CRAN (Comprehensive R Archive Network). Additionally, you want to archive your software for long-term preservation in repositories like Zenodo or 4TU.\n\n\n\n Packaging\nPackage your software.\n\nLearn more ¬ª\n\n\n\n Releases\nPublish your software.\n\nLearn more ¬ª\n\n\n\n Archiving\nArchive your software.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html",
    "title": "Creating a Python package",
    "section": "",
    "text": "By turning your code into a package and hosting it on a platform like PyPI (Python Package Index), you enhance the quality and sustainability of your software, promote reuse and embrace contributions from external collaborators.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#pyproject.toml",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#pyproject.toml",
    "title": "Creating a Python package",
    "section": "pyproject.toml",
    "text": "pyproject.toml\nIn the Development workflow section we looked at how to structure your project. Here we will focus on the pyproject.toml file, which is a configuration file used in Python projects to define build system requirements and project metadata. It is part of the PEP 517 and PEP 518 specifications, which aim to standardize the way Python projects are built and packaged. The pyproject.toml consists of TOML tables, and can include [build-system], [project], or [tools] tables.\n\n[build-system]\nThe [build-system] table is essential because it defines which build backend you will be using, and also which dependencies are required to build your project. This is needed because frontend tools like pip are not responsible for transforming your source code into a distributable package, and this is handled by one of the build backends (e.g.¬†setuptools, Hatchling).\n\n\n\n\n\n\nTip Example\n\n\n\n\n\n[build-system]\nrequires = [\"setuptools&gt;=64.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n\n\n\n\n[project]\nUnder the [project] table you can describe your metadata. It can become quite extensive, but this is where you would list the name of your project, version, authors, licensing, dependencies specific to your project, and other requirements, as well as other optional information. For a detailed list of what can be included under [project] check the Declaring project metadata section of Python Packaging Guide.\n\n\n\n\n\n\nTip Example\n\n\n\n\n\n[project]\n\nname = \"exampleproject\"\n# Define the name of your project here. This is mandatory. Once you publish your package for the first time,\n# this name will be locked and associated with your project. It affects how users will\n# install your package via pip, like so:\n#\n# $ pip install exampleproject\n#\n# Your project will be accessible at: https://pypi.org/project/exampleproject/\n#\nversion = \"2.0.0\"\n# Version numbers should conform to PEP 440, and are also mandatory (but they can be set dynamic)\n# https://www.python.org/dev/peps/pep-0440/\n#\ndescription = \"Short description of your project\"\n# Provide a short, one-line description of what your project does. This is known as the\n# \"Summary\" metadata field:\n# https://packaging.python.org/specifications/core-metadata/#summary\n#\nreadme = \"README.md\"\n# Here, you can include a longer description which often mirrors your README file.\n# This description will appear on PyPI when your project is published.\n# This corresponds to the \"Description\" metadata field:\n# https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#readme\n#\nrequires-python = \"&gt;=3.10\"\n# Indicate the versions of Python your project is compatible with. Unlike the\n# 'Programming Language' classifiers, 'pip install' will verify this field\n# and prevent installation if the Python version does not match.\n#\nlicense = {file = \"LICENSE.txt\"}\n# This specifies the license.\n# It can be a text (e.g. license = {text = \"MIT License\"}) or a reference to a file with the license text as shown above.\n#\nkeywords = [\"field_specific_keyword1\", \"field_specific_keyword2\"]\n# Keywords that describe your project. These assist users in discovering your project on PyPI searches.\n# These should be a comma-separated list reflecting the nature or domain of the project.\n#\nauthors = [\n  {name = \"A. Doe\", email = \"author@tudelft.nl\" }\n]\n# Information about the original authors of the project and their contact details.\n#\nmaintainers = [\n  {name = \"B. Smith\", email = \"maintainer@tudelft.nl\" }\n]\n# Information about the current maintainers of the project and their contact details.\n#\n#\n#\n# Classifiers help categorize the project on PyPI and aid in discoverability.\n# For a full list of valid classifiers, see https://pypi.org/classifiers/\nclassifiers = [\n  # Indicate the development status of your project (maturity). Commonly, this is\n  #   3 - Alpha\n  #   4 - Beta\n  #   5 - Stable\n  #.  6 - Mature\n  \"Development Status :: 4 - Beta\",\n\n  # Target audience\n  \"Intended Audience :: Developers\",\n  \"Topic :: Scientific/Engineering\",\n\n  # License type\n  \"License :: OSI Approved :: MIT License\",\n\n  # Python versions your software supports. This is not checked by pip install, and is different from \"requires-python\".\n  \"Programming Language :: Python :: 3\",\n  \"Programming Language :: Python :: 3.8\",\n  \"Programming Language :: Python :: 3.9\",\n  \"Programming Language :: Python :: 3.10\",\n  \"Programming Language :: Python :: 3.11\",\n  \"Programming Language :: Python :: 3 :: Only\",\n]\n\n# Dependencies needed by your project. These packages will be installed by pip when\n# your project is installed. Ensure these are existing, valid packages.\n#\n# For more on how this field compares to pip's requirements files, see:\n# https://packaging.python.org/discussions/install-requires-vs-requirements/\ndependencies = [\n  \"numpy\", \n  \"pandas&gt;=1.5.3\", \n  \"matplotlib&gt;=3.7.1\"\n]\n#\n# You can define additional groups of dependencies here (e.g., development dependencies).\n# These can be installed using the \"extras\" feature of pip, like so:\n#\n#   $ pip install exampleproject[dev]\n#\n# These are often referred to as \"extras\" and provide optional functionality.\n[project.optional-dependencies]\ntest = [\"coverage\"]\n#\n[project.urls]\n\"Homepage\" = \"https://github.com/your_handle_or_organisation\"\n\"Source\" = \"https://github.com/your_handle_or_organisation/exampleproject\"\n#\n# List of relevant URLs for your project. These are displayed on the left sidebar of your PyPI page.\n# This can include links to the homepage, source code, changelog, funding, etc.\n#\n#\n# This [project] example was adopted from https://github.com/pypa/sampleproject/blob/main/pyproject.toml\n\n\n\n\n\n[tools]\nThe [tool] table contains subtables specific to each tool. For example, Poetry uses the [tool.poetry] table instead of the [project] table.\n\n\n\n\n\n\nTip Example: Poetry project setup\n\n\n\n\n\n\n\n\n\n\n\n\nImportant Difference between [build system] and [project]\n\n\n\n The [build-system] and [project] tables serve distinct roles. The [build-system] table must always be included, as it specifies the build tool used, regardless of the backend. On the other hand, the [project] table is recognized by most build backends for defining project metadata, though some backends may not and use a different format.\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nWriting your pyproject.toml\npyproject.toml specification\nConfiguring setuptools using pyproject.toml files\n\n\n\nBefore shifting to pyproject.toml, a common approach was to use a setup.py build script. You might encounter them in legacy projects.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nIs setup.py deprecated?\nHow to modernize a setup.py based project?",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#package-structuring",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#package-structuring",
    "title": "Creating a Python package",
    "section": "Package structuring",
    "text": "Package structuring\nIf you want to distribute your Python code as a package, you will need to have an __init__.py file in the root directory of your package. This allows Python to treat that directory as a package that can be imported. Every subfolder should also contain an __init__.py file.\nWhen importing a package, Python searches through the directories on sys.path looking for the package subdirectory. The presence of __init__.py files within these directories is crucial, as it tells Python that these directories should be treated as packages. This mechanism helps avoid the scenario where directories with commonplace names accidentally overshadow valid modules that appear later in the search path.\nWhile __init__.py can simply be an empty file, serving just to mark a directory as a package, it can also contain code that runs when the package is imported. This code can initialize package-level variables, import submodules, and other tasks.\nReferring to the project structure in our Development workflow guide, we can build on top of that structure.\n\n\n\n\n\n\nTip Reminder about flat vs src layout\n\n\n\n\n\nIn a flat layout, the project‚Äôs root directory directly contains the package directories and modules. This layout is straightforward and works well for simple projects.\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ your_pkg_name/ \n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ module.py\n‚îÇ   ‚îî‚îÄ‚îÄ subpkg1/\n‚îÇ       ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ\n...\nThe src layout places the package directory inside a top-level src directory. This layout helps prevent accidental imports from the current working directory, ensuring that you always import from the installed package rather than the source directory.\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ your_pkg_name/ \n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ module.py\n‚îÇ       ‚îî‚îÄ‚îÄ subpkg1/\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ\n...\n\n\n\nSo our example package structure would now look like this:\n\nPython\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                     # Documentation directory\n‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks\n‚îú‚îÄ‚îÄ src/                      # Contains your main code\n‚îÇ   ‚îî‚îÄ‚îÄ your_pkg_name/        # A folder where your organized code lives - your package\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # A marker file for package initialization\n‚îÇ       ‚îú‚îÄ‚îÄ module.py         # A nested module\n‚îÇ       ‚îî‚îÄ‚îÄ subpkg1/          # A sub-package\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py   # A marker file for sub-package initialization\n‚îú‚îÄ‚îÄ tests/                    # Your test directory  \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # Data files used in the project (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # Files from your analysis (if applicable)\n‚îú‚îÄ‚îÄ results/                  # Results (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # Untracked files \n‚îú‚îÄ‚îÄ pyproject.toml            # TOML file\n‚îÇ\n‚îÇ\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # License information\n\n\n\nYou might notice that in our updated structure the requirements.txt is absent. In many cases, if you have a pyproject.toml file, you may not need a requirements.txt file anymore, since the pyproject.toml file is part of the new standardized Python packaging format (defined in PEP 518) and can include dependencies.\nHowever, some deployment and CI/CD pipelines might still expect a requirements.txt file, because a set of fixed dependency versions creates more stable pipelines. For simple projects, you can still prefer to use a requirements.txt for its simplicity and wide adoption.\n\nIt is not considered best practice to use the pyproject.toml to pin dependencies to specific versions or to specify sub-dependencies (i.e.¬†dependencies of your dependencies). This is overly-restrictive, and prevents a user from gaining the benefit of dependency upgrades. For more info, see this discussion.\n\n\nlib/ and build/ directories\nIt is possible that you might have lib/ and build/ directories in your project. These directories are not part of the standard Python package structure, but they can be created by certain tools or processes.\n\nThe build/ directory is typically used to store compiled or built artifacts of your project, such as binary executables, wheels, or other distribution files. This directory is usually not part of your source code repository and is generated during the (automated) build or packaging process.\nThe lib/ directory stores third-party libraries or dependencies that are not installed through a package manager. By specifying your project‚Äôs dependencies in the pyproject.toml file, and using a package manager like pip or poetry to install and manage them, these dependencies will be automatically downloaded and installed in the appropriate location (usually the site packages directory).\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGood integration practices\nPython documentation packages",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#local-package-installation",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#local-package-installation",
    "title": "Creating a Python package",
    "section": "Local package installation",
    "text": "Local package installation\nBy installing a Python package locally during development you can test your changes in an environment that mimics how the package will be used once it is deployed. This process allows you to ensure that your package works correctly when installed and imported by others.\nYou can use pip to install your package in editable mode (-e). This way, changes you make to the source code are immediately available without needing to reinstall the package.\npip install -e .",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#next-steps",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#next-steps",
    "title": "Creating a Python package",
    "section": "Next steps",
    "text": "Next steps\nOnce you have your package ready, you can publish it. Visit our Release your Python package guide for information on how to publish your package to PyPI.\n\nUsing uv for building and releasing Python packages\nIf you‚Äôre looking to streamline your Python packaging workflow even further, consider using uv. Alongside traditional tooling, uv offers a fast, unified workflow for packaging. It reads project metadata from pyproject.toml, builds source and wheel distributions, and can publish directly to package indexes (e.g., PyPI). This consolidates steps that typically require separate tools (e.g., build and twine) and helps keep release processes consistent across local development and continuous integration.\nIn practice, uv build produces artifacts under dist/ using the configured build backend, and uv publish can upload those artifacts using PyPI API tokens or interactive credentials. Because uv also manages environments and Python runtimes, the same tool can be used to create an isolated build environment, lock dependencies for reproducibility, and run test commands before releasing.\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nuv documentation",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html",
    "href": "docs/software/releases_archiving/releases/releases.html",
    "title": "Releases",
    "section": "",
    "text": "Once you have your software packaged, you can release it to platforms like PyPI or CRAN. While your software might be already publicly available on GitHub, releasing it to a package repository allows easy access and installation for users. The most common platforms for releasing software packages are PyPI for Python packages and CRAN for R packages.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#changelog",
    "href": "docs/software/releases_archiving/releases/releases.html#changelog",
    "title": "Releases",
    "section": "Changelog",
    "text": "Changelog\nMaintaining a changelog provides a clear history of changes made to your software and version information. It helps users and developers understand what has changed between versions, what modifications, fixes, and enhancements have been made.\n\nHow do I make a good changelog?\nExample from eScience Center",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#semantic-versioning",
    "href": "docs/software/releases_archiving/releases/releases.html#semantic-versioning",
    "title": "Releases",
    "section": "Semantic versioning",
    "text": "Semantic versioning\nSemantic Versioning (SemVer) is a versioning scheme that reflects changes in your software systematically. It consists of three numbers: major, minor, and patch (e.g., 1.9.1).\n\nMajor version increments are meant for significant changes that may make backward-incompatible changes.\nMinor version increments add functionality in a backward-compatible manner.\nPatch version increments are for backward-compatible bug fixes only.\n\nAdopting this practice helps both users and developers anticipate the impact of updating the software. It clearly communicates the nature of the changes.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#release-notification",
    "href": "docs/software/releases_archiving/releases/releases.html#release-notification",
    "title": "Releases",
    "section": "Release notification",
    "text": "Release notification\nAfter a release, it is important to communicate what has changed. Release notes are detailed descriptions of the new changes, fixes, and sometimes known issues. They are usually published alongside the changelog in repositories. You could use GitHub Releases, it is a feature that allows you to present your software, along with the corresponding source code, changelog, and release notes.\nIt is typical for a release on PyPI or CRAN to mirror what was put under a GitHub release. This is not a requirement, but it is a good practice to keep the information consistent across platforms.\n\nAutomatically generated release notes\nGitHub provides a useful feature to automatically generate release notes for new versions of your software. It scans the commits between your releases and compiles a summary of the changes, fixes, and enhancements made. This can not only save time but also help to avoid undocumenting changes.\nHow to enable automatic release notes:\n\nGo to your repository on GitHub and navigate to the releases section.\nDraft a new release. When you select a tag, GitHub will offer an option to auto-generate the release notes based on the commits since the last release.\nCustomize the release notes. You can edit the auto-generated content to add more details or format it according to your preferences.\nWhen publishing, the release notes will be attached to your release.\n\nThis feature is particularly helpful for maintaining accurate and up-to-date release documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html",
    "title": "Release your Python package",
    "section": "",
    "text": "After bundling your source code into a package, you can publish it to PyPI. This will allow others to easily install your package using pip.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html#testing-packaging-on-testpypi-before-publishing-to-pypi",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html#testing-packaging-on-testpypi-before-publishing-to-pypi",
    "title": "Release your Python package",
    "section": "Testing packaging on TestPyPI before publishing to PyPI",
    "text": "Testing packaging on TestPyPI before publishing to PyPI\nBy testing your package on TestPyPI before publishing it to PyPI, you can identify and address any issues with your package metadata, dependencies, or distribution files before making your package publicly available.\nYou‚Äôll need to create an account for TestPyPI. The next step is to create distribution packages for your package. These packages are archives that can be uploaded to TestPyPI/PyPI and installed using pip. Then you can use Twine to upload your package to TestPyPI.\n\nRegister on TestPyPI.\nCheck if PyPA build is installed:\npip install --upgrade build\nTo create distribution packages, navigate to the directory where your pyproject.toml file is located, and run:\n\n\nLinux/macOSWindows\n\n\npython3 -m build\n\n\npy -m build\n\n\n\n\n\n\n\n\n\nTip Output\n\n\n\nAfter running this command, you‚Äôll see a substantial amount of text output. Upon completion, it will generate two files (a wheel and .tar.gz file) in the dist/ directory. The .tar.gz file represents a source distribution, while the .whl file is a built distribution. More recent versions of pip prioritize the installation of built distributions, reverting to source distributions if necessary. It‚Äôs advisable to always upload a source distribution and include built distributions compatible with the platforms your project supports.\n\n\n\nThen, install Twine:\npip install twine\nUpload to TestPyPI by specifying the --repository flag:\ntwine upload --repository testpypi dist/*\n\nYou will be prompted to enter your username and password for TestPyPI.\nIf you have a ~/.pypirc file, Twine will use the credentials from there.\n\nYou will find your package on https://test.pypi.org/project/your_pkg_name. You can then pip install it by adding the --index-url flag:\npip install --index-url https://test.pypi.org/simple/your_pkg_name",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html#publishing-to-pypi",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html#publishing-to-pypi",
    "title": "Release your Python package",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nOnce you have tested your package on TestPyPI and ensured everything works as expected, you can publish it to the official PyPI repository. It will be accessible to anyone in the Python community, allowing them to install your package using a simple pip install your_pkg_name command.\nYou‚Äôll also need an account for PyPI. TestPyPI and PyPI use separate databases so you need to register on both sites. However, the workflow is about the same:\n\nRegister on PyPI.\nRun pip install --upgrade build to ensure you have the latest version of the build tool.\nNavigate to the directory where your pyproject.toml file is located, and run:\n\n\nLinux/macOSWindows\n\n\npython3 -m build\n\n\npy -m build\n\n\n\n\nUpload your package to PyPI:\ntwine upload dist/*\n\nInput your credentials associated with the account you registered on the official PyPI platform.\n\nYour package is live on PyPI!\nYou can now install it by simply pip install your_pkg_name.\n\n\n\n\n\n\n\nTip Tip\n\n\n\nIf you need a particular name for your package, check whether it is taken on PyPI and claim it as soon as possible if available.\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nRealPython Packaging guide\nTwine Read the Docs\nUsing TestPyPI",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/testing/intermediate.html",
    "href": "docs/software/testing/intermediate.html",
    "title": "More testing concepts",
    "section": "",
    "text": "Code Coverage\nCode coverage measures how much of your code is executed during testing. It is a useful metric to ensure that your tests are comprehensive and indicate your code‚Äôs quality. If your software becomes a dependency for others, a code coverage of 70% or higher is recommended for unit tests.\n\nPythonMATLAB\n\n\n\npytest-cov plugin (for pytest)\nCoverage.py documentation\n\n\n\n\nCollect code coverage with Command Window execution (since R2023b)\nCode coverage with Test Browser (since R2023a)\nCollect code coverage metrics\n\n\n\n\n\n\nError handling\nTests should check if your code behaves as expected when it encounters errors. This includes testing if the code raises the correct exceptions when given invalid input or when an error occurs.\n\nPythonMATLAB\n\n\n\nAssert raised exceptions\n\nExample in Python:\ndef divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n\ndef test_divide_by_zero():\n    with pytest.raises(ValueError):\n        divide(1, 0)\n\n\n\nVerify function throws specific exceptions\n\n\n\n\n\n\nFixtures\nFixtures are predefined states or sets of data used to set up the testing environment, ensuring consistent conditions for tests to run reliably. Fixtures can be used to set up databases, create temporary files, or initialize other resources, that then available to all tests in a test suite.\n\nPythonMATLAB\n\n\n\nHow to use fixtures\n\n\n\n\nCreate shared fixtures\n\n\n\n\n\n\nParameterization\nParameterization involves running the same test with different inputs or configurations to ensure broader coverage and identify potential edge cases.\n\nPythonMATLAB\n\n\n\nParameterizing unit tests\n\n\n\n\nCreate a basic parameterized test\n\n\n\n\n\n\nMocking\nMocking (or monkeypatching) is a technique used to simulate the behavior of dependencies or external systems during testing, allowing isolated testing of specific components. For example, if your software requires a connection to a database, you can mock this interaction during testing.\n\n\n\nMathWorks, MATLAB Mocking Diagram, MATLAB Documentation, link to image.\n\n\n\n\n\n\n\n\nNoteüêí Monkeypatching?\n\n\n\n\n\nThe term monkey patch seems to have come from an earlier term, guerrilla patch, which referred to changing code sneakily ‚Äì and possibly incompatibly with other such patches ‚Äì at runtime. The word guerrilla, nearly homophonous with gorilla, became monkey, possibly to make the patch sound less intimidating.\n\n\n\n\nPythonMATLAB\n\n\n\nHow to monkeypatch/mock modules and environments\n\n\n\n\nCreate Mock Object\n\n\n\n\n\n\nMarks and tags\nYou can use test tags to group tests into categories and then run tests with specified tags. This is useful when you want to run only a subset of tests, such as regression tests, or when ignoring slow tests during development.\n\nPythonMATLAB\n\n\n\nWorking with custom markers\n\n\n\n\nTag unit tests",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Advanced concepts"
    ]
  },
  {
    "objectID": "docs/software/testing/python.html",
    "href": "docs/software/testing/python.html",
    "title": "Testing in Python",
    "section": "",
    "text": "In Python, two widely used testing frameworks are pytest and unittest. This guide focuses on pytest, which is recommended for its simplicity and readability. If you are new to testing in Python, pytest is a great starting point.\n\n\n\n\n\n\nNote What is the difference between pytest and unittest?\n\n\n\n\n\n\npytest is a third-party testing framework that is more user-friendly, requires less boilerplate, and offers better readability.\nunittest is part of the Python standard library and follows a more traditional object-oriented style of writing tests.\n\n\n\n\n\nStep 1. Setup a testing framework\nInstall pytest\npip install pytest\nA good practice is to organize the codebase into a src directory for the source code and a tests directory for the test suite. For example:\nsrc/\n    mypkg/\n        __init__.py\n        add.py\n        draw_random_number.py\ntests/\n    test_add.py\n    test_draw_random_number.py\n    ...\n\n\nStep 2. Identify testable units\nIdentify functions, methods, or classes on your code that should be tested. Focus on critical computations and potential points of failure.\n\n\nStep 3. Write test cases\nWrite test functions using pytest. Here‚Äôs an example:\n# src/mypkg/add.py\ndef add(x,y):\n    return x + y\n\n# tests/test_add.py\nfrom mypkg.add import add\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(0, 0) == 0\n    assert add(-1, -1) == -2\n\n\n\n\n\n\nTip Tip\n\n\n\nLimit the number of assert statements in a single test function. Otherwise, when a particular assert fails, the remaining assertions in the test function will not be executed.\n\n\n\n\nStep 4. Run tests locally\nRun all tests in the project by executing the following command:\npytest\nRun a specific test file:\npytest tests/test_add.py\n\n\nStep 5. Debug and fix failing tests\nThe test results displayed in the console will help you to identify any failures or errors. If errors occur, debug the failing tests by examining failure messages and stack traces.\n\n\nStep 6. Run coverage report locally\nGenerate a coverage report to gain insights into which parts of the codebase have been executed during testing (see Code Coverage).\npip install pytest-cov\npytest --cov=mypkg tests/\n\n\nStep 7. Automate testing with Continuous Integration\nIntegrate youre test suite with a Continuous Integration service (e.g., GitHub Actions) to run tests automatically on every code change.\n\n\n\n\n\n\nTip Learning materials for automated testing\n\n\n\n\nIntermediate Research Software Development - CI for Automated Testing\nCode Refinery - Automated testing\n\n\n\n\n\nExamples of repositories with tests\n\neScience Center - Project matchms\nPandas library - Repository tests\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nPytest - Getting Started\nCode Refinery - Pytest exercise\nRealPython - Effective testing with pytest",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in Python"
    ]
  },
  {
    "objectID": "docs/software/testing/strategies.html",
    "href": "docs/software/testing/strategies.html",
    "title": "Testing strategy",
    "section": "",
    "text": "In designing test cases for research software, it can be useful to conceptually differentiate between tests that verify the technical correctness of the code and tests that check the scientific validity of the results. With technical software tests, you check whether a function behaves as expected. With a scientific test, you compare the outcome of a function to known (experimental) scientific results.\nThe following questions can help you decide what to test in your software:\n\nHow can I ensure the algorithms and mathematical models implemented in the software are correct?\nHow can I verify that the input data types, formats, and ranges adhere to expected standards and constraints?\nHow does the software behave at boundary conditions and extreme values of input parameters?\nHow does the software perform under varying workloads and dataset sizes, and is it scalable for large-scale simulations or data processing tasks?\nHow do I compare the software‚Äôs results against existing literature, experimental data, or previous simulations to check their accuracy?\n\n\nRoadmap to testing\n\nNew to testingFamiliar with testing\n\n\n\n\n\n\n\n\nNote1. Familiarize yourself with the basics\n\n\n\n\n\nBegin by learning a testing framework that is well-suited for your programming language; for example, you might explore pytest if you are using Python. It is also important to understand the basic types of tests, focusing primarily on unit tests and simple integration tests.\n\n\n\n\n\n\n\n\n\nNote2. Identify critical components in your code\n\n\n\n\n\nTake the time to inspect your codebase and determine which parts are most important or particularly prone to error. These are the areas where you should focus your testing efforts.\n\n\n\n\n\n\n\n\n\nNote3. Start small\n\n\n\n\n\nBegin by writing minimal (unit) tests for individual functions or modules. Creating tests based on expected inputs and outputs will help confirm that your code behaves as intended. The primary goal at this stage is to understand the testing process rather than aiming for complete coverage from the start.\n\n\n\n\n\n\n\n\n\nNote4. Incrementally expand the number of tests\n\n\n\n\n\nAs you introduce new functionality, adopt an approach by writing tests alongside your new code. Over time, gradually add tests to your existing code ‚Äì especially when you make changes or improvements ‚Äì to steadily increase overall test coverage and improve the reliability of your software. Make sure to focus on testing the critical parts of your codebase first.\n\n\n\n\n\n\n\n\n\nNote5. Refactor your code for testability\n\n\n\n\n\nIf you find it difficult to write tests for your codebase, consider refactoring it into smaller, more testable units. Clear documentation and comments on your functions will further aid in writing tests by providing a well-defined understanding of each component‚Äôs intent.\n\n\n\n\n\n\n\n\n\nNote6. Automate and adopt\n\n\n\n\n\nFinally, add automated testing to your development workflow by using a continuous integration tool to run tests automatically with each code change. Establishing a regular habit of testing will, over time, lead to significant improvements in the quality and reliability of your research code.\n\n\n\n\n\n\n\n\n\n\n\nNote1. Develop a testing strategy\n\n\n\n\n\nFor researchers already experienced with testing, it is useful to develop a testing strategy. Start by adopting the test pyramid approach: ensure that all core functions and algorithms are covered by unit tests, verify that modules interact correctly with integration tests, and, when applicable, use end-to-end tests to simulate real-world user workflows. Regularly running regression tests is also important to catch any unintended side effects of code changes.\nAim for comprehensive test coverage to ensure that critical parts of your codebase are thoroughly tested. A good benchmark is to test at least 70% of your code base with unit tests.\n\n\n\n\n\n\n\n\n\nNote2. Adopt a practice of writing tests first\n\n\n\n\n\nConsider methodologies such as Test-Driven Development (TDD) into your workflow. With TDD, you write tests before you write the actual code to define the desired behavior, ensure clear specifications, and obtain immediate feedback.\n\n\n\n\n\n\n\n\n\nNote3. Research-specific testing\n\n\n\n\n\nIn a research context, certain quality measures become particularly important. Prioritize reproducibility by writing tests that verify experiments yield consistent outputs for a given dataset and configuration. If your code relies on statistical methods, use fixed random seeds to ensure reproducibility across different runs. Additionally, consider implementing validation tests that compare your results against known benchmarks or experimental data.\n\n\n\n\n\n\n\n\n\nNote4. Consider advanced testing practices\n\n\n\n\n\n\nCoverage Analysis:\nEmploy code coverage tools to identify untested paths and critical areas that require additional testing.\nParameterization:\nRun tests with a range of inputs to validate robustness across different scenarios.\nError Handling: Verify that your code behaves as expected when encountering errors.\nFixtures: Use fixtures to set up a consistent and reusable testing environment.\nMocking:\nUse mocks to simulate external systems or heavy computations, isolating tests for faster feedback.\nTagging and Filtering: Organize tests into categories and run specific subsets based on tags or filters.  Advanced testing practices\n\n\n\n\n\n\n\n\n\n\nNote5. Integrate with CI/CD\n\n\n\n\n\nFinally, integrate your testing processes into a continuous integration/deployment pipeline. Automate your test runs so that every code commit triggers a suite of tests to catch issues early in the development cycle. Monitor test performance and code coverage over time to continuously refine and enhance your testing strategy, ensuring a high level of quality and reliability in your research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Roadmap to testing"
    ]
  },
  {
    "objectID": "docs/software/tools/tools.html",
    "href": "docs/software/tools/tools.html",
    "title": "Tooling",
    "section": "",
    "text": "üèóÔ∏è Under construction!"
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html",
    "href": "docs/software/development_workflow/branch_management.html",
    "title": "Branch management",
    "section": "",
    "text": "Branch management in Git is essential for collaborative software development in version-controlled environments. The core advantage of branching is that it provides separate, dedicated environments for code development, independent of the main (working) version. This approach enables parallel development streams, allowing for experimentation and modification without impacting the stable main version of the project. Note, this approach is also valuable for projects with only one developer!\nA branching strategy defines a set of best practices for writing, merging, and releasing code. Choosing the right approach helps teams collaborate efficiently while maintaining a stable codebase.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html#branch-models",
    "href": "docs/software/development_workflow/branch_management.html#branch-models",
    "title": "Branch management",
    "section": "Branch models",
    "text": "Branch models\nWhile multiple branching strategies exist, GitHub Flow and GitFlow are well-suited for research software projects, depending on collaboration style and release cycle:\n\nGitHub Flow is a simplified and straightforward workflow, relying on a single main branch. Developers create so-called feature branches off of the main branch, work on their changes, and then merge them back into the main branch via pull requests. The process is built around the principle of continuous collaboration and is particularly useful for projects where regular updates and deployments are common.\nGitflow relies on two primary branches - main and develop. Developers create feature branches off of the develop branch. Once a feature is complete, it is merged back into the develop branch, which itself is merged with main for each new release of the software. It can be a better choice for managing larger projects with distinct release cycles and versioning requirements (e.g., tools tied to publications) or requiring parallel development of features and experiments.\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nGitHub Flow getting started\nIntroduction GitHub Flow",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html#branch-management-in-action",
    "href": "docs/software/development_workflow/branch_management.html#branch-management-in-action",
    "title": "Branch management",
    "section": "Branch management in action",
    "text": "Branch management in action\n\nPersonal projects and small teams\nGitHub Flow model:\n\nTypically starts with just the main branch.\nUse branches for unfinished/untested ideas.\nUse branches when you are not sure about a change.\nAdd tags to mark important milestones.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#bf8040',\n              'git2': '#40bfbf'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph LR:\n   commit\n   commit\n   branch \"feature_a\"\n   checkout \"feature_a\"\n   commit\n   commit type:REVERSE\n   checkout main\n\n   commit\n   branch \"feature_b\"\n   checkout \"feature_b\"\n   commit\n   commit\n   commit\n\n   checkout main\n   merge \"feature_b\"\n   commit tag:\"v1.0.0\"\n\n\n\n\n\n\n\nWhen applying this workflow for small teams, you accept things breaking sometimes.\nWhen there is more control required, follow:\n\nThe main branch is write-protected.\nYou create new feature branches for changes.\nChanges are reviewed before they are merged to the main branch.\nOnly merges to main branch through pull requests (and optionally code reviews).\n\n\n\n\n\n\n\nTipWhat is the difference between master and main branches?\n\n\n\n\n\nGitHub decided to rename the default branch from master to main for new repositories after October 2020. This change was part of a broader industry move to replace terms that may be considered insensitive or non-inclusive. It‚Äôs important to note that while the terms main and master refer to the default branch in a repository, they are functionally the same. Be aware that repositories created before October 2020 may still use master instead of main.\n\n\n\n\n\nDistributing releases\nWhen you need to distribute releases, your main branch will serve as the latest stable version.\n\nThe main branch is protected and read-only.\nYou set up a develop branch for active development.\nCreate feature branches of the develop branch.\nMerge feature branches (through Pull Requests) back to develop.\nOnly merge develop into main when releasing a new stable (and tested) version.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#bf8040',\n              'git3': '#40bfbf'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n    \n   commit tag:\"v0.1.0\"\n   branch develop\n   commit\n\n   checkout develop\n   commit\n   branch feature_a\n   checkout feature_a\n   commit\n   checkout develop\n   merge feature_a\n   commit\n   commit\n   branch feature_b\n   checkout feature_b\n   commit\n   commit\n   checkout develop\n   merge feature_b\n   commit\n\n   checkout main\n   merge develop tag:\"v0.2.0\"\n\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop tag:\"v0.3.0\"\n\n   checkout develop\n   commit\n   commit\n\n\n\n\n\n\n\n\n\nAdd additional supporting branches\nWhen a critical bug in the stable version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the main branch that marks this version. After fixing, hotfix is then merged with both main and develop.\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#7da97a'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n    \n   commit tag:\"v0.2.0\"\n   branch develop\n   commit\n   commit\n   commit\n   commit\n   checkout main\n   branch hotfix\n   commit\n   commit\n   checkout develop\n   merge hotfix\n   checkout main\n   merge hotfix tag:\"v0.2.1\"\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop tag:\"v0.3.0\"\n   \n\n\n\n\n\n\n\n\nThe complete Gitflow model in action\nGitFlow‚Äôs structured approach with parallel production and integration branches, supplemented by feature/release/hotfix branches, was specifically designed for versioned software requiring maintenance of multiple production releases.\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#bf8040',\n              'git3': '#eae0b8',\n              'git4': '#7da97a'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n   commit\n   branch develop\n   commit\n   checkout develop\n   commit\n   branch feature\n   checkout feature\n   commit\n   commit\n   checkout develop\n   merge feature\n   commit\n   branch release\n   checkout release\n   commit\n   commit\n   checkout main\n   merge release\n   checkout develop\n   merge release\n   checkout main\n   branch hotfix/security\n   checkout hotfix/security\n   commit\n   checkout main\n   merge hotfix/security\n   checkout develop\n   merge hotfix/security\n\n\n\n\n\n\n\n\n\n\n\n\nNote Learn more\n\n\n\n\nCode Refinery - Branching models\nGitHub - Branch protection rules\nTag names following semantic versioning\nGitHub tags and releases",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/software_design.html",
    "href": "docs/software/development_workflow/software_design.html",
    "title": "Software design principles",
    "section": "",
    "text": "Writing code represents only one aspect of software design. The real challenge lies in creating systems that stand the test of time. Good software design creates systems that enable reproducible science, collaboration, and remain usable and trustworthy for the long term. It provides a shared structure that allows researchers to build on each other‚Äôs work rather than starting from scratch.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Software design"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/software_design.html#why-software-architecture-matters-in-research",
    "href": "docs/software/development_workflow/software_design.html#why-software-architecture-matters-in-research",
    "title": "Software design principles",
    "section": "Why software architecture matters in research",
    "text": "Why software architecture matters in research\nScientific research thrives on reproducibility and transparency. Software that is poorly structured or insufficiently documented risks becoming a ‚Äúblack box,‚Äù where results cannot be trusted or reused. A well-defined architecture helps to:\n\nEnsure reproducibility\nBy separating data handling, computation, and analysis, workflows can be rerun and verified.\n\nEnable collaboration\nClear modular design lowers the entry barrier for new contributors (students, collaborators, or other labs).\n\nSupport extensibility\nResearch questions evolve; a good design makes it easier to test new hypotheses, add new algorithms, or integrate new data sources.\n\nEnhance sustainability\nMany scientific codes outlive the projects they were written for. Sustainable architecture ensures they can be maintained and reused beyond the initial scope.\n\nBridge disciplines\nIn multidisciplinary research, software is often used by scientists who are not software engineers. Transparent architecture helps communicate design decisions to a wide audience.\n\n\n\n\n\n\n\nGood architecture in research software prioritizes flexibility over scale. The code should expand possibilities for scientific insight rather than restrict them.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Software design"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/software_design.html#step-by-step-guide-to-designing-research-software",
    "href": "docs/software/development_workflow/software_design.html#step-by-step-guide-to-designing-research-software",
    "title": "Software design principles",
    "section": "Step by step guide to designing research software",
    "text": "Step by step guide to designing research software\nDesigning architecture sounds abstract, but it can be done in simple steps. Think of it as planning your research workflow, but for code.\n\nStep 1: Define your goals\n\nFocus on 3‚Äì5 goals (also called quality attributes), such as:\n\nBe reproducible (others can run it and get the same results)\n\nBe flexible (easy to test new ideas)\n\nBe efficient (runs in reasonable time on available computers)\n\nBe portable (runs on my laptop, a cluster, or the cloud)\n\n\nWrite these goals down in plain language. They will guide your choices.\n\n\n\nStep 2: Capture scope and constraints\n\nResearch questions and hypotheses\n\nData types, sizes, and growth expectations\n\nCompute targets (laptops, HPC, cloud)\n\nLicensing, ethics, and data sensitivity (e.g., GDPR)\n\n\n\nStep 3: Draft a minimal architecture\n\nSketch the big steps of your research pipeline, identify modules, and define how data flows between them. For example:\n\nData collection\nCleaning and preprocessing\nAnalysis or modeling\nProducing results\nVisualization and reporting\n\nOnce you have these steps, make the design more concrete:\n\nIdentify modules: break the workflow into parts such as ingestion ‚Üí preprocess ‚Üí model ‚Üí analyze ‚Üí visualize.\n\nDefine data flow: show how inputs and outputs move between steps (what comes in, what goes out).\nDescribe interfaces: note what format or structure each step expects (e.g., ‚ÄúCSV file‚Äù, ‚ÄúJSON config‚Äù, ‚ÄúNetCDF dataset‚Äù).\n\nDraw a simple diagram: use a quick sketch, or a tool like Mermaid, PlantUML, or a C4 diagram (described below in more detail).\n\n\n\n\n\n\n\nflowchart LR\n  A[Data Collection] --&gt; B[Cleaning / Preprocess]\n  B --&gt; C[Analysis / Model]\n  C --&gt; D[Results]\n  D --&gt; E[Visualization / Reports]\n\n\n Example minimal research pipeline architecture \n\n\n\n\n\nStep 4: Build a small working version\n\nInstead of trying to design everything at once, create a very simple version of your software that runs from start to finish.\n\nUse a small dataset and pass it through all the steps (data ‚Üí preprocess ‚Üí analysis ‚Üí results).\n\nThis ‚Äúend-to-end slice‚Äù shows that your design works in practice and helps uncover problems early.\n\nIt also gives collaborators something concrete they can run, test, and discuss.\n\n\n\nStep 5: Automate and record what happens\n\nOnce the small version works, make it easy to repeat and share:\n\nPut your notes and diagrams in the same repository as the code.\n\nAdd a short test dataset that can be processed quickly (so others can check their setup).\n\nAutomate simple checks (e.g., run the pipeline on the test data after each change).\n\nRecord important details automatically, such as:\n\nthe software version\n\nthe parameters used\n\nthe input data files\n\n\n\nThese details make it possible for you and others to reproduce results later, which is central to good science.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Software design"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/software_design.html#the-c4-model-for-research-software-design",
    "href": "docs/software/development_workflow/software_design.html#the-c4-model-for-research-software-design",
    "title": "Software design principles",
    "section": "The C4 Model for research software design",
    "text": "The C4 Model for research software design\nWhen explaining software to collaborators, it is easy to get lost in too much detail or not enough detail. The C4 model provides a structured way to show software architecture at different levels, from the big picture down to the finer details. It is widely used in both industry and academia because it adapts well to different audiences: principal investigators, collaborators, new students, and developers.\nThe ‚ÄúC4‚Äù stands for Context, Containers, Components, and Code. Think of it as zooming in on your software with a microscope, each step shows more detail.\n\nLevel 1: System context\nThis is the big picture.\nIt shows how your research software fits into the overall scientific workflow:\n\nWho uses it (e.g., researchers, students, automated pipelines)\n\nWhat external systems or tools it connects to (e.g., databases, HPC cluster, external datasets)\n\nWhat the software produces (e.g., processed datasets, figures, reports)\n\nThis view is useful for project proposals, publications, and discussions with collaborators who don‚Äôt need technical details.\nHere is how a simple research workflow could be shown at Level 1 (System Context):\n\n\n\n\n\nflowchart TB\n  Researcher --&gt;|provides data + config| Software[Research Software System]\n  Software --&gt;|results, figures, reports| Researcher\n  Software --&gt;|stores results| Repository[(Data/Results Repository)]\n  ExternalDB[(External Database)] --&gt;|datasets| Software\n\n\n System context for a research pipeline \n\n\n\n\n\nLevel 2: Containers\nThis level zooms in to show the main building blocks inside your system.\nA ‚Äúcontainer‚Äù here does not mean Docker (though it could), it just means a large part of your software that can run on its own. Examples:\n\nA command-line tool\n\nA database or data store\n\nA web interface or API\n\nA workflow engine (e.g., Snakemake, Nextflow)\n\nThis view shows how these pieces talk to each other and what responsibilities each one has.\n\n\nLevel 3: Components\nEach container can be broken down into smaller parts called components.\nFor example, inside an ‚Äúanalysis‚Äù container, you might have:\n\nA data loader\n\nA statistical model\n\nA visualization module\n\nThis view is useful for developers and contributors who need to understand how a part of the system works internally.\n\n\nLevel 4: Code (optional)\nAt the deepest level, you can describe the code structure itself (e.g., class diagrams, functions, or modules).\nThis level is not always needed, but for critical parts of the software, it can help with onboarding new developers or ensuring correctness.\n\n\n\n\n\n\n\nTip Why use C4 for research software?\n\n\n\n\nClarity for different audiences: from PIs to PhD students to RSEs, each level provides the right amount of detail.\n\nShared understanding: reduces confusion when multiple people (from different fields) work on the same project.\n\nTransparency and reproducibility: makes it easier for others to understand how results are produced.\n\nSustainability: helps future researchers maintain or extend the software long after the original team moves on.\n\n\n\n\n\n\n\n\n\n\nNote Further reading\n\n\n\n\nC4 Model: official site with examples and explanations.\nSimon Brown‚Äôs book ‚ÄúSoftware Architecture for Developers‚Äù: practical introduction to C4.\nMartin Fowler - ‚ÄúEssays on Software Architecture‚Äù\n\nPractical tools\n\nAwesome Research Software Engineering: curated resources and tools.\n\nPlantUML: diagramming tool often used in research docs.\n\nMermaid Live Editor: another useful to create and preview diagrams for documentation.\n\nTalks and videos\n\nResearch Software Hour: YouTube playlist with practical software design tips\n\nSoftware Sustainability Institute YouTube: talks and tutorials",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Software design"
    ]
  }
]