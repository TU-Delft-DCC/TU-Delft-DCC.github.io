[
  {
    "objectID": "docs/software/development_workflow/collaboration.html",
    "href": "docs/software/development_workflow/collaboration.html",
    "title": "Collaboration",
    "section": "",
    "text": "Effective collaboration is a fundamental aspect of any successful software development project.\nKey points:",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Collaboration"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/collaboration.html#collaborative-workflow",
    "href": "docs/software/development_workflow/collaboration.html#collaborative-workflow",
    "title": "Collaboration",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\nFollowing the GitHub Flow model, everything starts from the main branch. Developers can create feature branches from the main branch to isolate their work. Once ready, changes are merged back to main.\n\n\n\n\n\n\n Workflow variations\n\n\n\nWhile the GitHub Flow workflow focuses on main, some opt for a hybrid approach to include a develop branch either as a staging area for pre-release testing or ongoing integration (aggregating features before merging to main). This approach borrows elements from GitFlow (e.g., a long-lived develop branch) without adopting its full branching model.\n\n\nA common workflow in a collaborative development project.\n\nCreate a feature branch: Start by creating a new branch of the main branch (or develop if used). This branch should have a descriptive name to give an idea of the work that will be done, such as a new feature or a bug fix. This separation allows you to work independently without affecting the main codebase.\nMake changes and commit: Work on your branch, making the necessary changes to the code. Commit these changes with clear, descriptive messages.\nOpen a pull request: Once you have made your changes, open a pull request (PR). PRs allow for discussion, review, and additional changes if necessary.\nReview: Before merging, your changes might go through a review process where other team members can give feedback. Some projects may require a review before merging is allowed.\nMerge: Finally, once your changes are reviewed and tested, you can merge the pull request into main (or develop, if applicable). This incorporates your contributions into the project, making them part of the official codebase.\nDelete feature branch: Feature branches should be short-lived, thus avoiding potential conflicts due to the divergence of the code.\n\n\n\n\n\n\n\nSequence diagram of workflow\n\n\n\n\n\n\n\n\n\n\nsequenceDiagram\n    participant A as Author\n    participant R as Reviewer\n    A-&gt;&gt;A: Write some code in your branch or fork\n    A-&gt;&gt;R: Open a pull request\n    R-&gt;&gt;R: Add comments to a review\n    R-&gt;&gt;A: Submit a review\n    loop Until approved\n        A-&gt;&gt;R: Address or respond to review comments\n        R-&gt;&gt;A: Clarify or resolve comments\n    end\n    R-&gt;&gt;A: Approve pull request\n    A-&gt;&gt;A: Merge pull request\n    A-&gt;&gt;A: Delete branch\n\n\n\n\n\n\n\n\n\n\nForking\nExternal collaborators who do not have the same administrative rights to the repository can fork the project. They make their changes on their forked repository in a new feature branch. Steps 3-5 remain the same.\n\n\n\n\n\n\n Create ‚ÄúDraft‚Äù Pull Requests!\n\n\n\nWith Draft PR‚Äôs you:\n\nwant to signal that a pull request is just the start of the conversation and your code isn‚Äôt in any state to be judged.\nor have no intention of ever merging it, but you‚Äôd still like people to check it out locally and give you feedback.\nor opened a pull request without any code at all in order to get the discussion started.\n\n\n\n\n\nConflict resolution\nConflicts occur when two or more changes compete with each other, typically during a merge or rebase operation in Git. With pull requests, code reviews and testing you can catch potential conflicts before they are merged into the main codebase.\nConflicts usually come up and are resolved during a pull request:\n\nReview Conflicts: When a conflict is detected in a pull request, GitHub will alert you. Start by reviewing the conflicting files to understand the nature of the conflict.\nPull and Merge Locally: Fetch the latest changes from the main branch and attempt to merge them into your feature/develop branch locally. This will allow you to resolve conflicts on your local machine.\nResolve Conflicts: This might involve choosing one change over another or merging the changes manually.\nTest Changes: After resolving conflicts, thoroughly test your changes to ensure that the merged code works as expected.\nCommit and Push: Once conflicts are resolved and changes are tested, commit the resolved conflicts and push your changes back to the feature/develop branch on GitHub.\nComplete the Pull Request: After resolving conflicts and pushing your changes, review the pull request again to ensure everything is in order. If all checks pass and your team approves the changes, you can complete the merge into the main branch.\n\nEffective conflict resolution ensures that changes can be integrated smoothly and that the project remains on track. Conflicts cannot always be avoided, but they can be managed. Clear communication and adherence to established practices is the way to go.\n\n\n\n\n\n\n Learn more\n\n\n\n\nFor more information on Draft PRs - Introducing Draft Pull Requests.\nGitHub - Resolving a merge conflict\nCode Refinery - Lesson on conflict resolution",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Collaboration"
    ]
  },
  {
    "objectID": "docs/tud-support/index.html",
    "href": "docs/tud-support/index.html",
    "title": "Research Support Staff Guide",
    "section": "",
    "text": "üöß Under construction! üèóÔ∏è"
  },
  {
    "objectID": "docs/software/testing/test_types.html",
    "href": "docs/software/testing/test_types.html",
    "title": "Types of tests",
    "section": "",
    "text": "In writing tests for research software, we often differentiate between four types of tests: unit tests, integration tests, and end-to-end tests. In addition, regression tests are used to ensure that recent code changes haven‚Äôt adversely affected existing features or functionality.\n\n\n\nTesting pyramid ¬© 2023 SketchingDev\n\n\n\nUnit tests\nA unit test is a type of test where individual units or components of the software application are tested in isolation from the rest of the application. A unit can be a function, method, or class. The main purpose of unit testing is to validate that each unit of the software performs as designed.\n\n\nIntegration tests\nIntegration testing is a level of software testing where individual units are combined and tested as a group. The purpose is to verify that the units work together as expected and that the interfaces between them function correctly. Integration tests aims to expose defects in the interactions between integrated components.\n\n\nEnd-to-end tests\nEnd-to-end testing is focused on checking the entire system from start to finish, simulating real use cases. The goal is to verify the software functions as a whole from the user‚Äôs perspective.\n\n\nRegression tests\nRegression testing aims to verify that recent code changes haven‚Äôt adversely affected existing features or functionality. It involves re-running previously executed test cases to ensure that the software still behaves as expected after modifications. The primary purpose of regression testing is to catch unintended side effects of code changes and ensure that new features or bug fixes haven‚Äôt introduced regressions or broken existing functionality elsewhere in the code. Regression tests can include both unit tests and integration tests, as well as higher-level tests.\n\n\nDesigning a test case\nFor more complex integration, regression, or end-to-end tests, it can be useful to first describe the test case in words.\n\nDescription: Description of test case\nPreconditions: Conditions that need to be met before executing the test case\nTest Steps: To execute test cases, you need to perform some actions. Mention all the test steps in detail and the order of execution\nTest Data: If required, list data that needed to execute the test cases\nExpected Result: The result we expect once the test cases are executed\nPostcondition: Conditions that need to be achieved when the test case was successfully executed",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Test types"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html",
    "href": "docs/software/testing/r_test.html",
    "title": "Testing in R",
    "section": "",
    "text": "R offers several testing frameworks, but the most widely used is the testthat package. It provides a straightforward way to write unit tests for your R code. The package is designed to be easy to use, even for those who are new to testing.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#setting-up-testthat",
    "href": "docs/software/testing/r_test.html#setting-up-testthat",
    "title": "Testing in R",
    "section": "Setting up testthat",
    "text": "Setting up testthat\nTo get started with testthat, you need to install the package and set up a testing structure in your R project. You need to have a tests directory in your project root. Inside this directory, you need a subdirectory called testthat. You can create this with:\n# Install packages if not already installed, and load them\ninstall.packages(c(\"devtools\", \"testthat\", \"usethis\", \"covr\"))\n# and then run\nusethis::use_testthat()\nIt will set up the tests/testthat directory and a testthat.R file. This file will contain the setup code for your tests. It will also add the testthat package to your package‚Äôs DESCRIPTION file under the ‚ÄòSuggests‚Äô field.\n\n\n\n\n\n\n Tip\n\n\n\nPlease visit our Project structure guide for a recommendation on how to structure your R project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#writing-tests-with-testthat",
    "href": "docs/software/testing/r_test.html#writing-tests-with-testthat",
    "title": "Testing in R",
    "section": "Writing tests with testthat",
    "text": "Writing tests with testthat\nYour files in the tests/testthat directory should be named test-*.R, where * is a descriptive name for the test. The testthat.R file will source all the test files in the tests/testthat directory, so you do not need to load them manually. They must match the same structure as the functions under your /R directory. For example, if you have a function my_function() in R/my_function.R, you should create a test file tests/testthat/test-my_function.R to test it.\nSo let‚Äôs say we have a trivial function count_tu_delft_faculties() in your project that counts the number of faculties at TU Delft. TU Delft has 8 faculties, so the function should return 8. You would create a file R/count_tu_delft_faculties.R with the following content:\n#' Count the number of faculties at TU Delft\n#'\n#' @return The number of faculties at TU Delft\n#' @export\ncount_tu_delft_faculties &lt;- function() {\n    # This is a placeholder function\n    return(8)\n}\nThis function is a simple placeholder that returns the number of faculties at TU Delft. In a real-world scenario, this function would contain logic to count the faculties dynamically, perhaps by querying a database or an API.\nThen, you would create a test file tests/testthat/test-count_tu_delft_faculties.R to test this function. The test file should contain tests that check if the function returns the expected value (8 in this case).\ntest_that(\"description of the test\", {\n  # Call the function\n  result &lt;- count_tu_delft_faculties()\n  \n  # Test that it returns the expected value\n  expect_identical(result, 8)\n  \n  # We could add more expectations\n  expect_type(result, \"double\")\n  # etc.\n})\nYou can use various expect_* functions to check the results of your code.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#running-tests",
    "href": "docs/software/testing/r_test.html#running-tests",
    "title": "Testing in R",
    "section": "Running tests",
    "text": "Running tests\nYou should run your tests regularly to ensure that your code is working as expected. You can run all the tests in your package using:\ndevtools::test()\nThis will run all the tests in the tests/testthat directory and report any failures or errors. You can also run individual test files by specifying the file name:\ndevtools::test_file(\"tests/testthat/test-count_tu_delft_faculties.R\")\nThis will run only the tests in the specified file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/r_test.html#test-coverage",
    "href": "docs/software/testing/r_test.html#test-coverage",
    "title": "Testing in R",
    "section": "Test coverage",
    "text": "Test coverage\nTo check the test coverage of your package, you can use devtools::test_coverage(). This will generate a report showing which part of your code is covered by tests and which are not.\nYou can also use the covr package to have a more detailed control over coverage reporting. To check the coverage of your package:\ncovr::package_coverage()\nYou can also use covr::report() to generate an HTML report of the coverage.\n\n\n\n\n\n\n Learn more\n\n\n\n\nR packages book - Testing\nIntroduction to testing R code\ntestthat package\ncovr package",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in R"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html",
    "href": "docs/software/testing/matlab.html",
    "title": "Testing in MATLAB",
    "section": "",
    "text": "This guide explains how to write and execute tests in MATLAB. For additional details, refer to the official MATLAB documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html#writing-tests",
    "href": "docs/software/testing/matlab.html#writing-tests",
    "title": "Testing in MATLAB",
    "section": "Writing tests",
    "text": "Writing tests\nMATLAB supports script-based, function-based, and class-based unit tests, allowing for a range of testing strategies from simple to advanced use cases. See the MATLAB documentation for more information:\n\nMatlab - Ways to write unit tests\n\nScript-based testing\nFunction-based testing\nClass-based testing\n\n\n\nConvention for writing tests\n\nIt is recommened place tests in a separate folder, typically named tests/.\nPrefix test files with ‚Äútest‚Äù followed by the file that is tested. For example, a test for the file DrawRandomNumber.m should be called TestDrawRandomNumber.m. Matlab will recognize any scripts that are prefixed or suffixed with the string ‚Äútest‚Äù as tests.\n\n\n\nClass-based unit tests\nBecause of the limited features of the script- and function-based testing, this guide will discuss class-based testing. Class-based tests give you access to shared test fixtures, test parameterization, and grouping tests into categories. Check out our additional testing concepts for more information about these concepts.\n\n\n\n\n\n\n Introduction to class-based testing\n\n\n\nCheck out this short MATLAB video on writing class-based tests.\n\n\nYou can find an example below with the matlab syntax for writing Class-based unit tests:\nclassdef TestSumNumbers &lt; matlab.unittest.TestCase\n    methods (Test)\n        function testSumNumbers(testCase)\n            result = sumNumbers(2, 3);            \n            testCase.verifyEqual(result, 5)\n        end\n    end\nend\n Check out the MATLAB documentation for an introductory example: Write Simple Test Case Using Classes\n\n\n\n\n\n\nAnnotated class-based unit test example\n\n\n\n\n\n% Test classes are created by inheriting (&lt; symbol) the  Matlab Testing \n% framework.\n%\n% e.g. classdef nameOfTest &lt; matlab.unittest.TestCase\n%      end\n\nclassdef (TestTags = {'Unit'}) test_example &lt; matlab.unittest.TestCase \n%                              It's convention to name the test file \n%                              test_\"filename being tested\".m\n%\n%         TestTags are an optional feature that are useful for identifying \n%         what kind of test you're coding, as you might only want to run \n%         certain tests that are related.\n\n    properties \n        % Class properties are not required, but are useful to contain \n        % common parameters between tests.\n    end\n    \n    methods (TestClassSetup) \n        % TestClassSetup methods are not required, but are usually used to\n        % setup common testing variables, or loading data. These methods\n        % are executed *prior to* the (Test) methods.\n    end\n    \n    methods (TestClassTeardown) \n        % TestClassTeardown methods are not required, but are useful to\n        % delete any files created during the test execution. These methods\n        % are executed *after* the (Test) methods.\n    end\n    \n    methods (Test) % Each test is it's own method function, and takes \n                   % testCase as their only argument.\n\n        function test_sumNumbers_returns_expected_value_for_integer_case(testCase) \n        % Use very descriptive test method names - this helps for debugging\n        % when error occurs.\n                        \n            % Call the function you'd like to test, e.g:\n%             actualValue = sumNumbers(2,2); % Test example integer case, 2+2\n            % Since the function sumNumbers is not defined, the test will\n            % fail. Instead, we will define the actual value.\n            actualValue = 4;\n\n\n            expectedValue = 4; % We know that we expect that 2+2 = 4\n\n            testCase.assertEqual(expectedValue, actualValue)\n            % Assert functions are the core of unit tests; if it fails,\n            % test log will return failed tests and details.\n            %\n            % They are called as methods of the testCase object.\n            %\n            % Example assert methods:\n            %\n            % assertEqual(expected, actual): Passes if the two input values\n            %                                are equal.\n            % assertTrue(boolValue): Passes if the value or statement is \n            %                        true (e.g. 2&gt;1)\n            % assertFalse(boolValue): Passes if the value or statement is\n            %                         false (e.g. 1==0)\n            %\n            % See Matlab's documentation for more assert methods: \n            % https://www.mathworks.com/help/matlab/ref/matlab.unittest.qualifications.assertable-class.html\n        end\n    end\n\nend",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/matlab.html#executing-tests",
    "href": "docs/software/testing/matlab.html#executing-tests",
    "title": "Testing in MATLAB",
    "section": "Executing tests",
    "text": "Executing tests\n\n1. Running tests in the MATLAB Command Window\nYou can run tests through the MATLAB Command Window, by executing the following command in the root of your repository:\nresults = runtests(pwd, \"IncludeSubfolders\", true);\n\n% The argument `pwd` specifies the current working directory\n% `IncludeSubfolders` specifies whether to include subfolders in the search for tests\nMATLAB will automatically find all tests. If you make use of tags to categorize tests, you can run specific tags with:\nresults = runtests(pwd, \"IncludeSubfolders\", true, \"Tag\", '&lt;tag-name&gt;');\n For more details: runtests() documentation\n\nCustom testsuite script\nWe have a custom script available to run tests in a more structured way. It can be useful to:\n\nrun tests with specific tags\nignore specific tests\ngenerate various test reports\n\nWhen placed in the folder tests/, the script can be executed by running the following command in the MATLAB Command Window:\nrun_testsuite('TestTag', 'Unit')\n\n\n\n2. Script editor\nYou can run tests interactively by opening a test file in the MATLAB Editor and selecting ‚ÄúRun All‚Äù or ‚ÄúRun Current Test‚Äù.\n\n\n\nMathWorks, Script editor testing, MATLAB Documentation, link to image.\n\n\n For more details: Script Editor documentation\n\n\n3. MATLAB Test Browser App\nThe Test Browser app (available since R2023a) enables you to run script-based, function-based, and class-based tests interactively. You can run all tests, selected tests, or individual tests.\n\n\n\nMathWorks, MATLAB test browser, MATLAB Documentation, link to image.\n\n\n For more details: MATLAB Test browser documentation\n\n\nSimulink testing\nFor Simulink models, MATLAB provides Simulink Test for simulation-based testing.\n\nSimulink Test - Introduction video\nSimulink Test - Examples",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in MATLAB"
    ]
  },
  {
    "objectID": "docs/software/testing/index.html",
    "href": "docs/software/testing/index.html",
    "title": "Software testing",
    "section": "",
    "text": "When you‚Äôre writing software ‚Äì especially for research ‚Äì it‚Äôs important to make sure your programs work as expected. Testing is like a safety net: it helps you catch mistakes early and keeps your code reliable as you add new features or make changes. In research software, tests are even more crucial because they help ensure that your results are accurate and that others can reproduce your work. Beyond detecting bugs early, it is an investment in the quality and long-term maintainability of your codebase.\n\n\n\n Approach to testing\nA guide to help you get started with testing your software.\n\nLearn more ¬ª\n\n\n\n Test types\nDifferent types of tests to ensure your software works as expected.\n\nLearn more ¬ª\n\n\n\n Additional concepts\nMore concepts to help you write better tests.\n\nLearn more ¬ª\n\n\n\n Python testing\nTesting your Python code.\n\nLearn more ¬ª\n\n\n\n MATLAB testing\nTesting your MATLAB code.\n\nLearn more ¬ª\n\n\n\n R testing\nTesting your R code.\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\nRecommended courses\n\n\n\n\nThe Turing Way - Testing\nCodeRefinery - Lesson on testing\nSoftware Carpentry - Testing and Continuous Integration with Python\nR Packages - Testing",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html",
    "href": "docs/software/releases_archiving/releases/releases_cran.html",
    "title": "Release your R package",
    "section": "",
    "text": "After building and testing your R package, you can release it to CRAN. This process involves several steps to ensure that your package is ready for distribution. Unlike PyPI for Python, CRAN has specific requirements and guidelines that must be followed.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html#preparing-for-a-cran-submission",
    "href": "docs/software/releases_archiving/releases/releases_cran.html#preparing-for-a-cran-submission",
    "title": "Release your R package",
    "section": "Preparing for a CRAN submission",
    "text": "Preparing for a CRAN submission\nEnsure that your package is well-documented, tested, and follows CRAN policies. This includes having a DESCRIPTION file with all necessary fields filled out, a NAMESPACE file, and proper documentation for all functions. We already mentioned the use of devtools::check() and devtools::build() in the R packaging guide. These functions will help you prepare your package for submission. The package should pass all tests and checks without errors or warnings. The tarball created by devtools::build() will be used for submission.\nAdditionally, you would want to run devtools::check_rhub() to check your package on different platforms and R versions. The difference between devtools::check() and devtools::check_rhub() is that the former checks your package on your local machine, while the latter checks it on a remote server with different configurations.\nThis will help you identify any potential issues that may arise when users try to install your package on different systems. It can check your package on various platforms, including Windows, macOS, and different Linux distributions. It will also check for common issues that may arise when users try to install your package on different systems. While R-hub includes Windows builders, many developers specifically use devtools::check_win_devel() and devtools::check_win_release() before CRAN submission, as Windows compatibility issues are common.\nReview the CRAN policies and guidelines to ensure your package complies with their requirements. You can find the policies here.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_cran.html#submitting-to-cran",
    "href": "docs/software/releases_archiving/releases/releases_cran.html#submitting-to-cran",
    "title": "Release your R package",
    "section": "Submitting to CRAN",
    "text": "Submitting to CRAN\nEnsure that your package size is within the limits set by CRAN. You can check the size of your package using file.info() on the tarball created in the previous step. Then:\n\nCreate a CRAN account: If you don‚Äôt already have one, create an account on the CRAN website.\nSubmit your package: Use devtools::submit_cran() to submit your package to CRAN. This function will prompt you to enter your CRAN username and password, and it will automatically upload your package tarball to CRAN.\nFill out the submission form: After submitting your package, you will be directed to a web form where you can provide additional information about your package, such as a description, title, and any other relevant details.\nWait for CRAN review: After submitting your package, it will be reviewed by CRAN. This is an important distinction from PyPI, where packages are typically available immediately after submission. The CRAN team will check your package for compliance with their policies and guidelines.\n\nIf your package passes the review, it will be published on CRAN.\nIf there are issues, they will contact you for additional information or to request changes to your package.\n\nRespond to feedback: If the CRAN team requests changes or has questions about your package, make any necessary adjustments.\nRelease your package: Once your package has been approved, it will be available on CRAN for others to install and use.\n\n\nAdditional resources\nThe R packages book also has a detailed section on the CRAN submission process, including how to handle feedback from CRAN maintainers and how to respond to requests for changes.\n\n\n\n\n\n\n Learn more\n\n\n\n\nR packages book - CRAN submission\nCRAN policies\nCRAN submission process",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your R package on CRAN"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html",
    "title": "Creating an R package",
    "section": "",
    "text": "In this guide we will briefly touch the necessary steps to create an R package. For a more detailed guide, please refer to the excellent R packages book written by Hadley Wickham and Jennifer Bryan. The book is available online for free and is licensed under the CC BY-NC-ND 4.0 license. The book covers everything from the basics of package creation to advanced topics. It provides clear explanations, practical examples, and helpful tips to guide you through the process of building high-quality R packages.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#prerequisites",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#prerequisites",
    "title": "Creating an R package",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou will need the following packages installed to develop R packages:\ninstall.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"usethis\"))\n\n\n\n\n\n\n Tip\n\n\n\nPlease refer to our R code documentation guide on how to document your R code. The guide covers the use of roxygen2 for generating documentation, which is a key part of creating an R package.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#creating-the-package-structure",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#creating-the-package-structure",
    "title": "Creating an R package",
    "section": "Creating the package structure",
    "text": "Creating the package structure\nTo create an R package, we can use the devtools and usethis packages which include a variety of tools aimed at package development. The devtools package provides a set of functions that simplify the process of package development in R. The usethis package is another option for creating an R package structure. It provides a set of functions that help you create and manage R packages, including functions for creating package skeletons, adding dependencies, and managing package metadata. The two packages are often used together. For example, you can use usethis to create a package structure and then use devtools to add functions, tests, and documentation.\nEither way, you can create a package structure using the following command:\n# Using devtools\ndevtools::create(\"your_package_name\")\n# Using usethis\nusethis::create_package(\"your_package_name\")\nAlternatively, you can create a package structure using the RStudio IDE. This is a more user-friendly option for those who prefer a graphical interface. To create a package structure in RStudio, follow these steps:\n\nOpen RStudio and create a new project.\nSelect ‚ÄúNew Directory‚Äù and then ‚ÄúR Package‚Äù.\nChoose a name for your package and select the location where you want to create it.\nClick ‚ÄúCreate Project‚Äù to generate the package structure.\n\nEvery option will create a basic folder structure with essential files including DESCRIPTION, NAMESPACE, and R/ directory. The DESCRIPTION file contains metadata about your package, such as its name, version, author, and dependencies. The NAMESPACE file defines the functions and objects that will be exported from your package. The R/ directory is where you will write your package functions.\n\n\n\n\n\n\n Tip\n\n\n\nPlease visit our Project structure guide for a recommendation on how to structure your R project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#adding-functions-and-documentation",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#adding-functions-and-documentation",
    "title": "Creating an R package",
    "section": "Adding functions and documentation",
    "text": "Adding functions and documentation\nOnce you have the package structure for your R project, you will need to:\n\nEdit the DESCRIPTION file and fill in the package metadata, including the package name, version, author, licensing information and dependencies.\nAll your package functions should be placed in the R/ directory. You can create multiple R scripts in this directory to organize your functions logically.\nUse roxygen2 and devtools::document() to generate the documentation for your package based on the comments in your code. This will create .Rd files in the man/ directory.\nThe NAMESPACE file should be generated automatically by roxygen2 based on @export tags.\nUse devtools::build_vignettes() to build the package vignettes. This will create HTML files in the vignettes/ directory.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#testing-and-building-the-package",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#testing-and-building-the-package",
    "title": "Creating an R package",
    "section": "Testing and building the package",
    "text": "Testing and building the package\nOnce you have added your functions and documentation, you can test and build your package using the following steps:\n\nUse devtools::test() to run your package tests. This will run any tests you have written in the tests/ directory.\nTo load and test your package, use devtools::load_all(). This will load your package into the R session, allowing you to test it interactively.\nUse devtools::check() to check your package for any issues or errors. This will run a series of checks on your package and report any problems.\nUse devtools::build() to create a tarball of your package. This will create a .tar.gz file in the parent directory of your package.\nUse devtools::install() to install your package locally for testing. This will install the package from the tarball you just created.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nR packages book\nusethis package\ndevtools package",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_r.html#next-steps",
    "href": "docs/software/releases_archiving/packaging/packaging_r.html#next-steps",
    "title": "Creating an R package",
    "section": "Next steps",
    "text": "Next steps\nThe next step would be to publish your package. Visit our Release your R package guide for information on how to publish your package to CRAN.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create an R package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging.html",
    "href": "docs/software/releases_archiving/packaging/packaging.html",
    "title": "Packaging",
    "section": "",
    "text": "Packaging allows developers to bundle their code into a format that is easily installable and manageable across different environments. By creating a package/library, you ensure that all necessary components, including dependencies and configuration files, are included in a single unit, simplifying the installation process for users. Here you can find how to package your Python or R projects for distribution.\n\n\n\n Create a Python package\nBundle your Python project.\n\nLearn more ¬ª\n\n\n\n Create an R package\nBundle your R project.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html",
    "href": "docs/software/releases_archiving/archiving.html",
    "title": "Archiving",
    "section": "",
    "text": "Archiving your software ensures long-term availability and accessibility. By archiving your software, you can preserve its state at a specific point in time, making it easier for others to access and use it in the future.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html#zenodo",
    "href": "docs/software/releases_archiving/archiving.html#zenodo",
    "title": "Archiving",
    "section": "Zenodo",
    "text": "Zenodo\nZenodo supports the archiving of research outputs, including software releases. Zenodo can automatically archive releases from GitHub repositories and assign a DOI, making each version citable.\nTo use Zenodo with GitHub:\n\nLink your GitHub account to Zenodo to allow access to repository information.\nEnable the repository you want to archive on the Zenodo dashboard.\nCreate a new release on GitHub. Zenodo will automatically archive this release and issue a DOI.\nYou can then share the DOI link provided by Zenodo in your project‚Äôs README or documentation, or paper, so a specific version of your software can be referenced.\n\n\n\n\n\n\n\n Tips\n\n\n\n\nZenodo can only access public repositories.\nIf you need to archive a repository from an organization, the owner of the organization might have to authorize the Zenodo application to access it.\nYou can also try out Zenodo Sandbox before archiving your projects to Zenodo. Zenodo Sandbox mimics the main Zenodo platform and is designed to test out the functionality of Zenodo without accidentally making mistakes with the real software/data and the main site. Since it‚Äôs an exact mirror of Zenodo it provides the same user experience and all the same tools and the same interface.\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nReferencing and citing content",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/archiving.html#tu.researchdata",
    "href": "docs/software/releases_archiving/archiving.html#tu.researchdata",
    "title": "Archiving",
    "section": "4TU.ResearchData",
    "text": "4TU.ResearchData\n4TU.ResearchData is another platform that offers reliable archiving of research data and software. Their servers are located in the Netherlands, and they are committed to long-term preservation. 4TU.ResearchData offers at least 15 years of archival storage.\nTo get started:\n\nLog in to your 4TU.ResearchData account (using institutional access).\nFrom the dashboard navigate to upload a new project.\nEither choose open access or you also have the option to choose embargoed or restricted access.\nUpload your relevant files. 4TU.ResearchData supports Git for version control. Either just drag and drop datasets, or to deposit software you can push your Git repository to the 4TU remote. Add the remote:\n\ngit remote add 4tu [link automatically generated by 4TU] Then, push your repository:\ngit push 4tu --all\ngit push 4tu --tags\n\nYou will have a DOI number reserved, and versioning is also supported.\nMaintain and update your archive as necessary to reflect any significant changes or additions to the software.\n\n\n\n\n\n\n\n Information\n\n\n\n\nWhen you are ready to publish your software on 4TU.ResearchData, ensure to choose the ‚ÄúSoftware deposit‚Äù option in the ‚ÄòFiles‚Äô section at the bottom of the upload form. This option allows you to upload your software files directly from your Git repository. If you have additional files, you can also manually drag your software files from your local drive into the upload box.\n4TU.ResearchData also has a sandbox available.\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nMore information on 4TU.ResearchData can be found in their FAQ.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Archiving"
    ]
  },
  {
    "objectID": "docs/software/fair_software/software_management_plan.html",
    "href": "docs/software/fair_software/software_management_plan.html",
    "title": "Software management plan",
    "section": "",
    "text": "A software management plan (SMP) helps to implement best practices during software development and ensures that software is accessible and reusable in the short and longer term. The Netherlands eScience Center and NWO, the Dutch Research Council, have taken the initiative to develop (national) guidelines for software management plans, which resulted in the publication of the Practical guide to Software Management Plans in October 2022.\nAs an introduction to Software Management Plans, TU Delft has created a video:\n\n‚ÄúNavigating Research Data and Software: A Practical Guide for PhD Supervisors 2025‚Äù, TU Delft Library. CC-BY-4.0\n\n\n\n\n\n\n Tip\n\n\n\nThe Intermediate Research Software Development course from The Carpentries is a good resource to learn the core principle of intermediate-level software development and best practices collaborating in a team.\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nSoftware Development Plan: A quick start on building software development plans.\nSoftware Development Life Cycle: A guide on how to follow the software development lifecycle (SDLC) systematically, and different SDLC methodologies.\nWriting Software Management Plans: A guide from the Software Sustainability Institute on how to write research software management plans.\nSoftware Management Plans documentation: A detailed documentation about software management plans, including up-to-date templates.\nA Framework for Understanding Research Software: A proposal for categorizing different types of research software, and a framework for sustaining research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "Software management plan"
    ]
  },
  {
    "objectID": "docs/software/fair_software/checklist.html",
    "href": "docs/software/fair_software/checklist.html",
    "title": "FAIR checklist for research software",
    "section": "",
    "text": "This checklist provides a set of recommendations for FAIR software. It outlines best practices and guidelines to ensure the quality, reproducibility, and sustainability of software projects. The checklist covers various aspects, such as version control, documentation, testing, licensing, and collaboration, providing a comprehensive framework for improving your software development process.\nTo support implementation, each checklist section contains a tab with pre-formatted markdown templates (FAIR cards). These can be copied directly into your repository as issues to systematically track progress in adopting FAIR research software best practices.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "FAIR checklist for research software"
    ]
  },
  {
    "objectID": "docs/software/fair_software/checklist.html#checklist",
    "href": "docs/software/fair_software/checklist.html#checklist",
    "title": "FAIR checklist for research software",
    "section": "Checklist",
    "text": "Checklist\n\nVersion controlVersion control (.md format)\n\n\nEssential\n\nUse git as a version control system\nUpload your project on GitHub or TU Delft GitLab\n\nRecommended\n\nMake your repository public\nConsider your branch hygiene\nUse a branching model (e.g.¬†GitFlow)\nUse meaningful commit messages\n\n\n\n_Essential_\n- [ ] Use [git](https://www.atlassian.com/git) as a version control system \n- [ ] Upload your project on [GitHub](https://github.com/) or [TU Delft GitLab](https://gitlab.tudelft.nl/)\n\n_Recommended_  \n- [ ] Make your repository [public](https://coderefinery.github.io/social-coding/)\n- [ ] Consider your [branch hygiene](https://coderefinery.github.io/git-branch-design/)\n- [ ] Use a branching model (e.g. [GitFlow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow))\n- [ ] Use [meaningful commit messages](https://www.git-scm.com/book/en/v2/Distributed-Git-Contributing-to-a-Project#_commit_guidelines)\n\n\n\n\nCollaborationCollaboration (.md format)\n\n\nEssential\n\nMake use of GitHub issues\n\nRecommended\n\nContribution guidelines\nCode of conduct\n\n\n\n_Essential_  \n- [ ] Make use of [GitHub issues](https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues)\n\n_Recommended_\n- [ ] [Contribution guidelines](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/setting-guidelines-for-repository-contributors)\n- [ ] [Code of conduct](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-code-of-conduct-to-your-project)\n\n\n\n\nProject documentationProject documentation (.md format)\n\n\nEssential\n\nREADME\nApply a TU Delft pre-approved LICENSE\nCITATION\n\n\n\n_Essential_  \n- [ ] [README](https://www.makeareadme.com)\n- [ ] Apply a TU Delft pre-approved [LICENSE](https://zenodo.org/records/4629635)\n- [ ] [CITATION](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files)\n\n\n\n\nSoftware documentationSoftware documentation (.md format)\n\n\nEssential\n\nSource code documentation (docstrings)\nDocument your project dependencies\nInstallation instructions\nUser documentation\n\nRecommended\n\nDeveloper documentation and setup\nExamples and tutorials (e.g.¬†Jupyter Notebooks)\n\nOptional\n\nDocumentation tools (Sphinx, JupyterBook, Quarto)\nBuild an API reference from docstrings\nHosting (GitHub Pages, Readthedocs)\n\n\n\n_Essential_  \n- [ ] Source code documentation ([docstrings](https://numpydoc.readthedocs.io/en/latest/format.html))\n- [ ] Document your project dependencies\n- [ ] Installation instructions\n- [ ] User documentation\n\n_Recommended_  \n- [ ] Developer documentation and setup\n- [ ] Examples and tutorials (e.g. Jupyter Notebooks)\n\n_Optional_\n- [ ] Documentation tools ([Sphinx](https://coderefinery.github.io/documentation/sphinx/), [JupyterBook](https://jupyterbook.org/intro.html), [Quarto](https://quarto.org/docs/guide/))\n- [ ] Build an [API reference](https://developer.lsst.io/python/numpydoc.html) from docstrings\n- [ ] Hosting ([GitHub Pages](https://pages.github.com/), [Readthedocs](https://readthedocs.org/))\n\n\n\n\nSoftware testingSoftware testing (.md format)\n\n\nEssential\n\nInstallation/execution verification\n\nRecommended\n\nDefensive programming\nTest your software with integration tests and unit tests\nMake use of Continuous Integration to automate testing\n\nOptional\n\nCode coverage check (e.g.¬†Sonarcloud, codecov)\n\n\n\n_Essential_\n- [ ] Installation/execution verification\n\n_Recommended_\n- [ ] [Defensive programming](https://swcarpentry.github.io/python-novice-inflammation/10-defensive.html)\n- [ ] Test your software with [integration tests](https://the-turing-way.netlify.app/reproducible-research/testing/testing-integrationtest.html) and [unit tests](https://the-turing-way.netlify.app/reproducible-research/testing/testing-unittest.html)\n- [ ] Make use of [Continuous Integration](https://coderefinery.github.io/testing/continuous-integration/) to automate testing\n\n_Optional_\n- [ ] Code coverage check (e.g. [Sonarcloud](https://sonarcloud.io/), [codecov](https://about.codecov.io))\n\n\n\n\nSoftware qualitySoftware quality (.md format)\n\n\nEssential\n\nOrganize your project for reproducibility\nRecord and manage your software dependencies\n\nRecommended\n\nMake refactoring part of your workflow\nFollow best coding practices\n\nRecommended for Python\n\nFollow PEP8 guidelines\nUse a tool for dependency management (e.g.¬†poetry)\nUse linter (e.g.¬†pylint, flake8)\nUse a formatter (e.g.¬†black)\n\n\n\n_Essential_\n- [ ] [Organize](https://coderefinery.github.io/reproducible-research/organizing-projects/) your project for reproducibility\n- [ ] [Record and manage](https://coderefinery.github.io/reproducible-research/dependencies/) your software dependencies \n\n_Recommended_\n- [ ] Make [refactoring](https://refactoring.guru/refactoring) part of your workflow\n- [ ] Follow [best coding practices](https://alan-turing-institute.github.io/rse-course/html/module07_construction_and_design/index.html)\n\n_Recommended for Python_\n- [ ] Follow [PEP8 guidelines](https://realpython.com/python-pep8/)\n- [ ] Use a tool for dependency management (e.g. [poetry](https://python-poetry.org/docs/))\n- [ ] Use linter (e.g. [pylint](https://pypi.org/project/pylint/), [flake8](https://pypi.org/project/flake8/))\n- [ ] Use a formatter (e.g. [black](https://github.com/psf/black))\n\n\n\n\nReleasesReleases (.md format)\n\n\nEssential\n\nObtain a DOI (Zenodo or 4TU.ResearchData)\n\nRecommended\n\nUse semantic versioning\nCreate tagged releases (GitHub)\nCHANGELOG\nUpload to registry (e.g.¬†PyPI, conda)\nReleasing guide\n\nOptional\n\nContinuous Integration for automated build and release\n\n\n\n_Essential_  \n- [ ] Obtain a DOI ([Zenodo](https://zenodo.org/) or [4TU.ResearchData](https://data.4tu.nl/info/about-your-data/getting-started))\n\n_Recommended_  \n- [ ] Use [semantic versioning](https://semver.org/)\n- [ ] Create tagged releases ([GitHub](https://docs.github.com/en/repositories/releasing-projects-on-github))\n- [ ] [CHANGELOG](https://keepachangelog.com/en/1.0.0/)\n- [ ] Upload to [registry](https://github.com/NLeSC/awesome-research-software-registries) (e.g. [PyPI](https://realpython.com/pypi-publish-python-package/), [conda](https://conda.io/projects/conda-build/en/latest/user-guide/tutorials/build-pkgs.html))\n- [ ] [Releasing guide](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository)\n\n_Optional_\n- [ ] [Continuous Integration](https://the-turing-way.netlify.app/reproducible-research/ci/ci-options.html) for automated build and release\n\n\n\n\nExample repositories\n\neScience Center - matchms - Matchms is an open-source Python package to import, process, clean, and compare mass spectrometry data.\nTU Delft - Transposonmapper - Transposonmapper is an open-source python package and Docker image for mapping transposons from sequencing data.\n\n\nFor more information on the principles behind FAIR software, please have a look at the following resources:\n\nThe Turing Way - Guide for Reproducible Research - general guide to reproducible research\nTowards FAIR principles for research software - publication on the translation of FAIR principles for data to FAIR principles for software\nFrom FAIR research data toward FAIR and open research software\nFAIR Principles for Research Software\n\n\n\n\nAcknowledgements\nThe checklist was in part based on the checklist provided by the eScience Center, licensed under CC BY 4.0.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software",
      "FAIR checklist for research software"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html",
    "href": "docs/software/documentation/tooling.html",
    "title": "Tooling",
    "section": "",
    "text": "There are various tools available that can help you create, manage, and deploy project documentation more effectively.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#sphinx",
    "href": "docs/software/documentation/tooling.html#sphinx",
    "title": "Tooling",
    "section": "Sphinx",
    "text": "Sphinx\nSphinx is a versatile documentation tool that is well-suited for documenting Python projects due to its easy integration with Python‚Äôs docstrings. Its capabilities extend beyond Python, making it a great solution for creating comprehensive documentation for projects in various programming languages (e.g.¬†MATLAB).\nSome key features of Sphinx include:\n\nCross-referencing code and documentation across files.\nAutomatic generation of documentation from docstrings.\nSyntax highlighting for code examples.\nSupport for extensions and custom themes.\nMultiple output formats.\n\n\nGetting started with Sphinx\n\n\n\n\n\n\n Tip\n\n\n\nTo get started with Sphinx, we recommend the Coderefinery lesson on Sphinx and Markdown\n\n\n\nInstall dependency: You can install Sphinx in various ways, either through apt-get for Linux, Homebrew for macOS, or through Chocolatey for Windows. Assuming you have Python on your machine you can install it through conda or pip.\nSetup documentation: Create a directory for your documentation (/docs), and run sphinx-quickstart in that directory. The default answers to the questions are fine.\nConfigure Sphinx: Once you have the conf.py and index.rst files, you will need to modify them further. The index.rst file acts as the front page of your documentation and the root of the table of contents. The conf.py file is the main configuration file for the Sphinx documentation. It holds all your extensions and controls various aspects of the build process that can be customized to suit your needs. For example, sphinx.ext.autodoc is used for pulling documentation from docstrings, and sphinx.ext.mathjax for displaying mathematical content.\n\nBuilt-in extensions\nThird-party extensions\n\nWrite content: Add content to your documentation. In addition to reStructureText, Sphinx also integrates with markdown documentation through the MyST parser.\nBuild documentation: Once you have added the documentation files, you can build the documentation from the folder /docs with sphinx-build . _build/ or make html.\nFurther customization: You can customize the look of your documentation by changing themes in the conf.py file.\n\n\n\n\n\n\n\n Sphinx configuration template\n\n\n\n\n\n\n\n\n\nconfig.py\n\n\n\n\n\n# Configuration file for the Sphinx documentation builder\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, as shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \"Project\"\ncopyright = \"year, name\"\nauthor = \"name\"\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# The full version, including alpha/beta/rc tags\nrelease = \"0.1.0\"\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"myst_parser\", # MyST markdown parser\n    \"sphinxcontrib.matlab\", # Required for MATLAB\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx_copybutton\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_tabs.tabs\"\n]\n\nmyst_enable_extensions = [\n    \"linkify\",\n]\n\n# MATLAB settings for autodoc\n# here = os.path.dirname(os.path.abspath(__file__))\n# matlab_src_dir = os.path.abspath(os.path.join(here, \"..\"))\n# primary_domain = \"mat\"\n\n# Napoleon settings\nnapoleon_google_docstring = True\n# napoleon_numpy_docstring = True\n# napoleon_use_param = False\n# napoleon_preprocess_types = True\n\n# This value contains a list of modules to be mocked up.\n# autodoc_mock_imports = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\n# html_theme = \"sphinx_book_theme\"\nhtml_theme = \"sphinx_rtd_theme\"\n# html_theme = \"pydata_sphinx_theme\"\n\n\nhtml_title = \"title\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n\n When using Sphinx extensions or custom themes beyond core functionality, a requirements.txt file is mandatory for reproducible documentation builds across environments. For example, this is required for hosting platforms like Read the Docs.\n\n\n\n\n\n\nrequirements.txt\n\n\n\n\n\n# Core\nsphinx\n\n# Sphinx extensions\nsphinxcontrib-matlabdomain # Only for matlab source code\nsphinx-tabs\nsphinx-copybutton\n\n# MyST parser\nmyst-parser\nlinkify-it-py\n\n# Themes\npydata-sphinx-theme\nsphinx-book-theme\nsphinx-rtd-theme\n\n\n\n\n\n\n\n\n\n\n\n Example repositories using Sphinx for Python:\n\n\n\n\nPython‚Äôs official documentation is created using Sphinx\nRead The Docs - the platform for hosting documentation is itself documented using Sphinx.\nNumPy\n\n\n\n\n\nSphinx autodoc\nOnce the Sphinx config.py is set up, you can generate the API reference documentation by using the sphinx-autodoc extension. By creating .rst files with the autodoc syntax, Sphinx will build the API reference.\n\n\nSphinx-matlabdomain\nFor documenting MATLAB projects, Sphinx can be extended for MATLAB. The sphinxcontrib-matlabdomain extension allows Sphinx to interpret and render MATLAB specific documentation. The extension can be installed through pip install sphinxcontrib-matlabdomain and add the extension to the conf.py file.\n\n\n\n\n\n\n Example repositories using sphinx for MATLAB:\n\n\n\n\nENIGMA Toolbox - provides documentation in both Python and MATLAB, generated by Sphinx and hosted using Read the Docs.\nCobra Toolbox\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCoderefinery lesson on Sphinx and Markdown\nGetting started with Sphinx",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#jupyter-book",
    "href": "docs/software/documentation/tooling.html#jupyter-book",
    "title": "Tooling",
    "section": "Jupyter Book",
    "text": "Jupyter Book\nJupyter Book uses Sphinx to convert notebooks and Markdown documents into an interactive publishing framework (with executable content). It integrates Jupyter Notebooks with Sphinx‚Äôs documentation capabilities, enabling features like cell execution and output caching directly within the documentation. Jupyter Book is essentially a specialized wrapper around Sphinx and the MyST-NB extension, designed to make publishing content easier.\n\n\n\n\n\n\n The TU Delft OPEN Interactive Textbooks platform uses Jupyter Book to create textbooks.\n\n\n\n\nFeatures\n\nJupyter Book can integrate outputs by allowing code execution within the content, making it ideal for tutorials, courses, and technical documentation that require live examples.\nJupyter Book uses Markdown for Jupyter (MyST) which extends the traditional Markdown syntax to include features normally available in reStructuredText (reST). This makes it easier to include complex formatting and dynamic content directly in Markdown files.\nJupyter Book can execute notebook cells and cache outputs. This means that content including code outputs can be generated once and reused.\n\n\n\nGetting started\nJupyterBook has extensive documentation on getting started with building a book.\n\n\n\n\n\n\n Further reading\n\n\n\n\nHow Jupyter Book and Sphinx relate to one another",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#mkdocs",
    "href": "docs/software/documentation/tooling.html#mkdocs",
    "title": "Tooling",
    "section": "MkDocs",
    "text": "MkDocs\nMkDocs is a static site generator that uses markdown for all documentation, simplifying the writing process, and is configured with a single YAML file. It is lightweight compared to Sphinx but less feature-rich for complex use cases, and is best suitable for straightforward project documentation without heavy API generation needs.\n\nGetting started\n\nYou can install it through pip (pip install mkdocs). Then you can initialize your MkDocs project by running mkdocs new your_project_name.\nPlace your markdown documentation in your docs directory and define the structure in your mkdocs.yml file.\nYou can preview your site locally and see live updates as you make changes by running mkdocs serve.\nWhen you want to publish your documentation run mkdocs build.\nMkDocs is designed to be hosted on almost any static file server and works well with GitHub Pages.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nMkDocs official site that includes a Getting Started and User Guide.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#quarto",
    "href": "docs/software/documentation/tooling.html#quarto",
    "title": "Tooling",
    "section": "Quarto",
    "text": "Quarto\nSimilar to Jupyter Book, Quarto is a publishing framework that allows you to create dynamic documents, presentations, reports, websites, and more. It supports multiple programming languages, including Python, R, and Julia, enabling the inclusion of executable code, interactive visualizations, equations, and rich formatting directly within the documents.\n\n\n\n\n\n\n‚Æï All of these guides are created with Quarto!\n\n\n\n\nGetting started\n\nDownloading: You can download the installer for your operating system from the Quarto website.\nRunning Quarto: You can run Quarto either from your command line or from VS Code, JupyterLab, RStudio, or any text editor. For VS Code you will need to install a Quarto extension. It is a stand-alone application and does not require Python.\nMarkdown flavour: Quarto projects use .qmd files which are a Markdown flavour.\n\n\n\n\n\n\n\n Basic structure of a Quarto file\n\n\n\n\n\n    ---\n    title: \"Your Document Title\"\n    format: html # Or pdf, word, etc.\n    ---\n\n    # Introduction\n\n    Some text...\n\n    ## Section 1\n\n    Some text...\n\n    ```python\n    # This is a code block\n    import pandas as pd\n    data = pd.read_csv(\"data.csv\")\n    print(data.head())\n    ```\n\n    ## Section 2\n\n    Some more text....\n\n\n\n\n\nAdding content: Write your text using standard Markdown syntax and add code blocks.\nBuilding documentation:\n\nTo compile a Quarto document, use quarto render your-file-name.qmd. This command converts your .qmd file into the output format specified in the file‚Äôs header (e.g., HTML, PDF).\nYou can watch a file or directory for changes and automatically re-render with quarto preview your-file-name.qmd, which is useful to see live updates.\n\nAdditional features:\n\nQuarto supports cross-referencing figures, tables, and other elements within your document. You can also use BibTeX for citations.\nYou can have interactive components for web outputs (e.g.¬†embeded Plotly charts).\nExtensive options for custom styles and layouts.\n\nPublishing: Quarto documents are portable and can be shared as is, allowing others to compile them on their own systems or published by hosting the output files on a server like GitHub Pages.\n\n\n\n\n\n\n\n Examples\n\n\n\n\nQuarto gallery\n\n\n\n\n\n\n\n\n\n PDF engine\n\n\n\nIn order to create PDFs you will need to install a LaTeX engine if you do not have one installed already. You could use a lightweight distribution like TinyTeX, which you can install with quarto install tool tinytex.\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nGetting started with Quarto\nComprehensive guide to using Quarto\nCarpentries Incubator - Introduction to Working with Quarto documents",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/tooling.html#tools-for-r",
    "href": "docs/software/documentation/tooling.html#tools-for-r",
    "title": "Tooling",
    "section": "Tools for R",
    "text": "Tools for R\nR project documentation generally includes in-line comments and function documentation using roxygen2. Additionally, comprehensive examples and usage guides are often provided through vignettes, which are included within an R package itself.\nTo extend roxygen2 documentation into a static website for your package you can use pkgdown. pkgdown automatically generates a website from your package‚Äôs documentation and vignettes, similar to how Sphinx is used for Python projects.\n\n\n\n\n\n\n Learn more\n\n\n\n\npkgdown",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/documentation/index.html",
    "href": "docs/software/documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation serves as a bridge between the developer and user, and effectively communicating and explaining the code is as important as the code itself. Often, two types of documentation are distinguished - user and developer documentation. Both are essential for the success of a software project, and they serve different purposes.\n\nUser documentation\nUser documentation is aimed at those who will use the software. This documentation typically includes user manuals and tutorials, possibly FAQs and troubleshooting guides. The focus is on simplicity and accessibility, ensuring that anyone can understand how to use the software.\n\n\n\n\n README\nHow to write a good README.\n\nLearn more ¬ª\n\n\n\n Licenses\nApply an open-source license.\n\nLearn more ¬ª\n\n\n\n CITATION\nCite your software.\n\nLearn more ¬ª\n\n\n\n\n\nDeveloper documentation\nDeveloper documentation targets developers who need to understand the internal parts of the software for purposes of development, maintenance, or integration. It can include additional details such as API documentation and development guidelines. Developer documentation is more detailed providing insights necessary for modifying and enhancing the software.\n\n\n\n\n Code Documentation\nDocumenting your codebase.\n\nLearn more ¬ª\n\n\n\n Tooling\nDeploy your documentation.\n\nLearn more ¬ª\n\n\n\n Hosting\nHost your documentation.\n\nLearn more ¬ª\n\n\n\n Contributing Guidelines\nDefine how to contribute to your project.\n\nLearn more ¬ª\n\n\n\n Code of Conduct\nSet expectations for respectful, inclusive collaboration among contributors.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/software/documentation/contributing_guidelines.html",
    "href": "docs/software/documentation/contributing_guidelines.html",
    "title": "Contributing guidelines",
    "section": "",
    "text": "A well-maintained README provides an overview of your project‚Äôs current state, while a CONTRIBUTING guide encourages user/developer involvement. Together, these documents help maintain project clarity and make it easier to manage contributions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Contributing guidelines"
    ]
  },
  {
    "objectID": "docs/software/documentation/contributing_guidelines.html#setting-up-a-contributing-guide",
    "href": "docs/software/documentation/contributing_guidelines.html#setting-up-a-contributing-guide",
    "title": "Contributing guidelines",
    "section": "Setting up a CONTRIBUTING guide",
    "text": "Setting up a CONTRIBUTING guide\nThere are no strict guidelines for a contributing guide and the content will depend on the project size, the number of collaborators, and your particular workflow. Consider including:\n\nIntroduction: Welcome the contributors and express appreciation for community contributions.\nAdd a code of conduct: This helps to maintain a respectful and inclusive environment.\nHow to contribute: Explain precise contribution guidelines\n\nIssue tracking: Explain how to report issues (bugs, feature requests, etc.).\nPull requests: Detail the process for submitting pull requests. This includes instructions on forking the repository, creating a branch, making changes, and the follow-up steps for a successful pull request.\nCode review process: Describe how contributions will be reviewed and integrated.\n\nCommunity and communication: List the channels through which contributors can communicate and set their expectations regarding the responsiveness and availability of project maintainers.\nStyle guide and coding standards: Providing a (separate) coding style guide or documenting coding standards would be best practice. This way contributors would ensure consistency across the codebase.\nLegal implications: Inform contributors about the licensing under which their contributions will be used and any intellectual property considerations they should be aware of.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nGitHub‚Äôs guide to setting guidelines for repository contributors\nGitHub‚Äôs own CONTRIBUTING guide",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Contributing guidelines"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html",
    "href": "docs/software/documentation/code_documentation/r_documentation.html",
    "title": "R documentation",
    "section": "",
    "text": "The standard approach for documenting R projects is using roxygen2, a package that enables inline documentation for R scripts and packages. It is tightly integrated with the R ecosystem, making it straightforward to generate user-friendly documentation in the form of help files (.Rd files). roxygen2 allows you to write documentation alongside your code as specially formatted comments, which are then parsed and converted into the appropriate documentation format as .Rd files.\nKey features of roxygen2:\n\nInline documentation: Document functions, arguments, and return values directly in the script.\nIntegration with R‚Äôs help option: Generates .Rd files, which are then converted into the help pages users can access with ?function_name.\nCross-referencing: You can easily link to other functions within the documentation.\nNamespace management: Automatically manages the NAMESPACE file for proper imports and exports. A NAMESPACE file is typical for CRAN submissions.\n\n\n\n\nInstall with install.packages(\"roxygen2\").\nEnsure you are in the correct project directory and it has the standard R package structure.\nWriting documentation: Documentation is written as comments starting with #' directly above your function definitions. roxygen2 processes this and stores .Rd files in the man/ directory. Common tags include:\n\n@param: Describes function arguments.\n@return: Describes the return value.\n@examples: Provides example usage.\n@import: For importing functions from other packages.\n@inheritParams: To inherit parameter descriptions from another documented function.\n@export: Makes the function available to package users.\n@seealso : For cross-referencing.\n\nGenerating documentation: Run roxygenise() to convert your documentation into .Rd files. You can also use devtools::document()since it is a wrapper around roxygenise().\nWhen packaging your R project, you can use devtools::check() to ensure your documentation is consistent with your function definitions.\n\n\n\n\n\n\n\n roxygen2 example\n\n\n\n\n\n#' Summarize a numeric vector\n#'\n#' This function calculates the mean, median, and standard deviation of a given \n#' numeric vector and returns the results in a data frame.\n#'\n#' @param x A numeric vector.\n#' @return A data frame with the following columns:\n#' \\describe{\n#'   \\item{mean}{The mean of the numeric vector.}\n#'   \\item{median}{The median of the numeric vector.}\n#'   \\item{sd}{The standard deviation of the numeric vector.}\n#' }\n#' @details This function provides a quick summary for a numeric vector, returning \n#' measures of central tendency (mean and median) and a measure of \n#' dispersion (standard deviation) using R‚Äôs base functions mean(), median(), and sd().\n#' @examples\n#' # Basic example\n#' summarize_vector(c(1, 2, 3, 4, 5))\n#' \n#' @export\nsummarize_vector &lt;- function(x) {\n  if (!is.numeric(x)) stop(\"Input must be a numeric vector.\")\n  \n  mean_val &lt;- mean(x)\n  median_val &lt;- median(x)\n  sd_val &lt;- sd(x)\n  \n  return(data.frame(mean = mean_val, median = median_val, sd = sd_val))\n}\n\n\n\n\n\n\n\n\n\n Example repositories using roxygen2:\n\n\n\n\nggplot2 on GitHub\ndplyr on GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html#roxygen2",
    "href": "docs/software/documentation/code_documentation/r_documentation.html#roxygen2",
    "title": "R documentation",
    "section": "",
    "text": "The standard approach for documenting R projects is using roxygen2, a package that enables inline documentation for R scripts and packages. It is tightly integrated with the R ecosystem, making it straightforward to generate user-friendly documentation in the form of help files (.Rd files). roxygen2 allows you to write documentation alongside your code as specially formatted comments, which are then parsed and converted into the appropriate documentation format as .Rd files.\nKey features of roxygen2:\n\nInline documentation: Document functions, arguments, and return values directly in the script.\nIntegration with R‚Äôs help option: Generates .Rd files, which are then converted into the help pages users can access with ?function_name.\nCross-referencing: You can easily link to other functions within the documentation.\nNamespace management: Automatically manages the NAMESPACE file for proper imports and exports. A NAMESPACE file is typical for CRAN submissions.\n\n\n\n\nInstall with install.packages(\"roxygen2\").\nEnsure you are in the correct project directory and it has the standard R package structure.\nWriting documentation: Documentation is written as comments starting with #' directly above your function definitions. roxygen2 processes this and stores .Rd files in the man/ directory. Common tags include:\n\n@param: Describes function arguments.\n@return: Describes the return value.\n@examples: Provides example usage.\n@import: For importing functions from other packages.\n@inheritParams: To inherit parameter descriptions from another documented function.\n@export: Makes the function available to package users.\n@seealso : For cross-referencing.\n\nGenerating documentation: Run roxygenise() to convert your documentation into .Rd files. You can also use devtools::document()since it is a wrapper around roxygenise().\nWhen packaging your R project, you can use devtools::check() to ensure your documentation is consistent with your function definitions.\n\n\n\n\n\n\n\n roxygen2 example\n\n\n\n\n\n#' Summarize a numeric vector\n#'\n#' This function calculates the mean, median, and standard deviation of a given \n#' numeric vector and returns the results in a data frame.\n#'\n#' @param x A numeric vector.\n#' @return A data frame with the following columns:\n#' \\describe{\n#'   \\item{mean}{The mean of the numeric vector.}\n#'   \\item{median}{The median of the numeric vector.}\n#'   \\item{sd}{The standard deviation of the numeric vector.}\n#' }\n#' @details This function provides a quick summary for a numeric vector, returning \n#' measures of central tendency (mean and median) and a measure of \n#' dispersion (standard deviation) using R‚Äôs base functions mean(), median(), and sd().\n#' @examples\n#' # Basic example\n#' summarize_vector(c(1, 2, 3, 4, 5))\n#' \n#' @export\nsummarize_vector &lt;- function(x) {\n  if (!is.numeric(x)) stop(\"Input must be a numeric vector.\")\n  \n  mean_val &lt;- mean(x)\n  median_val &lt;- median(x)\n  sd_val &lt;- sd(x)\n  \n  return(data.frame(mean = mean_val, median = median_val, sd = sd_val))\n}\n\n\n\n\n\n\n\n\n\n Example repositories using roxygen2:\n\n\n\n\nggplot2 on GitHub\ndplyr on GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/r_documentation.html#vignettes",
    "href": "docs/software/documentation/code_documentation/r_documentation.html#vignettes",
    "title": "R documentation",
    "section": "Vignettes",
    "text": "Vignettes\nVignettes are detailed guides or tutorials included with R packages, providing users with in-depth explanations and examples, complementing the shorter help files. They are useful for demonstrating package functionality and use cases. Vignettes are automatically compiled and included in the package documentation.\nYou can create a vignette by running:\nusethis::use_vignette(\"your_vignette_name\")\nThis creates a template in the vignettes/ directory with the necessary YAML header and structure. You can combine text, code chunks, and outputs using standard R Markdown syntax. Include explanations, usage examples, and visualizations to enhance clarity.\nOnce written, build the vignette using:\ndevtools::build_vignettes()\nThis generates HTML or PDF versions of the vignette, which are then included in the package documentation. Use devtools::install(build_vignettes = TRUE) to test your package and built vignettes together (see how a user would experience them).\n\n\n\n\n\n\n Learn More\n\n\n\n\nExplore vignette(\"roxygen2\") in R\nroxygen2 documentation\nFunction documentation from R Packages\nWriting vignettes\nCRAN Submission guidelines",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "R projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/matlab_documentation.html",
    "href": "docs/software/documentation/code_documentation/matlab_documentation.html",
    "title": "MATLAB documentation",
    "section": "",
    "text": "üöß Coming soon! ‚è≥",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "MATLAB projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/citation.html",
    "href": "docs/software/documentation/citation.html",
    "title": "CITATION.cff",
    "section": "",
    "text": "It‚Äôs straightforward to cite research papers, but with software sometimes it‚Äôs not as obvious. It is recommended to place a CITATION.cff file in the root of your repository to inform others about the preferred way to cite the software. GitHub can automatically parse the .cff file to create citation snippets in APA or BibTeX format. If you‚Äôd prefer the software to be cited through a journal publication, you can mention this in the README and in the CITATION.cff file.\n\n\n\n\n\n\n An example of a CITATION.cff\n\n\n\n\n\ncff-version: 1.2.0\nmessage: \"If you are using this software, please cite it as shown below.\"\nauthors:\n- family-names: \"Doe\"\n  given-names: \"Jane\"\n  orcid: \"https://orcid.org/9999-9999-9999-9999\"\ntitle: \"Name of your software\"\nversion: 1.0.1\ndoi: \"11.1111/11111\"\ndate-released: 2024-12-31\nlicense: MIT\nurl: \"https://github.com/your_repo\"\n\n\n\nWhen citing a paper that is linked to the software you can use preferred-citation argument.\n\n\n\n\n\n\n An example of a CITATION file citing a research article\n\n\n\n\n\ncff-version: 1.2.0\nmessage: \"If you are using this software, please cite it as shown below.\"\nauthors:\n- family-names: \"Doe\"\n  given-names: \"Jane\"\n  orcid: \"https://orcid.org/9999-9999-9999-9999\"\ntitle: \"Name of your software\"\nversion: 1.0.1\ndoi: \"11.1111/11111\"\ndate-released: 2024-12-31\nlicense: MIT\nurl: \"https://github.com/your_repo\"\npreferred-citation:\n    type: article\n    authors:\n    - family-names: \"Doe\"\n      given-names: \"Jane\"\n      orcid: \"https://orcid.org/9999-9999-9999-9999\"\n    doi: \"11.1111/11111\"\n    journal: \"The title of the journal\"\n    month: 12\n    start: 19 # the first page number\n    end: 29 #the last page number\n    title: \"Name of your submitted paper\"\n    issue: 9\n    volume: 2\n    year: 2024\n    \n\n\n\n\n\n\n\n\n\n How the citation would look on GitHub\n\n\n\n\n\nOn GitHub, it will show in either APA or BibTeX formatting, as they are the currently supported formats. If you add a CITATION.cff file to your repository, then a label for citing will automatically be generated and will show up on the right sidebar of the repository.\nAPA\n\nDoe, J. (2024). Name of your software (Version 1.0.1) [Computer software]. https://doi.org/11.1111/11111\n\nBibTeX\n\n@software{Joe_Name_of_your_software_2024, author = {Doe, Jane}, doi = {11.1111/11111}, month = {12}, title = {{Name of your software}}, url = {https://github.com/your_repo}, version = {1.0.1}, year = {2024} }\n\nThis is an example of software citation.\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCITATION.cff documentation\nGitHub documentation on CITATION files - this resource also includes how to cite something other than software or a journal article.\nGenerate CITATION.cff files\nCitation File Format GitHub",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "CITATION"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html",
    "href": "docs/software/development_workflow/reusing_projects.html",
    "title": "Project templates and reusability",
    "section": "",
    "text": "Templates can help you to standardize your software development process.\n\n\nYou can turn an existing repository into a template, so you and others can generate new repositories with the same directory structure, branches, and files. Note, the template repository cannot include files stored using Git LFS.\n\n\n\n\n\n\n Repository templates\n\n\n\n\nCreating a template repository\nRepository template example to make your code more compliant with FAIR principles\n\n\n\n\n\n\nCookiecutter creates Python projects from project templates. The advantage of using Cookiecutter is that new projects are set up quickly from a standardized template structure and can include everything needed to get started on a project, such as directory layouts, sample code, and even integrations with tools and services.\n\n\n\n\n\n\n Tutorials\n\n\n\n\nTutorial for Cookiecutter\nFor installation instructions, have a look at Cookiecutter installation instructions.\n\n\n\n\n\n\n\n\n\n Cookiecutter templates\n\n\n\n\nCookiecutter templates on GitHub\nCookiecutter PyPackage: template for distributing Python libraries.\n\nGitHub - cookiecutter-pypackage\nGitHub - Netherlands eScience Center template\n\nCookiecutter Machine Learning: template for machine learning projects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html#project-templates",
    "href": "docs/software/development_workflow/reusing_projects.html#project-templates",
    "title": "Project templates and reusability",
    "section": "",
    "text": "Templates can help you to standardize your software development process.\n\n\nYou can turn an existing repository into a template, so you and others can generate new repositories with the same directory structure, branches, and files. Note, the template repository cannot include files stored using Git LFS.\n\n\n\n\n\n\n Repository templates\n\n\n\n\nCreating a template repository\nRepository template example to make your code more compliant with FAIR principles\n\n\n\n\n\n\nCookiecutter creates Python projects from project templates. The advantage of using Cookiecutter is that new projects are set up quickly from a standardized template structure and can include everything needed to get started on a project, such as directory layouts, sample code, and even integrations with tools and services.\n\n\n\n\n\n\n Tutorials\n\n\n\n\nTutorial for Cookiecutter\nFor installation instructions, have a look at Cookiecutter installation instructions.\n\n\n\n\n\n\n\n\n\n Cookiecutter templates\n\n\n\n\nCookiecutter templates on GitHub\nCookiecutter PyPackage: template for distributing Python libraries.\n\nGitHub - cookiecutter-pypackage\nGitHub - Netherlands eScience Center template\n\nCookiecutter Machine Learning: template for machine learning projects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/reusing_projects.html#reusing-projects-and-repositories",
    "href": "docs/software/development_workflow/reusing_projects.html#reusing-projects-and-repositories",
    "title": "Project templates and reusability",
    "section": "Reusing projects and repositories",
    "text": "Reusing projects and repositories\nOne of the easiest ways to reuse code across projects is by packaging it into an installable library that can be used as a dependency. Alternatively, you can integrate external code into your project using Git submodules or Git subtree.\n\n\n\n\n\n\nPractices to avoid\n\n\n\n\nStoring commonly-used folders in a separate folder on your system and adding the folder to the PATH. Other users/developers will not have access to these folders.\nDirect copy-and-pasting of code as you lose any upstream changes to the external repository.\n\n\n\n\nWhat‚Äôs the Difference?\n\n\n\n\n\n\n\n\nFeature\nGit Submodules\nGit Subtree\n\n\n\n\nHow it works\nAdds an external repository inside your project as a separate Git reference.\nMerges an external repository‚Äôs contents into your project‚Äôs directory structure.\n\n\nVersion Control\nTracks a specific commit of the external repository (not automatically updated).\nThe external repository‚Äôs commits are fully merged into your project‚Äôs commit history.\n\n\nUpdating\nRequires running git submodule update --remote to pull new changes.\nUpdates by merging changes from the external repository into your project.\n\n\nIdeal For\nKeeping external code separate while still using it in your project.\nFully integrating external code while keeping its history.\n\n\n\nIn short:\n\nUse Git submodules when you want to include an external repository but keep it separate, track its exact version, and update it manually.\nUse Git subtree if you want to fully integrate an external repository‚Äôs code into your project while keeping its commit history.\n\n\n\nGit submodules\nA Git submodule allows you to add a separate Git repository inside another repository as a subdirectory. It is a record that points to a specific commit in another external repository. Submodules are useful for incorporating external code or libraries into your project while keeping them separate and easily updatable.\n\n\n\n\n\n\nGit submodule commands\n\n\n\n\n\n\nAdding a submodule\nTo add an external repository as a submodule inside your project, use:\ngit submodule add &lt;repo-url&gt;\n\n\nCloning a repository with submodules\nWhen cloning a repository that contains submodules, follow these steps: 1. Clone the repository:\ngit clone &lt;repo-url&gt;\n\nInitialize the submodules:\n\ngit submodule init\n\nFetch the submodule content:\n\ngit submodule update\nAlternatively, you can use the shorthand command to clone and initialize submodules:\ngit clone --recurse-submodules &lt;repo-url&gt;\n\n\nUpdating submodules\nTo update the submodules to the latest commit, use:\ngit submodule update --remote\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nSimplified Git submodules tutorial\nGuide on Git submodules - comprehensive guide that covers everything from the basics to advanced workflows\n\n\n\n\n\n\n\n\n\nIf you are using GitHub Desktop\n\n\n\n\n\nIf you are using GitHub Desktop, be aware that there can be limitations when working with submodules. While GitHub Desktop supports basic submodule functionality, some operations may require using the command line. Known issues include\n\ndifficulties in initializing submodules\nswitching branches with submodules\nvisualizing submodule changes.\n\nFor more details, check out this discussion or visit the GitHub Desktop issue tracker.\n\n\n\n\n\nGit subtree\nUnlike Git submodules, Git subtree merges the history of one repository into another as a subdirectory. This makes the external repository‚Äôs files appear as if they are part of your project while still allowing updates.\nFor more details, check out this Git subtree guide.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project templates and reusability"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html",
    "href": "docs/software/development_workflow/project_management.html",
    "title": "Project management",
    "section": "",
    "text": "Git is a distributed version control system that enables you to track changes in your code over time. Platforms like GitHub, GitLab and Bitbucket extend the features of git by providing a centralized location for storing repositories, collaborating, and providing powerful tools to plan, organize, and track your work efficiently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#version-control-platforms",
    "href": "docs/software/development_workflow/project_management.html#version-control-platforms",
    "title": "Project management",
    "section": "Version control platforms",
    "text": "Version control platforms\nThe choice between GitHub, GitLab and Bitbucket depends on your required features, privacy and other preferences, but all are Git-based platforms for version control. While numerous detailed comparisons exist online, here we will focus on GitHub.\n\n\n\n\n\n\nTU Delft GitLab\n\n\n\nTU Delft has its own GitLab instance hosted on campus. For more information, please visit the documentation and our TU Delft GitLab guides in the Computing Infrastracture section.\n Choosing a Repository Manager - for TU Delft Researchers\n\n\nSimilarly, whether you are using a version control system through your terminal or Integrated Development Environment (IDE), or using a GUI like GitHub Desktop, the core functionality remains the same.\n\n\n\n\n\n\n Learn more about GitHub Desktop\n\n\n\n\n\nGitHub Desktop is a great choice if you are just starting out with version control, providing a user-friendly graphical interface that simplifies Git operations. It makes it easy to visualize changes, create branches, and manage pull requests without needing to use command-line Git commands.\n Getting started with GitHub Desktop",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#github-issues",
    "href": "docs/software/development_workflow/project_management.html#github-issues",
    "title": "Project management",
    "section": "GitHub issues",
    "text": "GitHub issues\nGitHub issues help you keep track of tasks, bugs, feature ideas in your project. They are like a to-do list items that everyone on your team can see and update.\nHow to use issues effectively:\n\nUse descriptive titles: Write short, specific titles that make it easy to understand what the issue is about.\nProvide detailed descriptions: Include all relevant information, steps to reproduce (if reporting a bug), and expected outcomes in the issue description. This ensures that anyone working on the issue has all the necessary context.\nUse labels: Labels act like tags to help you organize and prioritize tasks.\nAssign people: By assigning someone (or yourself) you let others know that you are picking up and working on this issue.\nLink related issues: Connect related work by linking issues to provide context (add #issue_number to reference an issue).\n\nGitHub Issues make it easier to manage your project, collaborate with others, and keep track of progress. As your project grows, you can use additional tools like milestones and project boards while still benefiting from well-organized issues.\n\n\n\n\n\n\n Learn more\n\n\n\n\nQuickstart for GitHub Issues\nMastering GitHub Issues",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#project-boards",
    "href": "docs/software/development_workflow/project_management.html#project-boards",
    "title": "Project management",
    "section": "Project boards",
    "text": "Project boards\nProject boards on GitHub are designed for planning, organizing, and tracking work within a project. They serve as visual management interfaces that integrate directly with GitHub issues and pull requests. Project boards can be configured as Kanban boards, tables, or roadmaps, offering various layouts to suit different project management needs. Project boards can be particularly useful for visualizing the overall progress of your project and identifying bottlenecks in your workflow. They provide a high-level view that complements the detailed tracking offered by issues.\n\nMilestones\nUsing milestones you can break down large projects into smaller, more manageable parts. While project boards offer a visual, dynamic interface to manage and track your tasks, milestones serve as structured markers that help you monitor progress toward key project phases/goals.\n\n\n\n\n\n\n Learn more\n\n\n\n\nExample project board - TU Delft Astrodynamics Toolkit (Tudat)\nDefine a milestone\nGitHub Guides for Issues and Projects",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#managing-projects-on-github",
    "href": "docs/software/development_workflow/project_management.html#managing-projects-on-github",
    "title": "Project management",
    "section": "Managing projects on GitHub",
    "text": "Managing projects on GitHub\nWhen managing your project on GitHub, we recommend two approaches:\n\na simplified approach using only issues, or\na more structured approach using milestones and project boards.\n\nBoth methods have advantages, and the choice depends on the size of the project, its complexity, and your preferences.\n\nSimplified approachStructured approach\n\n\nYou can use a simplified approach and just track your progress in the form of issues and work without defining milestones or using a project board. This can be particularly useful for smaller projects or when you are just starting out with GitHub.\nIn this approach, you would:\n\nCreate issues for each task or feature you need to work on.\nOptionally, use labels to categorize and prioritize your issues.\nAssign issues to yourself or team members/collaborators.\nUse comments to update progress and discuss any challenges.\nClose issues as you complete them.\n\nThis method allows for a flexible workflow while still maintaining a good level of organization and transparency in your project. If your project grows or becomes more complex, you can always adopt milestones and project boards for more structured project management.\n\n\nWhen working on a big project, it‚Äôs helpful to create a roadmap - a simple plan that outlines what needs to be done and when. A roadmap gives you and your team a clear view of what‚Äôs happening now and what‚Äôs coming next.\nTo create a roadmap, it is useful to map out the key milestones and the tasks needed to accomplish the milestones. You can then use GitHub milestones and project boards to track progress and manage your project:\n\nDefine key milestones.\nCreate a milestone in GitHub\nAdd related issues to your milestone.\nSet up a project board.\nAdd issues to your project board.\nUse task lists in your issues to break down the work.\nAssign tasks to team members.\nLinking milestones, issues and pull requests to track progress.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_management.html#github-benefits-for-researchers-and-organisations",
    "href": "docs/software/development_workflow/project_management.html#github-benefits-for-researchers-and-organisations",
    "title": "Project management",
    "section": "GitHub benefits for researchers and organisations",
    "text": "GitHub benefits for researchers and organisations\nResearchers at TU Delft are eligible to receive GitHub Educational benefits, which includes\n\nGitHub Team plan at no cost (check out the benefits)\nGitHub codespaces\nGitHub pages for private repositories\nAnd more!\n\nTo qualify for the benefits, you must:\n\nHave a GitHub account\nBe an educator, faculty member, or researcher at a recognized educational institution\nBe able to provide documentation from your institute demonstrating your employment\n\n\n\n\n\n\n\n Apply for Educational benefits",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/r_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/r_envs_dependencies.html",
    "title": "Environment and dependency management in R",
    "section": "",
    "text": "R users often rely on RStudio Projects and renv to manage their development environments. RStudio Projects organize your workspace by managing file paths and configurations, while renv tracks and restores package dependencies to ensure reproducibility. Together, they provide a structured and reliable workflow for managing R projects.\nAlthough Conda can be used to create isolated R environments, it is less common in R workflows. Conda is most useful when managing multiple R versions, working in a Python + R setup, or handling system dependencies that are difficult to install through CRAN.\n\nProjects\nRStudio Projects provide an organized workspace for your analyses and scripts, ensuring that file paths and working directories remain consistent across sessions. When you open an RStudio Project, it automatically sets the project‚Äôs root directory as your working directory and loads project-specific settings stored in the .Rproj file. Using RStudio Projects helps you to keep your code, data, and output bundled together, and avoid issues with file paths.\nCreating a new RStudio Project:\n\nIn RStudio, go to File ‚Üí New Project.\nChoose whether to create a new directory, use an existing directory, or clone a project.\nFollow the prompts to configure your project settings.\n\n\n\nrenv\nThe renv package manages R package dependencies within a project. It creates a reproducible snapshot of your package environment, ensuring that collaborators (or your future self) can recreate the exact setup. Some key renv commands:\nInitialize renv in your project:\n# From your R console within the project directory:\nrenv::init()\nThis creates a dedicated library and a renv.lock file that tracks all installed packages. If a renv.lock file exists, renv::init() will automatically install the recorded dependencies. Otherwise, it sets up a new environment.\nCreate a snapshot of your dependencies:\nrenv::snapshot()\nThis updates the renv.lock file to reflect the current package versions in your project.\nTo restore an environment from a lockfile:\nrenv::restore()\nThis reinstalls packages according to the renv.lock file and reconstructs the environment.\n\n\n\n\n\n\n Learn more\n\n\n\n\nUsing RStudio Projects\nrenv for R",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "R"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/matlab_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/matlab_envs_dependencies.html",
    "title": "Environment and dependency management in MATLAB",
    "section": "",
    "text": "MATLAB does not use virtual environments like Python, where isolated environments manage dependencies. Instead, MATLAB handles project-specific dependencies using:\n\nToolboxes - Pre-packaged libraries that must be licensed and available\nMATLAB projects - A feature that manages paths and environments for a project\nPath management - Manually adding paths to the MATLAB search path with addpath and rmpath\n\nTo check dependencies in a project:\n\nUse requiredfilesandproducts to identify required MathWorks toolboxes for a script of function.\nUse the the Dependency Analyzer to detect file dependencies.\n\n\nCustom MATLAB Dependency Manager\nTo offer a solution for managing dependencies in MATLAB through a dependency file, we have created a Dependency Manager:\n DependencyManager",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/containers/intro.html",
    "href": "docs/software/containers/intro.html",
    "title": "Introduction to containers",
    "section": "",
    "text": "Just as you document your software dependencies, it may be useful to record and manage your development environments. This may involve documenting the specific configurations, tools, and versions used during development to ensure that everything runs consistently across different setups.\nBy recording your environments, you create a reproducible framework that significantly reduces the infamous ‚Äúit works on my machine‚Äù syndrome, where code runs on one machine but fails on others.\nEnvironment management includes specifying your operating system, programming language (and their versions), system libraries, and any other software or tools required. This practice not only helps in maintaining consistency across development, testing, and production stages but also streamlines onboarding for new team members, as they can quickly set up their environment to match the recorded specifications."
  },
  {
    "objectID": "docs/software/containers/intro.html#introduction-to-containers",
    "href": "docs/software/containers/intro.html#introduction-to-containers",
    "title": "Introduction to containers",
    "section": "",
    "text": "Just as you document your software dependencies, it may be useful to record and manage your development environments. This may involve documenting the specific configurations, tools, and versions used during development to ensure that everything runs consistently across different setups.\nBy recording your environments, you create a reproducible framework that significantly reduces the infamous ‚Äúit works on my machine‚Äù syndrome, where code runs on one machine but fails on others.\nEnvironment management includes specifying your operating system, programming language (and their versions), system libraries, and any other software or tools required. This practice not only helps in maintaining consistency across development, testing, and production stages but also streamlines onboarding for new team members, as they can quickly set up their environment to match the recorded specifications."
  },
  {
    "objectID": "docs/software/containers/intro.html#why-use-containers",
    "href": "docs/software/containers/intro.html#why-use-containers",
    "title": "Introduction to containers",
    "section": "Why use containers?",
    "text": "Why use containers?\nContainers remove the need for manual installation or troubleshooting, ensuring that software runs consistently across different machines. Containers act as a self-contained unit that bundle everything needed to run software, including dependencies and system libraries, into a single package known as an image.\nThe definition files, like Dockerfiles or Singularity definition files, are essentially instruction manuals to build a container image.\n\n\n\n\n\n\nAn analogy for a container image\n\n\n\n\n\nConsider a container image like a detailed movie script that outlines every scene. When the movie is shot (i.e., the container is executed), a temporary set is built where all filming occurs in a controlled environment. Once filming ends, the set is dismantled, but the script remains unchanged, allowing the software to be re-executed with the same consistent results every time, just as each scene is reproduced faithfully with the same script.\n\n\n\nContainers can be useful for various purposes:\n\nIf installing certain software is complex or incompatible with your operating system, you can use a pre-built container image to run the software seamlessly.\nLikewise, if you want to make sure that the people you are working with use the exact same environment, you could provide them with an image of your container.\nIf you are facing issues due to different system architectures you can distribute a definition file to build an image that tailors to different machines. While it might not replicate your environment exactly, it often provides a sufficiently close alternative.\n\nPopular containerisation solutions:\n\nDocker and Podman are great for general software development.\nSingularity is tailored for high-performance computing (HPC) environments.\nFor managing and scaling containers across multiple machines, tools like Kubernetes are commonly used.\n\nDocker‚Äôs official samples and examples\n\n\n\n\n\n\nFurther reading\n\n\n\n\nRecording environments lesson on CodeRefinery\nCarpentries incubator lesson on Docker\nGuides and manuals for Docker\nSingularity documentation"
  },
  {
    "objectID": "docs/software/containers/intro.html#advantages-and-disadvantages-of-using-containers",
    "href": "docs/software/containers/intro.html#advantages-and-disadvantages-of-using-containers",
    "title": "Introduction to containers",
    "section": "Advantages and disadvantages of using containers",
    "text": "Advantages and disadvantages of using containers\nContainers have gained widespread popularity due to their significant benefits in solving various challenges:\n\nThey enable a smooth transition of workflows across different operating systems and configurations.\nThey address the common issue of software behaving differently on different machines by providing a consistent runtime environment.\nFor software with nested dependencies, containers can be a vital tool for ensuring long-term reproducibility.\nContainers package software in isolated files, simplifying management, installation, and removal compared to traditional methods.\n\nBut it is important to consider the downsides:\n\nThe convenience of containers might lead to overlooking underlying software installation issues and not adhering to good software development practices.\nThere‚Äôs a risk of creating a new form of dependency - software that ‚Äúonly works in a specific container‚Äù, which could limit flexibility and interoperability.\nContainer images can grow in size substantially, especially if not carefully managed.\nModifying existing containers can sometimes be challenging, requiring a good understanding of the container‚Äôs configuration.\nEnsuring container security is vital, as misconfigurations can expose vulnerabilities.\n\n\n\n\n\n\n\nCaution\n\n\n\nIt‚Äôs crucial to source your container images from reputable and official channels. There have been instances where images were found to be malicious, so it is very important to apply the same caution as when installing software packages from untrusted sources.\nAlways download images from trusted sources like Docker Hub. Utilize container scanning tools, such as Docker scanning tools to mitigate risks from malicious images."
  },
  {
    "objectID": "docs/software/code_quality/refactoring.html",
    "href": "docs/software/code_quality/refactoring.html",
    "title": "Refactoring",
    "section": "",
    "text": "‚ÄúAlways leave the code you‚Äôre editing a little better than you found it.‚Äù\nRobert C. Martin (Uncle Bob)\n\n\nWhat is refactoring?\nRefactoring is the process of restructuring existing code without changing its external behavior. It improves maintainability and readability, making future developments smoother and reducing the likelihood of bugs. Key benefits include:\n\nImproving readability - Writing code that‚Äôs easier to understand, benefits both yourself and future developers.\nReducing complexity - Simplifying complex structures by breaking down large functions or removing unnecessary dependencies.\nOptimizing design - Creating a more robust and adaptable codebase for long-term growth.\nEliminating redundancies - Removing duplicate or unnecessary code.\nEnsuring consistency - Following a consistent coding style for a cleaner, more maintanable codebase.\n\n\n\nWhen should you refactor?\n\n\n\nCC-BY-4.0 ¬© 2021 Balaban et al.\n\n\n\nRule of three: If you find yourself writing the same or similiar code for the third time, it‚Äôs time to refactor.\nBefore adding a feature: Cleaning up existing code makes it easier to integrate a new functionality.\nWhen fixing a bug: Cleaning up surrounding code can help uncover and fix the issue faster.\nDuring code reviews: Refactoring during code reviews can prevent issues from becoming part of the public codebase and streamline the development process.\nWhen you spot a code smell: Addressing code smells early prevents them from evolving into more serious bugs.\n\n\n Learn more: When to refactor?\n\n\n\nHow to refactor code effectively?\nRefactoring should be done gradually, improving code in small controlled steps without introducing new functionalities. Keep these principles in mind:\n Maintain clean code - Aim for clarity, simplicity, and readability.\n Avoid adding new features - Focus on improving structure, not functionality.\n Ensure tests pass - Verify that all existing tests still succeed to prevent new bugs.\n\n Learn more: How to refactor?\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRefactoring techniques from Refactoring.Guru\neScience Center - Lesson on refactoring\nThe Alan Turing Institute - Refactoring",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Refactoring"
    ]
  },
  {
    "objectID": "docs/software/code_quality/index.html",
    "href": "docs/software/code_quality/index.html",
    "title": "Code quality",
    "section": "",
    "text": "‚ÄúEveryone knows that debugging is twice as hard as writing a program in the first place. So if you‚Äôre as clever as you can be when you write it, how will you ever debug it?‚Äù\nBrian W. Kernighan\n\nThe quality of your research software plays a crucial role in its reliability, maintainability, and scalability. Writing clean code means developing code that is easy to read, understand - not for just you, but for others as well. Well-structured code simplifies debugging and allows for future modifications and extensions, ensuring your software remains useful and adaptable over time.\n\n\n\n Code Style\nConventions and guidelines used to write and format code.\n\nLearn more ¬ª\n\n\n\n Refactoring\nRestructuring existing code without changing its external behavior.\n\nLearn more ¬ª\n\n\n\n Code Smells\nSymptoms of poor code quality that can indicate deeper problems.\n\nLearn more ¬ª\n\n\n\n Online services\nServices that provide code quality analysis.\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nThe Turing Way - Writing Robust Code\nUtrecht University - Workshop on Writing Reproducible Code",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html",
    "href": "docs/software/code_quality/code_smells/side_effects.html",
    "title": "Side effects and external state",
    "section": "",
    "text": "Side effects occur when a function modifies external state or interacts with the outside world beyond simply returning a value. This makes code less predictable, harder to test, and more difficult to debug.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#symptoms",
    "href": "docs/software/code_quality/code_smells/side_effects.html#symptoms",
    "title": "Side effects and external state",
    "section": "Symptoms",
    "text": "Symptoms\n\nUnexpected changes to the global state\nNon-deterministic behavior\nHidden dependencies\n\n\n\n\n\n\n\nTip\n\n\n\nPure functions are deterministic (always return the same output for the same input) and have no side-effects.\nInstead, non-pure functions often:\n\nModify global variables or shared state, leading to unintended behavior.\nChange input parameters (mutating function arguments).\nPerform I/O operations like reading from or writing to files, databases, or APIs.\nGenerate random numbers, making them non-deterministic.\nDepend on external state, meaning results may change due to external factors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#example---function-with-side-effects",
    "href": "docs/software/code_quality/code_smells/side_effects.html#example---function-with-side-effects",
    "title": "Side effects and external state",
    "section": "Example - Function with side effects",
    "text": "Example - Function with side effects\n# Modifies global state (side effect)\ndata = []\n\ndef add_item(item):\n    data.append(item)  # Changes an external variable\n\nadd_item(\"A\")\nprint(data)  # ['A'] - Output depends on previous calls\n\nSolutions\n\n1. Separate pure and non-pure functions\nKeep your computational logic (pure) separate from side-effect operations (non-pure).\ndef process_data(data):  # Pure function: no external state modification\n    return [x**2 for x in data]\n\ndef save_to_file(filename, data):  # Non-pure: writes to a file\n    with open(filename, \"w\") as f:\n        f.write(\"\\n\".join(map(str, data)))\n\n# Usage\nnumbers = [1, 2, 3]\nprocessed = process_data(numbers)\nsave_to_file(\"output.txt\", processed)\n\n\n2. Avoid mutating global variables\nUse function parameters and return values instead of modifying external variables.\ndef add_item(data, item):\n    return data + [item]  # Returns a new list instead of modifying global state\n\ndata = []\ndata = add_item(data, \"A\")  # Safe: no side effects",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/side_effects.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/side_effects.html#key-takeaways",
    "title": "Side effects and external state",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nEnsure that each function or module has a single responsibility.\nBreak down complex functions into smaller, focused functions that perform specific tasks.\nIsolate non-pure functions with side effects from pure functions.\n\n\n\n\nCC-BY-4.0 CodeRefinery",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Side effects"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html",
    "href": "docs/software/code_quality/code_smells/long_method.html",
    "title": "Long Method",
    "section": "",
    "text": "‚ÄúFunctions should do one thing. They should do it well. They should do it only.‚Äù\nRobert C. Martin (Uncle Bob)\nA ‚Äúlong method‚Äù is a common code where a method or function becomes overly long and handles multiple responsibilities at once. This makes the code hard to read, understand, test, and maintain. Long methods often indicate that a function is doing too much and may benefit from being broken into smaller, more focussed helper functions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#symptoms",
    "href": "docs/software/code_quality/code_smells/long_method.html#symptoms",
    "title": "Long Method",
    "section": "Symptoms",
    "text": "Symptoms\nA long method often:\n\nPerforms multiple tasks rather than a single, well-defined responsibility.\nHas deeply nested control structures, making it harder to follow.\nIncludes multiple sections of logic that could be extracted into separate functions.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#example---long-method",
    "href": "docs/software/code_quality/code_smells/long_method.html#example---long-method",
    "title": "Long Method",
    "section": "Example - Long method",
    "text": "Example - Long method\nBelow is an example of a function that is doing too much:\ndef load_data(filepath: str):\n    # Check if data file exists\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(\"File not found\")\n    \n    _, extension = os.path.splitext(filepath)\n\n    # Load data based on file extension    \n    if extension == \".json\":\n        with open(filepath, \"r\") as file:\n            # If file extension is .json: load json data\n            data = json.load(file)\n    elif extension == \".pickle\":\n        with open(filepath, \"rb\") as file:\n            # If file extionsion is .pickle: load pickled data\n            data = pickle.load(file)\n    elif extension == \".csv\":\n        # If file extionsion is .csv: load cvs data\n        data = read_csv(filepath)\n    else:\n        raise ValueError(f\"Unsupported file format: {extension}\")\n    \n    # Verify content of data set\n    if not isinstance(data, (list, dict, pd.DataFrame)):\n        raise ValueError(\"Invalid data format\")\n\n    return data\n\nIssues\n\nThe function is handling file validation, data loading, and data verification, which are separate concerns.\nIt is now difficult to test individual parts in isolation.\nAdding support for new file types requires modifying a large function.\n\n\n\nSolution\nIdentify logical blocks of code within the long method/function and extract them into separate methods with descriptive names. We should aim to make each method responsible for a singular task and compose more complex functionalities from modular components.\n\nExample solution long method\ndef load_data(filepath: str) -&gt; Data:\n    verify_filepath(filepath: str)  \n    data = read_data(filepath: str)\n    verify_data(data)\n    return data\n\n# Helper function to verify file path\ndef verify_filepath(filepath: str):\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(\"File not found\")\n\n# Helper function to read data from file based on its extension\ndef read_data(filepath: str) -&gt; Data:\n    # Extract file extension\n    _, extension = os.path.splitext(filepath)\n    \n    # Create dictionary mapping file extensions to read functions\n    data_types = {\n        \".json\": read_from_json,\n        \".pickle\": read_from_pickle,\n        \".csv\": read_from_csv,\n    }\n\n    # Select read function based on file extension\n    try:\n        read_function = data_types[extension]\n    except KeyError:\n        raise ValueError(f\"Unsupported file format: {extension}\")\n    return data_types[extension](filepath)\n\n# Placeholder for helper functions to read data from different file formats\ndef read_from_json(filepath: str): pass\ndef read_from_pickle(filepath: str): pass\ndef read_from_csv(filepath: str): pass",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/long_method.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/long_method.html#key-takeaways",
    "title": "Long Method",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nBreaking a long method into smaller, well-named helper functions makes the code easier to read and understand.\nEach function now has a single responsibility, reducing complexity and making future modifications more manageable.\nWith isolated functions, individual components can be tested independently, leading to more reliable and maintainable code.\n\n By breaking the long method into smaller helper functions, we improve the overall structure and maintainability of the code.\n\n\n\n\n\n\n Learn more\n\n\n\n\neScience Center - Slides on writing modular code\nCarpentries Incubator - Modular Code Development",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Long method"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html",
    "title": "Inappropriate Intimacy",
    "section": "",
    "text": "This code smell occurs when one part of the system knows too much about the internal details of another, leading to tight coupling. When components are too dependent on each other, it becomes difficult to modify or extend the system without breaking other parts.\nA good design principle to follow is the Law of Demeter, also known as the ‚ÄúDon‚Äôt talk to strangers‚Äù rule. It suggests that a module should only interact with its direct dependencies rather than deeply nested objects.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#symptoms",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#symptoms",
    "title": "Inappropriate Intimacy",
    "section": "Symptoms",
    "text": "Symptoms\n\nA class accesses properties of another object‚Äôs properties, exposing too much detail.\nChanges in one part of the code require changes in multiple other places.\nBecause multiple classes depend on each other‚Äôs internal structures, small changes can cause unintended issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#example---violating-law-of-demeter",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#example---violating-law-of-demeter",
    "title": "Inappropriate Intimacy",
    "section": "Example - Violating Law of Demeter",
    "text": "Example - Violating Law of Demeter\nIn this example, a SensorSystem directly accesses the TemperatureSensor internal attributes, creating tight coupling.\nclass TemperatureSensor:\n    def __init__(self, temperature):\n        self.temperature = temperature  # Internal detail exposed\n\nclass SensorSystem:\n    def __init__(self, sensor):\n        self.sensor = sensor\n\n    def get_temperature(self):\n        # Law of Demeter violation: Directly accessing sensor's attribute\n        return self.sensor.temperature\n\n# Usage\nsensor = TemperatureSensor(25)\nsystem = SensorSystem(sensor) \ntemperature = system.get_temperature()\nprint(temperature)  # 25\nProblem: The SensorSystem class depends on the internal structure of TemperatureSensor. If the way temperature is stored changes (e.g., a new sensor model), SensorSystem must also change.\n\nSolutions\n\nExample solution - Using getter methods\nInstead of directly accessing attributes, define getter methods in TemperatureSensor to limit exposure.\nclass TemperatureSensor:\n    def __init__(self, temperature):\n        self._temperature = temperature  # Use a private variable\n\n    def get_temperature(self):\n        return self._temperature  # Encapsulated access\n\nclass SensorSystem:\n    def __init__(self, sensor):\n        self.sensor = sensor\n\n    def get_temperature(self):\n        return self.sensor.get_temperature()  # Indirect access through method\n\n# Usage\nsensor = TemperatureSensor(25)\nsystem = SensorSystem(sensor)\nprint(system.get_temperature())  # 25\nWhy is this better?\n\nThe SensorSystem no longer needs to know the internal structure of TemperatureSensor.\nIf TemperatureSensor changes, only get_temperature() needs to be updated, not every place it‚Äôs used.\n\n\n\nExample solution - Removing the dependency\nA better design is to pass only the needed data instead of an entire object.\nclass SensorSystem:\n    def __init__(self, temperature):\n        self.temperature = temperature\n\n    def get_temperature(self):\n        return self.temperature  # Works directly with the value\n\n# Usage\ntemperature = 25\nsystem = SensorSystem(temperature)\nprint(system.get_temperature())  # 25\nWhy is this better?\n\nSensorSystem no longer depends on TemperatureSensor, making it more modular and reusable.\nWorks even if the source of temperature data changes (e.g., from a file, API, or another sensor).\n\n\n\n\n\n\n\nBalance between dependecy injection and encapsulation. If the data is simple and does not require complex operations, pass it directly. If the data is complex or requires additional logic, encapsulate it in a class.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/inappropriate_intimacy.html#key-takeaways",
    "title": "Inappropriate Intimacy",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFollow the Law of Demeter - Only interact with direct dependencies.\nEncapsulate data - Use getters and setters to access and modify data.\nReduce dependencies - pass only the necessary information to other components.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRealPython - Python Classes\nRealPython - Getters and Setters",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Inappropriate intimacy"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html",
    "href": "docs/software/code_quality/code_smells/duplication.html",
    "title": "Duplicated code",
    "section": "",
    "text": "‚ÄúPerfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.‚Äù\nAntoine de Saint-Exup√©ry\nDuplicated code occurs when similar or identical blocks of code appear multiple times within a codebase. This can increase maintenance efforts, as changes in one place might require corresponding changes elsewhere, leading to inconsistencies and higher changes or errors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#symptoms",
    "href": "docs/software/code_quality/code_smells/duplication.html#symptoms",
    "title": "Duplicated code",
    "section": "Symptoms",
    "text": "Symptoms\n\nThe same logic appears in multiple places, sometimes with minor variations.\nFixing a bug required modifying the same code in multiple places.\nAdding a new feature results in copy-pasting existing code rather than reusing it.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#example---duplicate-code-in-functions",
    "href": "docs/software/code_quality/code_smells/duplication.html#example---duplicate-code-in-functions",
    "title": "Duplicated code",
    "section": "Example - Duplicate code in functions",
    "text": "Example - Duplicate code in functions\ndef time_of_flight_ball(initial_velocity, launch_angle):\n    g = 9.81  # Earth's gravity (m/s¬≤)\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\ndef time_of_flight_rocket(initial_velocity, launch_angle):\n    g = 9.81\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\ndef time_of_flight_satellite(initial_velocity, launch_angle):\n    g = 9.81\n    return (2 * initial_velocity * np.sin(launch_angle)) / g\n\nSolution\n\nRefactor the code to accept parameters as arguments, instead of hard-coding them.\nExtract common functionality into functions or methods.\nRefactor duplicated code into higher-level abstractions.\nMake use of utility functions to centralize common code and avoid duplication.\n\ndef time_of_flight(initial_velocity, launch_angle, gravity=9.81):\n    \"\"\"Compute time of flight for any projectile.\"\"\"\n    return (2 * initial_velocity * np.sin(launch_angle)) / gravity\n\n# Usage\ntof_ball = time_of_flight(30, np.pi/4)       # Time of flight for a ball\ntof_rocket = time_of_flight(100, np.pi/3)    # Time of flight for a rocket\ntof_mars_probe = time_of_flight(300, np.pi/6, gravity=3.71)  # Gravity adjusted for Mars",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/duplication.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/duplication.html#key-takeaways",
    "title": "Duplicated code",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nExtracting common functionality into functions or methods can help reduce duplication and improve code reuse.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Code duplication"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html",
    "href": "docs/software/code_quality/code_smells/dead_code.html",
    "title": "Commented-out code",
    "section": "",
    "text": "Dead code refers to unused or unreachable code that remains in the codebase but serves no functional purpose. Commented-out code consists of inactive code blocks that developers have disabled rather than deleting. Both contribute to clutter and reduce maintainability.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Commented out code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#symptoms",
    "href": "docs/software/code_quality/code_smells/dead_code.html#symptoms",
    "title": "Commented-out code",
    "section": "Symptoms",
    "text": "Symptoms\n\nUnused variables or functions.\nConditional blocks that never execute.\nLarge blocks of commented-out code.\nUsing comments to disable code to change the behavior of the code.\n\n\n\n\n\n\n\nTip\n\n\n\nDo not use comments to change the behavior of the code. Instead, make use of input parameters or configuration settings to control the behavior of the code.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Commented out code"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/dead_code.html#solution",
    "href": "docs/software/code_quality/code_smells/dead_code.html#solution",
    "title": "Commented-out code",
    "section": "Solution",
    "text": "Solution\n\nIf code is not needed, delete it. Use version control (e.g., Git) to restore it if necessary. Commit the removal of the commented-out code with a meaningful commit message explaining why it was removed. This allows you to track the change and easily revert it if necessary.\nTo change the executation of your code, use input parameters or configuration settings to control the behavior of the code. This makes the code more readable and maintainable.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Commented out code"
    ]
  },
  {
    "objectID": "docs/resources/curriculum.html",
    "href": "docs/resources/curriculum.html",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "These materials represent a curated curriculum designed to help you develop and maintain your repository and code base. All of the resources listed below are free to access and use, and supplementary material like video lessons has been added where possible and relevant.\n\n\n\nWhat is Bash? - of all the shells available, Bash is one of the most popular, the most powerful, and the most friendly\nBash Essentials ‚Äì Bash commands commonly used to navigate file directories in your Terminal/GitBash\nThe Unix Shell lesson from Software Carpentries - use of the shell is fundamental to using a wide range of other powerful tools and computing resources. These lessons will start you on a path towards using these resources effectively\nInstallation Instructions - particularly important for Windows users, as Bash comes pre-installed on Mac. Windows users will need to install GitBash following the instructions at this link\nUsing the Terminal in Mac - The Terminal app allows you to control your Mac using a command prompt\n\n\n\n\n\nWhat is Git?- 2-minute video overview of the technology and how it works\nInstall Git from GitHub Guides - Check if Git is already installed on your machine; if not, follow these instructions to get started. Notes: If you‚Äôve already installed GitBash on Windows OS, you will have Git already. Installing GitHub Desktop will also install the latest version of Git if you don‚Äôt already have it.\nInstalling Git from Software Carpentries - Alternative installation instructions from Software Carpentries, including videos and details per OS.\nIntro to version control with Git from Code Refinery ‚Äì self-paced introductory lesson to version control using Git\nGit Intro video lesson from Code Refinery - Day 1 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 1/2\nGit Intro video lesson from Code Refinery - Day 2 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 2/2\nBranching and merging ‚Äì lesson from Code Refinery on concept of branching in Git (featuring octopus diagram)\nWhat is .gitignore? ‚Äì introduction to how and why to use the .gitignore file to not track some files in a project folder (e.g., because of their size or sensitivity)\nGit command cheat sheet ‚Äì commonly used Git commands in one page that can also be downloaded\n\n\n\n\n\nUnderstanding the GitHub flow ‚Äì guide from GitHub on how and why to work with branches\nCollaborative distributed version control - We have learned how to make a git repository for a single person. What about sharing?\nSSH connection to GitHub ‚Äì instructions to set up SSH connection to GitHub so that you do not need to input your login credentials with every push/pull\nGitlab and SSH keys - instructions to add an SSH key to your (TU Delft) GitLab account for the same reason as above\nGitHub without the Command Line from Code Refinery - practice collaborating and sharing using either the GitHub website or GitHub desktop application\nGitHub Guides: Mastering Markdown - Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\n\n\n\n\n\nIntroduction to Jupyter and JupyterLab - lesson material on the user interface of JupyterLab, how Jupyter notebooks work, and what some common and powerful usecases are\n\n\n\n\n\nAnaconda Installation Guide from Software Carpentries - Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with the latest version of Python and Jupyter Notebooks by default\nIntro to Anaconda Navigator - Anaconda Navigator is a graphical user interface to the conda package and environment manager. This 10-minute guide to Navigator will have you navigating the powerful conda program in a web-like interface without having to learn command line commands\nIntroduction to Conda for (Data) Scientists - Conda is an open source package and environment management system that easily creates, saves, loads, and switches between environments on your local computer\nManaging Conda environments - documentation on performing a range of common tasks with Conda using the command line\n\n\n\n\n\nScientific Computing with Python - a free video course series that teaches the basics of using Python 3\nApplied Data Science with Python Specialization - Coursera course in which you can enroll for free\nLearnPython.org - Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the Python programming language\nProgramming with Python from Software Carpentries - this introduction to Python is built around a common scientific task: data analysis\nPlotting and Programming with Python from Software Carpentries - an introduction to programming in Python for people with little or no previous programming experience using plotting as its motivating example\nData Analysis and Visualization with Python for Social Scientists - basic information about Python syntax, the Jupyter notebook interface, how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python\nCan You Speak Python? - test your knowledge of some important features of the Python programming language and the NumPy and Pandas libraries\n\n\n\n\n\nGetting started with Pandas - documentation and quick start guide for Pandas, an essential Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data\nPandas Tutorial - 14-part tutorial series featuring live code examples and tests of your knowledge\nPandas Data Wrangling Cheat Sheet - a cheat sheet of some of the most used syntax that you probably don‚Äôt want to miss\nPandas Cheat Sheet - Visual - visual, printable 2-page reference guide on commonly performed operations using Pandas\nUltimate Pandas Guide ‚Äî Inspecting Data Like a Pro - Whether you‚Äôre working on a simple analysis or a complex machine learning model, there‚Äôs a lot of value in being able to answer quick, exploratory questions about the nature of your data. This is a walk through of several DataFrame attributes and methods that make data inspection painless and productive\n10 Efficient Ways for Inspecting a Pandas DataFrame Object - A guide to using pandas effectively and efficiently\n\n\n\n\n\nGetting Started with Plotly - The plotly Python library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases\nPlotly Python Open Source Graphing Library - Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts\nHeatmaps with Plotly - How to make Heatmaps in Python with Plotly\n\n\n\n\n\nIpywidgets documentation - ipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.\nIntroduction to ipywidgets - in this tutorial video, learn about ipywidgets, a Python library for building interactive HTML widgets for your Jupyter browser.\nIpywidgets Interact Function | ipywidgets Examples of Slider, Dropdown, Checkbox, Text Box - Video demo on how to make an ipywidgets slider, ipywidgets dropdown, ipywidgets checkbox, or an ipywidgets text box using Python code.\n\n\n\n\n\nInstall R guide from Software Carpentries - R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio, which must also be installed separately from here\nProgramming with R from Software Carpentries - this introduction to R is built around a common scientific task: data analysis\nR for Reproducible Data Analysis from Software Carpentries - write modular code and best practices for using R for data analysis\nR for Social Scientists - basic information about R syntax, the RStudio interface, how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting\n\n\n\n\n\nUsing git with MATLAB - Introduction into using MATLAB and version control with git\nProgramming with MATLAB - Lesson from the Software Carpentries on the basics of programming with MATLAB\n\n\n\n\n\nWriting tests - lesson from CodeRefinery on automated testing\nVideo testing lesson - recording from software testing workshop by Code Refinery\nModular coding - modular code development from Code Refinery\n\n\n\n\n\nInstalling Docker - installation instructions for Windows, macOS, and Linux\nInstall WSL2 update - manual WSL2 update for Windows\nDockerfile reference - information on how to write a Dockerfile\n\n\n\n\n\nSetting up VSCode for Linux - guide to getting started using VSCode with Windows Subsystem for Linux\n\n\n\n\n\nGitHub Actions introduction course - an introductory course from GitHub on how to use GitHub Actions\n\n\n\n\n\nReproducible Research material from Code Refinery - demonstrates how version control, workflows, containers, and package managers can be used to record reproducible environments and computational steps\nReproducible Research video lesson from Code Refinery - Recorded video lesson from Code Refinery workshop in May 2021 on Reproducible Research material\nData + Code + Software = PDF - Slides to an overview on how to integrate data and software into a PDF."
  },
  {
    "objectID": "docs/resources/curriculum.html#bash",
    "href": "docs/resources/curriculum.html#bash",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "What is Bash? - of all the shells available, Bash is one of the most popular, the most powerful, and the most friendly\nBash Essentials ‚Äì Bash commands commonly used to navigate file directories in your Terminal/GitBash\nThe Unix Shell lesson from Software Carpentries - use of the shell is fundamental to using a wide range of other powerful tools and computing resources. These lessons will start you on a path towards using these resources effectively\nInstallation Instructions - particularly important for Windows users, as Bash comes pre-installed on Mac. Windows users will need to install GitBash following the instructions at this link\nUsing the Terminal in Mac - The Terminal app allows you to control your Mac using a command prompt"
  },
  {
    "objectID": "docs/resources/curriculum.html#git",
    "href": "docs/resources/curriculum.html#git",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "What is Git?- 2-minute video overview of the technology and how it works\nInstall Git from GitHub Guides - Check if Git is already installed on your machine; if not, follow these instructions to get started. Notes: If you‚Äôve already installed GitBash on Windows OS, you will have Git already. Installing GitHub Desktop will also install the latest version of Git if you don‚Äôt already have it.\nInstalling Git from Software Carpentries - Alternative installation instructions from Software Carpentries, including videos and details per OS.\nIntro to version control with Git from Code Refinery ‚Äì self-paced introductory lesson to version control using Git\nGit Intro video lesson from Code Refinery - Day 1 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 1/2\nGit Intro video lesson from Code Refinery - Day 2 - Recorded lesson from a May 2021 Code Refinery workshop on material in Intro to version control with Git, part 2/2\nBranching and merging ‚Äì lesson from Code Refinery on concept of branching in Git (featuring octopus diagram)\nWhat is .gitignore? ‚Äì introduction to how and why to use the .gitignore file to not track some files in a project folder (e.g., because of their size or sensitivity)\nGit command cheat sheet ‚Äì commonly used Git commands in one page that can also be downloaded"
  },
  {
    "objectID": "docs/resources/curriculum.html#githubgitlab-remote-repositories",
    "href": "docs/resources/curriculum.html#githubgitlab-remote-repositories",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Understanding the GitHub flow ‚Äì guide from GitHub on how and why to work with branches\nCollaborative distributed version control - We have learned how to make a git repository for a single person. What about sharing?\nSSH connection to GitHub ‚Äì instructions to set up SSH connection to GitHub so that you do not need to input your login credentials with every push/pull\nGitlab and SSH keys - instructions to add an SSH key to your (TU Delft) GitLab account for the same reason as above\nGitHub without the Command Line from Code Refinery - practice collaborating and sharing using either the GitHub website or GitHub desktop application\nGitHub Guides: Mastering Markdown - Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform."
  },
  {
    "objectID": "docs/resources/curriculum.html#jupyter-notebooks-and-jupyterlab",
    "href": "docs/resources/curriculum.html#jupyter-notebooks-and-jupyterlab",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Introduction to Jupyter and JupyterLab - lesson material on the user interface of JupyterLab, how Jupyter notebooks work, and what some common and powerful usecases are"
  },
  {
    "objectID": "docs/resources/curriculum.html#anaconda-navigator-and-managing-conda-environments",
    "href": "docs/resources/curriculum.html#anaconda-navigator-and-managing-conda-environments",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Anaconda Installation Guide from Software Carpentries - Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with the latest version of Python and Jupyter Notebooks by default\nIntro to Anaconda Navigator - Anaconda Navigator is a graphical user interface to the conda package and environment manager. This 10-minute guide to Navigator will have you navigating the powerful conda program in a web-like interface without having to learn command line commands\nIntroduction to Conda for (Data) Scientists - Conda is an open source package and environment management system that easily creates, saves, loads, and switches between environments on your local computer\nManaging Conda environments - documentation on performing a range of common tasks with Conda using the command line"
  },
  {
    "objectID": "docs/resources/curriculum.html#python",
    "href": "docs/resources/curriculum.html#python",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Scientific Computing with Python - a free video course series that teaches the basics of using Python 3\nApplied Data Science with Python Specialization - Coursera course in which you can enroll for free\nLearnPython.org - Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the Python programming language\nProgramming with Python from Software Carpentries - this introduction to Python is built around a common scientific task: data analysis\nPlotting and Programming with Python from Software Carpentries - an introduction to programming in Python for people with little or no previous programming experience using plotting as its motivating example\nData Analysis and Visualization with Python for Social Scientists - basic information about Python syntax, the Jupyter notebook interface, how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python\nCan You Speak Python? - test your knowledge of some important features of the Python programming language and the NumPy and Pandas libraries"
  },
  {
    "objectID": "docs/resources/curriculum.html#pandas",
    "href": "docs/resources/curriculum.html#pandas",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Getting started with Pandas - documentation and quick start guide for Pandas, an essential Python library used for working with data sets. It has functions for analyzing, cleaning, exploring, and manipulating data\nPandas Tutorial - 14-part tutorial series featuring live code examples and tests of your knowledge\nPandas Data Wrangling Cheat Sheet - a cheat sheet of some of the most used syntax that you probably don‚Äôt want to miss\nPandas Cheat Sheet - Visual - visual, printable 2-page reference guide on commonly performed operations using Pandas\nUltimate Pandas Guide ‚Äî Inspecting Data Like a Pro - Whether you‚Äôre working on a simple analysis or a complex machine learning model, there‚Äôs a lot of value in being able to answer quick, exploratory questions about the nature of your data. This is a walk through of several DataFrame attributes and methods that make data inspection painless and productive\n10 Efficient Ways for Inspecting a Pandas DataFrame Object - A guide to using pandas effectively and efficiently"
  },
  {
    "objectID": "docs/resources/curriculum.html#plotly",
    "href": "docs/resources/curriculum.html#plotly",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Getting Started with Plotly - The plotly Python library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases\nPlotly Python Open Source Graphing Library - Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts\nHeatmaps with Plotly - How to make Heatmaps in Python with Plotly"
  },
  {
    "objectID": "docs/resources/curriculum.html#ipywidgets",
    "href": "docs/resources/curriculum.html#ipywidgets",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Ipywidgets documentation - ipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.\nIntroduction to ipywidgets - in this tutorial video, learn about ipywidgets, a Python library for building interactive HTML widgets for your Jupyter browser.\nIpywidgets Interact Function | ipywidgets Examples of Slider, Dropdown, Checkbox, Text Box - Video demo on how to make an ipywidgets slider, ipywidgets dropdown, ipywidgets checkbox, or an ipywidgets text box using Python code."
  },
  {
    "objectID": "docs/resources/curriculum.html#r",
    "href": "docs/resources/curriculum.html#r",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Install R guide from Software Carpentries - R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio, which must also be installed separately from here\nProgramming with R from Software Carpentries - this introduction to R is built around a common scientific task: data analysis\nR for Reproducible Data Analysis from Software Carpentries - write modular code and best practices for using R for data analysis\nR for Social Scientists - basic information about R syntax, the RStudio interface, how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting"
  },
  {
    "objectID": "docs/resources/curriculum.html#matlab",
    "href": "docs/resources/curriculum.html#matlab",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Using git with MATLAB - Introduction into using MATLAB and version control with git\nProgramming with MATLAB - Lesson from the Software Carpentries on the basics of programming with MATLAB"
  },
  {
    "objectID": "docs/resources/curriculum.html#modular-code-and-testing",
    "href": "docs/resources/curriculum.html#modular-code-and-testing",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Writing tests - lesson from CodeRefinery on automated testing\nVideo testing lesson - recording from software testing workshop by Code Refinery\nModular coding - modular code development from Code Refinery"
  },
  {
    "objectID": "docs/resources/curriculum.html#docker",
    "href": "docs/resources/curriculum.html#docker",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Installing Docker - installation instructions for Windows, macOS, and Linux\nInstall WSL2 update - manual WSL2 update for Windows\nDockerfile reference - information on how to write a Dockerfile"
  },
  {
    "objectID": "docs/resources/curriculum.html#vscode",
    "href": "docs/resources/curriculum.html#vscode",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Setting up VSCode for Linux - guide to getting started using VSCode with Windows Subsystem for Linux"
  },
  {
    "objectID": "docs/resources/curriculum.html#continuous-integration",
    "href": "docs/resources/curriculum.html#continuous-integration",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "GitHub Actions introduction course - an introductory course from GitHub on how to use GitHub Actions"
  },
  {
    "objectID": "docs/resources/curriculum.html#reproducible-research",
    "href": "docs/resources/curriculum.html#reproducible-research",
    "title": "Research Software Curriculum",
    "section": "",
    "text": "Reproducible Research material from Code Refinery - demonstrates how version control, workflows, containers, and package managers can be used to record reproducible environments and computational steps\nReproducible Research video lesson from Code Refinery - Recorded video lesson from Code Refinery workshop in May 2021 on Reproducible Research material\nData + Code + Software = PDF - Slides to an overview on how to integrate data and software into a PDF."
  },
  {
    "objectID": "docs/listing.html",
    "href": "docs/listing.html",
    "title": "References",
    "section": "",
    "text": "üöß Under construction! üèóÔ∏è"
  },
  {
    "objectID": "docs/infrastructure/moving_data.html",
    "href": "docs/infrastructure/moving_data.html",
    "title": "Moving data to remote servers",
    "section": "",
    "text": "This section describes how to transfer data to and from a TU Delft virtual server. The procedure is different depending on whether the server runs a Windows or Linux-based operating system. Although there are many ways to transfer data from one machine to another, TU Delft servers only support a few of them.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Moving data to remote servers"
    ]
  },
  {
    "objectID": "docs/infrastructure/moving_data.html#linux-servers",
    "href": "docs/infrastructure/moving_data.html#linux-servers",
    "title": "Moving data to remote servers",
    "section": "Linux servers",
    "text": "Linux servers\nThe scp command is a secure file transfer utility that allows you to copy files between Linux-based hosts (this includes macOS) on a network. It uses SSH for data transfer, providing the same authentication and security as SSH. Common scenarios in which you might need to transfer data between hosts include uploading scripts, downloading results, or transferring configuration files.\n\n\n\n\n\n\n Moving large data batches\n\n\n\nIf you need to transfer large files or a multitude of small files, consider using tools like rclone or rsync instead. The instructions below are for transferring small files or a few files at a time.\n\n\n\n\n\n\n\n\n Overwriting files\n\n\n\nNotice that scp will overwrite files in the destination directory without prompting if they already exist. Always double-check the paths and filenames to avoid accidental data loss.\n\n\n\nPrerequisites\nBefore starting, you need:\n\nSCP (Secure Copy Protocol) installed on your local machine. SCP is a command-line utility that allows you to securely transfer files between hosts on a network.\nSSH access to the remote host (e.g., VPS) you want to connect to.\n\n\n\nMoving data via SCP\nTo copy data to and from a remote host using the scp command, you can use the following syntax:\n# Copy TO Remote Host\nscp &lt;path-my-local-file&gt; &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/remote-directory&gt;/\n# Copy FROM Remote Host\nscp &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/my-remote-file&gt; &lt;path-my-local-directory&gt;/\n\n\n\n\n\n\n Moving files to restricted directories\n\n\n\nSome directories on the remote host may require elevated permissions to write files. If you encounter a ‚ÄúPermission denied‚Äù error, you may need to use sudo to copy files to those directories. However, using scp with sudo directly is not supported. Instead, you can copy the file to a temporary directory /tmp where you have write access and then move it to the desired location using sudo after connecting to the remote host.\n\n\n\n\nTransferring files using ProxyJump\nIn the case of a VPS hosted by TU Delft, you need to copy data to a remote host via a bastion host (an intermediary server). Therefore, you must use the -o option of scp to specify a ProxyJump that will connect to the bastion host first. Alternatively, you can choose to transfer files using SSH tunneling.\n\nTransfer to remote host\n# If using default SSH key name, for example, id_ed25519 or id_rsa\nscp -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" &lt;path-my-local-file&gt; \\ \n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path/remote-directory&gt;/\n\n# If using a custom SSH key name, for example, my_custom_key\nscp  -i &lt;path-to-custom-private-ssh-key&gt; -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\ \n&lt;path-my-local-file&gt;  &lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-directory&gt;/\n\n\nTransfer from remote host\n# If using default SSH key name, for example, id_ed25519 or id_rsa\nscp -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\ \n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/\n\n# If using a custom SSH key name, for example, my_custom_key\nscp -i &lt;path-to-custom-private-ssh-key&gt; -o \"ProxyJump &lt;bastion-username&gt;@linux-bastion-ex.tudelft.nl\" \\\n&lt;target-username&gt;@&lt;remote-host&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/\n\n\n\nTransferring files using SSH tunneling\nIf SSH tunneling has been configured correctly for the remote host, you can copy files to and from a remote host as follows:\n# Copy TO remote host\n$ &lt;path-my-local-file&gt; &lt;host-nickname&gt;:&lt;full-path-remote-directory&gt;/\n# Copy FROM remote host\n$ scp &lt;host-nickname&gt;:&lt;full-path-remote-file&gt;/ &lt;path-my-local-directory&gt;/",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Moving data to remote servers"
    ]
  },
  {
    "objectID": "docs/infrastructure/moving_data.html#windows-servers",
    "href": "docs/infrastructure/moving_data.html#windows-servers",
    "title": "Moving data to remote servers",
    "section": "Windows servers",
    "text": "Windows servers\nTransferring data to and from a TU Delft Windows server is done via the Citrix platform, using the app‚Äôs built-in menu shown in the image below.\n\n\n\nCitrix Menu. Buttons from left to right: Download, Upload, Multimonitor, Clipboard, and Settings.\n\n\n\nTransferring files\n\nOpen a web browser and log into your Windows server as usual via the Citrix portal.\nOpen the Citrix menu located at the center-top of the server window and click on the Upload or Download button, as shown in the image above.\nA pop-up window will open on where you can select the files you wish to transfer to the server.\n\nIt is not possible to directly transfer files to or from the server‚Äôs C: or D: drives. Instead, you upload or download files to your personal TU Delft drive which is connected to the server. Using the Windows File Explorer and standard copy/paste or drag-and-drop operations, you can transfer the data from your personal drive to the server‚Äôs C: or D: drives.\n\n\n\n\n\n\n Warning\n\n\n\nTU Delft Windows servers have a limited amount of disk space in the C: drive. ICT instructs users to install applications and store data in the D: drive of the server to avoid running out of memory. Alternatively, you can use your personal TU Delft drive, which is also connected to the server. More information can be found at the bottom of the TOPdesk form to request a new VPS.\n\n\n\n\nUsing the clipboard\nTU Delft Windows servers do not allow to directly copy or paste text from the clipboard. Instead, you must use Citrix‚Äôs clipboard functionality. To do so, open the Citrix‚Äôs menu located at the center-top of the window and click on the Clipboard button. A pop-up window will open on which you can copy or paste the text you wish to transfer. Note that it is only possible to transfer text via the clipboard; images or files are not supported.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Moving data to remote servers"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/runner_matlab.html",
    "href": "docs/infrastructure/gitlab/runner_matlab.html",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "",
    "text": "With the continuous method of software development, you continuously build, test, and deploy iterative code changes. This iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.\n\n\nWith this guide, you will create a Continuous Integration Pipeline on a repository within the TU Delft Gitlab to use a Matlab environment.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Setting up a Gitlab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/runner_matlab.html#background",
    "href": "docs/infrastructure/gitlab/runner_matlab.html#background",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "",
    "text": "With the continuous method of software development, you continuously build, test, and deploy iterative code changes. This iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.\n\n\nWith this guide, you will create a Continuous Integration Pipeline on a repository within the TU Delft Gitlab to use a Matlab environment.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Setting up a Gitlab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/runner_matlab.html#prerequisites",
    "href": "docs/infrastructure/gitlab/runner_matlab.html#prerequisites",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID\nMATLAB account\nBasic knowledge of Linux (for setting up a server)\nBasic knowledge of Docker (for creating a custom MATLAB image)\n\n\n\n\n\n\n\nTip\n\n\n\nTo learn more about Docker containers, please look at the Reproducible Computational Environments Using Docker lesson from the Software Carpentries.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Setting up a Gitlab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/runner_matlab.html#glossary-of-terms",
    "href": "docs/infrastructure/gitlab/runner_matlab.html#glossary-of-terms",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Glossary of terms",
    "text": "Glossary of terms\nCI/CD pipeline\nA CI/CD pipeline automates your software delivery process. The pipeline builds code, runs tests (Continuous Intergation), and safely deploys a new version of the application (Continuous Delivery).\nDocker\nWe use a Docker container to run the Gitlab runner and initialise the CI/CD pipeline.\nGitlab runner (from GitLab documentation)\nRunners are the agents that run the CI/CD jobs that come from GitLab. When you register a runner, you are setting up communication between your GitLab instance and the machine where GitLab Runner is installed. Runners usually process jobs on the same machine where you installed GitLab Runner.\nGitlab jobs\nPipeline configuration begins with jobs. Jobs are the most fundamental element of a .gitlab-ci.yml file. Each job is executed by a Gitlab runner. See Gitlab documentation for more info.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Setting up a Gitlab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/runner_matlab.html#steps",
    "href": "docs/infrastructure/gitlab/runner_matlab.html#steps",
    "title": "Setting up a GitLab runner for MATLAB",
    "section": "Steps",
    "text": "Steps\n\nRequest a TU Delft Virtual Private Server\nSet up a Gitlab runner\nCreate a Docker image with a custom Matlab installation\nRegister a gitlab runner for the Matlab container\nObtain a Matlab license file\nConfigure the CI/CD pipeline\nAdd a job to test the pipeline\nOptional: Updating the Matlab version\n\n\nStep 1. Request a TU Delft VPS\nIf you want to work with the TU Delft Gitlab instance and you want to implement CI/CD pipelines, then you need to install a Gitlab runner on your own. Runners are the agents that run the CI/CD jobs that come from GitLab. Currently, the TU Delft instance does not provide this feature out-of-the-box. Therefore, we need a separate (virtual) server to run the Gitlab runners and execute the jobs in the CI/CD pipeline.\nThe TU Delft offers Virtual Private Servers (VPS) for researchers through the TopDesk selfservice portal. If you don‚Äôt have a VPS already, please follow this guide to request a Virtual Private Server)\nVPS requirements\n\n50Gb disk space (the Matlab installation in this guide requires ~10 Gb, but this depends on the size of the installed addons)\n\n\n\nStep 2. Setting up Gitlab runners\nTo set up a gitlab runner on the VPS, please follow this guide for setting up GitLab runners.\nTLDR\n\nInstall docker with\nsudo apt install docker.io\nVerify installation with\nsudo docker --version\nOptional: Move default storage location to larger drive\nIf the file space in the Docker Root directory is not adequate, we must relocate the Docker Root. Please consult this guide for instructions.\nDeploy the gitlab-runner with\ndocker run -d --name gitlab-runner --restart always \\\n-v /srv/gitlab-runner/config:/etc/gitlab-runner \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\ngitlab/gitlab-runner:latest\nVerify deployment with\nsudo docker ps -a\n\n\n\nStep 3. Create a Docker image containing a custom Matlab installation\nIn order for a Gitlab runner to execute MATLAB code, it needs to be able to access a container with MATLAB installed. The aim of this step is to create a Docker image with MATLAB installation that can be used by a Gitlab runner. By building our own Docker image, we can specify the MATLAB version and customize the installed toolboxes.\n\n\n\n\n\n\nNote\n\n\n\nWe have looked into using the Docker images developed by Mathworks. When running these images, you are prompted to supply your MATLAB‚Äôs account username and password to activate the instance. Although it is possible to create a new image from such an activated container and use it on the VPS, we have so far not been able to get this solution working with Gitlab runners. We thus rely on downloading a license file (step 6) and storing it as a Variable on Gitlab (step 7).\n\n\nThis Dockerfile is based on MATLAB‚Äôs Dockerfile template. We will make the following modifications to this template:\n\nset bash as the default run command (Gitlab runners need to access a shell)\nadd additional MATLAB products with the flag --products. In this example, we have added the Parallel Computing Toolbox and the Mapping Toolbox.\n\nIn your user folder on the VPS (/home/username), create a file called Dockerfile\nsudo nano Dockerfile\nand copy the content below in the Dockerfile. Make sure to update the MATLAB release and installed addons to your requirements (see in bold).\n\n\n\n\n\n\n# Copyright 2019 - 2021 The MathWorks, Inc.\n\n# To specify which MATLAB release to install in the container, edit the value of the MATLAB_RELEASE argument.\n# Use lower case to specify the release, for example: ARG MATLAB_RELEASE=r2020a\nARG MATLAB_RELEASE=r2021b\n\n# When you start the build stage, this Dockerfile by default uses the Ubuntu-based matlab-deps image.\n# To check the available matlab-deps images, see: https://hub.docker.com/r/mathworks/matlab-deps\nFROM mathworks/matlab-deps:${MATLAB_RELEASE}\n\n# Declare the global argument to use at the current build stage\nARG MATLAB_RELEASE\n\n# Install mpm dependencies\nRUN export DEBIAN_FRONTEND=noninteractive && apt-get update && \\\n    apt-get install --no-install-recommends --yes \\\n    wget \\\n    unzip \\\n    ca-certificates && \\\n    apt-get clean && apt-get autoremove\n\n# Run mpm to install MATLAB in the target location and delete the mpm installation afterwards\nRUN wget -q https://www.mathworks.com/mpm/glnxa64/mpm && \\ \n    chmod +x mpm && \\\n    ./mpm install \\\n    --release=${MATLAB_RELEASE} \\\n    --destination=/opt/matlab \\\n    --products MATLAB Parallel_Computing_Toolbox Mapping_Toolbox && \\\n    rm -f mpm /tmp/mathworks_root.log && \\\n    ln -s /opt/matlab/bin/matlab /usr/local/bin/matlab\n\n# Add \"matlab\" user and grant sudo permission.\nRUN adduser --shell /bin/bash --disabled-password --gecos \"\" matlab && \\\n    echo \"matlab ALL=(ALL) NOPASSWD: ALL\" &gt; /etc/sudoers.d/matlab && \\\n    chmod 0440 /etc/sudoers.d/matlab\n\n# Set user and work directory\nUSER matlab\nWORKDIR /home/matlab\nCMD [\"bash\"]\n\n\n\n\nTo build a Docker image with the name matlab-gitlab and the version reference r2021b, run the following command in the folder containing the Dockerfile:\nsudo docker build . -t matlab-gitlab:r2021b\nYou can verify the presence of the image with\nsudo docker images\nThis image is now available locally on the VPS.\n\n\n\n\n\n\nTip\n\n\n\nYou can also upload your Docker image to Dockerhub and have it available from there. This removes the need to build the image on the VPS as it can be pulled directly from DockerHub.\n\n\n\n\nStep 4. Register the MATLAB runner\nAfter deploying the gitlab-runner in step 2, we need to register a new runner for our matlab-gitlab image. Run the following command to register your runner and configure it to deploy in a Docker container on your server.\ndocker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register\nIn response to this command you will be prompted to answer a series of questions. You can find the required gitlab-ci token in your Gitlab repository under Settings -&gt; CI/CD -&gt; Runners:\nsudo docker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register \\\n  --non-interactive \\\n  --url \"https://gitlab.tudelft.nl/\" \\\n  --registration-token \"REPOSITORY_TOKEN\" \\\n  --executor \"docker\" \\\n  --docker-image matlab-gitlab:r2021b \\\n  --description \"matlab-runner\" \\\n  --tag-list \"matlab\" \\\n  --docker-privileged=true \\\n  --docker-cap-add \"NET_ADMIN\" \\\n  --docker-pull-policy \"if-not-present\" \\\nFor the changes to take effect, restart the gitlab-runner with\nsudo docker restart gitlab-runner\nThe runner configurations are stored in /srv/gitlab-runner/config/config.toml. If you would like to view or or modify the MATLAB runner, run\nsudo nano /srv/gitlab-runner/config/config.toml\nAfter registering the runner, the configuration file should contain:\n\n\n\n\n\n\nNote\n\n\n\n\n\nconcurrent = 4\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = \"matlab-gitlab\"\n  url = \"https://gitlab.tudelft.nl\"\n  token = \"&lt;token&gt;\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"matlab-gitlab:r2021b\"\n    privileged = true\n    disable_entrypoint_overwrite = false\n    cap_add = [\"NET_ADMIN\"]\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/cache\"]\n    pull_policy = \"if-not-present\"\n    shm_size = 0\n\n\n\n\n\nStep 5. Obtain a MATLAB license file\nEvery TU Delft employee has access to an Individual MATLAB license. Normally, you would activate MATLAB only once after installation through an online activation step. However, this does not work for a Docker container as it is relaunched for each CI trigger.\nThe following steps for activating MATLAB on an offline machine are adapted from the MATLAB Forum:\n\nObtain your Host ID\nObtain your computer login name or username\nActivate the license through the License Center to obtain license file\n\n1. Obtain your Host ID\nThe MATLAB license can only be activated for a specifc computer. In the Docker container, we will set the hostID of the container to 0242ac11ffff.\n\n\n\n\n\n\nNote\n\n\n\nDocker automatically assigns an IP address to each running container, starting from 172.17.0.2 until 172.17.0.255. These IP addresses determine the container‚Äôs MAC address (see here for more details), which in turn needs to match with our license. To prevent the MAC address of the MATLAB container from switching and thereby invalidating the license, we will set it to 02:42:ac:11:ff:ff in the .gitlab-ci.yml file.\n\n\n2. Obtain your computer login name or username\nThe MATLAB license is created for a specific user. In the Docker container, we will set the username to matlab.\n3. Activate the license through the License Center to obtain license file\n\nGo to the License Center: https://www.mathworks.com/mwaccount\nUnder My Software, click the license number you want to activate. If you do not see your license number, in the bottom right hand corner, click View Additional Licenses or Trials.\nClick the Install and Activate tab\nClick Activate to Retrieve License File and/or Activate a Computer\nEnter the following information:\n\nthe release you are activating = r2021b (same version as in the Dockerfile)\nthe operating system = Linux\nthe host ID = 0242ac11ffff\nyour user or login name = matlab\nthe Activation Label = matlab-gitlab\n\n\nDownload the license.lic file\n\n\n\nStep 6. Configure the CI/CD pipeline on Gitlab\nBefore we can run a CI job, we need to configure a few settings in our Gitlab repository\n1. Add tag to MATLAB runner\nUnder Settings -&gt; CI/CD -&gt; Runners we can find the available specific runners. Press the edit button on the matlab-gitlab runner and add the tag matlab-gitlab. With this, we can call more easily call this specific runner within our CI pipeline.\n2. Add license as Variable\nUnder Settings -&gt; CI/CD -&gt; Variables add a new variable called MATLAB_LICENSE, past the content of the downloaded license.lic file and set type to file. Having the license available as a Gitlab variable allows us to update it without having to change the MATLAB image.\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we could have added the license file directly to the Docker image. With the license file in the same folder as the Dockerfile and adding the following command to the Dockerfile, we can build a Docker image with an activated MATLAB:\nCOPY license.lic /opt/matlab/licenses/\nHere, we opted to have it accessible through the Gitlab settings together with the accompanying hostid.\n\n\n\n\n\n\nWarning\n\n\n\nNever share any Docker images that contain license files or other confidential information.\n\n\n\n\n\n\nStep 7. Add a job to test the pipeline\nTo test the pipeline, add the following content to .gitlab-ci.yml via CI/CD -&gt; Editor in your repository.\nvariables:\n  MAC_ADDRESS: 02:42:ac:11:ff:ff\n\ncheck_matlab:\n  tags: \n    - matlab-gitlab\n  before_script:\n    # Change the mac-address to match the MATLAB license\n    - sudo ifconfig eth0 hw ether \"$MAC_ADDRESS\"\n\n    # Add the Matlab license to the Matlab installation in the container\n    - sudo mkdir /opt/matlab/licenses\n    - sudo mv ${MATLAB_LICENSE} /opt/matlab/licenses/license.lic   \n  script:    \n    # Run a MATLAB function/script through the -batch argument\n    - matlab -batch \"disp('hello world!')\"\nAfter commiting, the pipeline should run and execute the job check_matlab. You can check the status of the pipeline via CI/CD -&gt; Pipelines.\nIf all went well, you have successfully setup a Gitlab runner to run MATLAB code. Congrats!\n\n\nStep 8. Optional: Updating the MATLAB version\nIf you need to update the MATLAB version of the Docker container, you will need to go throught the following steps:\n\nUpdate the MATLAB version in the Dockerfile\nBuild the docker image with sudo docker build . -t matlab-gitlab:&lt;version&gt;\nDownload a new license.lic file (see step 5 of this guide)\nUpdate the CI Variable MATLAB_LICENSE with the new license content\nUpdate the image names (not the tags) in .gitlab-ci.yml to use the new image.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to test your code with multiple MATLAB versions to ensure backward compatibility, please look at this example to use multiple docker images.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Setting up a Gitlab runner for MATLAB"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_intro.html",
    "href": "docs/infrastructure/gitlab/gitlab_intro.html",
    "title": "TU Delft GitLab",
    "section": "",
    "text": "Imagine the following: you are working with a group on a research software code base. At this point your codebase might be quite large, dozens of scripts, and it has a good amount of dependencies. Furthermore, other researchers depend on your code to work properly for their own research. When your code becomes more relevant to yourself and your community, you will feel the urge to have more control on the quality of contributions. You would like to be able to easily upgrade and maintain the code, but also automate the process of packaging and publishing your code, instead of doing it manually everytime.\nThe TU Delft offers a local instance of GitLab at gitlab.tudelft.nl. GitLab is an online Git repository management tool with a wiki, issue tracker, Continuous Integration and Continuous Deployment built-in. The service is intended for researchers. Similar services are, for example, GitHub.com or GitLab.com. In contrast to these services, GitLab TU Delft is hosted by the TU Delft itself, on campus. For more information, please consult the documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_intro.html#github-or-tu-delft-gitlab",
    "href": "docs/infrastructure/gitlab/gitlab_intro.html#github-or-tu-delft-gitlab",
    "title": "TU Delft GitLab",
    "section": "GitHub or TU Delft GitLab?",
    "text": "GitHub or TU Delft GitLab?\nThe current instance of the TU Delft GitLab has a few limitations:\n\nHosting a website through pages is currently deactivated\nContinuous integration is not available by default. See our guide on setting this up.\nContainer registry has been disabled\n‚Ä¶\n\nThe free edition of GitLab has the following limitations:\n\nWiki is not available in a private repository\n‚Ä¶",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html",
    "title": "CI with Gitlab",
    "section": "",
    "text": "If you happen to be working with TU Delft GitLab instance and you want to implement DevOps or CI/CD pipelines, then you need to install a GitLab runner on your own. This should runner should be in a server, responding to changes such as commits or pull requests in your GitLab repository.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#background",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#background",
    "title": "CI with Gitlab",
    "section": "",
    "text": "If you happen to be working with TU Delft GitLab instance and you want to implement DevOps or CI/CD pipelines, then you need to install a GitLab runner on your own. This should runner should be in a server, responding to changes such as commits or pull requests in your GitLab repository.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#quick-overview-of-how-it-works",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#quick-overview-of-how-it-works",
    "title": "CI with Gitlab",
    "section": "Quick overview of how it works",
    "text": "Quick overview of how it works\nIn order to be ready to run CI/CD pipeline, a gitlab-runner Docker container is running on the server all the time. When a new commit is made in the GitLab repository, this triggers the CI/CD process to run a job (e.g., unit test) based on the pipeline defined in the .gitlab-ci.yml file in the repository. The container used to carry out the CI/CD tests is defined in the .gitlab-ci.yml file in the first line, and spawned from within the continuously running gitlab-runner container. In our example, we define image:python:3.12.3 so every time a commit is made in the repository, a new container based on the python:3.12.3 Docker image is started and used to run tests on the python scripts and generate artifacts as defined in the .gitlab-ci.yml file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#what-this-documentation-will-help-achieve",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#what-this-documentation-will-help-achieve",
    "title": "CI with Gitlab",
    "section": "What this documentation will help achieve",
    "text": "What this documentation will help achieve\nThe documentation below will help you deploy GitLab runner in a Docker container on a server to automatically run CI/CD tests and store artifacts every time there is a new commit to a GitLab repository.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#prerequisites",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#prerequisites",
    "title": "CI with Gitlab",
    "section": "Prerequisites",
    "text": "Prerequisites\nServer: This example uses a server to run the whole process. You can request a server from TU Delft ICT service following these directions here. It is useful to set this up on a server so that Docker can be running continuously, and be ready to run CI/CD tests whenever a new commit occurs in the repository.\nDocker: We use a Docker container to run the GitLab runner and initialise the CI/CD pipeline.\nGitlab runner: (from GitLab documentation) ‚ÄúRunners are the agents that run the CI/CD jobs that come from GitLab. When you register a runner, you are setting up communication between your GitLab instance and the machine where GitLab Runner is installed. Runners usually process jobs on the same machine where you installed GitLab Runner.‚Äù Link\nGitLab repository: A remote repository that can store your code and keeps track of your project development. You‚Äôre on one right now! :) If you haven‚Äôt already, you use your netID and password to login to TU Delft‚Äôs GitLab instance at gitlab.tudelft.nl and create a repository containing your project code.\nCI/CD pipeline: ‚ÄúA CI/CD pipeline automates your software delivery process. The pipeline builds code, runs tests (CI), and safely deploys a new version of the application (CD)‚Äù.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#toolssoftware",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#toolssoftware",
    "title": "CI with Gitlab",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nGitLab (TU Delft instance)\nDocker\ngitlab-runner Docker image",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_docker.html#steps",
    "href": "docs/infrastructure/gitlab/gitlab_docker.html#steps",
    "title": "CI with Gitlab",
    "section": "Steps",
    "text": "Steps\n\nRequest the server\nConnect to the server via ssh\nInstall Docker on the server\nPull in the gitlab-runner image\nCreate a unit test function stored as a file in the repository\nMake the .gitlab-ci.yml file\nSet up the GitLab runner\nDeploy the GitLab runner in a Docker container\nRegister the runner\nTest the CI/CD pipeline\n\n\nStep 1. Request server running Ubuntu\nIf you don‚Äôt have a VPS already, you can request one from TU Delft ICT. Instructions for requesting a server and storage from ICT can be found under Remote servers/Request a Virtual Private Server.\nWe recommend the following configuration for configuring a GitLab runner:\n\nBasic Configuration 4 (Ubuntu)\nNo additional ports need to be configured for deploying a GitLab runner with Docker.\nAdditional space if your Docker images are larger than ~10Gb.\n\n\n\nStep 2. Connect to the server via ssh\nThe email response from Sysadmin@TUDelft.nl confirming the successful deployment of your server should contain instructions to connect via ssh.\nThe default login procedure is to connect to the Bastion host (an intermediary server) and then to your server, so it is a two-step process. Please check your email for Steps A and B as described by ICT admin.\nYou can also connect to your server using Putty (Windows) or, on Mac/Linux, configure one-step access by storing ssh keys between your local machine and your server and designating an alias.\nWhen you are successfully connected, you should see in your terminal/command prompt something like this:\n\n\n\nStep 3. Install Docker on the server\nServers often do not come with Docker installed, so you may need to do it by yourself. To check whether Docker is installed, run docker --version. If you get an error message, you can install it using the following commands:\nsudo su (this will give you the root access) apt install docker.io\nNow, Docker is installed. You can check if it is installed successfully by again typing docker --version in the terminal. The result should show the version of Docker you just installed.\n\n\nStep 4. Pull the gitlab-runner Docker image\nIn order to run CI/CD jobs for your repository, you need to install GitLab Runner. GitLab Runner is an application that works with GitLab CI/CD to run jobs in a pipeline. Rather than install GitLab Runner directly on the server, we will run a lightweight version of it as a Docker container. To do so, we first need to pull the gitlab-runner Docker image by running:\ndocker pull gitlab/gitlab-runner\nYou can check whether it was successful by running docker images - you should see the gitlab/gitlab-runner image listed in the output.\n\n\nStep 5. Create a unit test function stored as a file in the repository\n\n\nStep 6. Set up the CI by configuration of the .gitlab-ci.yml file\nThis file in the root of your repository defines what CI/CD tests to run upon each new commit. It does this by prescribing what container to run, what scripts to run inside the container, and what things to store as artefacts.\nIn the first line of the file, you can write image: &lt;image_name&gt;:&lt;tag&gt; to indicate you want to run the runner in a Docker container container. &lt;image_name&gt; should be replaced by the name of the image you want to use to start a container and &lt;tag&gt; should be replaced by the tag of the image you want to use(image names can be found on DockerHub). Tag will be set to latest by default.\nImportant: The file must be named exactly .gitlab-ci.yml so it will be recognizable by GitLab runner. Below is an example .gitlab-ci.yml file.\n# This is a sample build configuration for Python.\n# Check our guides at https://confluence.atlassian.com/x/x4UWN for more examples.\n# Only use spaces to indent your .yml configuration.\n# -----\ntest:\n  # You can specify a custom docker image from Docker Hub as your build environment.\n  image: python:3.12.3\n  cache:\n    paths:\n      - .cache/pip\n      - venv/\n  script: # Modify the commands below to build your repository.\n    - pip install -r requirements.txt\n    - nosetests --with-coverage  --cover-html\n  artifacts:\n    paths:\n      - cover/\nTags can be added as a final section in your .gitlab-ci.yml file (they are optional, we didn‚Äôt enter any in this example). You then need to enter these tags exactly when registering the runner.\n\n\nStep 7. Setup the GitLab runner\n\nFollow the instructions here till step 7 to create a GitLab runner for your repository\nChoose ‚ÄòLinux‚Äô under operating systems\nCopy the authentication token generated and keep it handy. It will be required in Step 9.\n\n\n\nStep 8. Deploy GitLab runner in a Docker container\ndocker run -d --name gitlab-runner --restart always \\\n-v /srv/gitlab-runner/config:/etc/gitlab-runner \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\ngitlab/gitlab-runner:latest\non Mac OS, use /Users/Shared/ instead of /srv/\nCheck that gitlab-runner container is running using docker ps -a\n\n\nStep 9. Register the runner using authentication token\nRun the following command to register your runner and configure it to deploy in a Docker container on your server.\ndocker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register\non Mac OS, use /Users/Shared/ instead of /srv/\nIn response to this command you will be prompted to answer the following questions\n- GitLab URL: https://gitlab.tudelft.nl\n- gitlab-ci token: Paste the authentication token generated in Step 7.\n- Enter name of the runner: example-runner\n- Type of executor: docker\n- Default Docker image: Specify the same image as the one specified in the 'image' field of the .gitlab-ci.yml file.\nSee an example below.\ndocker run --rm -it -v /srv/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner register\nEnter the GitLab instance URL (for example, https://gitlab.com/):\nhttps://gitlab.tudelft.nl\nEnter the registration token:\nxxxxxxxxxxxxxxx\nVerifying runner... is valid                        runner=xxxxxxx\nEnter a name for the runner. This is stored only in the local config.toml file:\n[xxxxxxx]: example-runner\nEnter an executor: instance, kubernetes, docker-windows, docker-autoscaler, parallels, shell, ssh, virtualbox, docker+machine, custom, docker:\ndocker\nEnter the default Docker image (for example, ruby:2.7):\njulia:1.6\nRunner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded!\n\nConfiguration (with the authentication token) was saved in \"/etc/gitlab-runner/config.toml\"\n\n\nStep 10. Check that the CI/CD process is activated and your pipeline runs successfully\n\nIn your repository, navigate to Settings &gt; CI/CD &gt; Click on Expand under Runners: You should see that the runner is active as indicated by a green dot. This means that the CI pipeline is ready to run, but it needs to be triggered.\nIn order to trigger the CI pipeline, you should make a new commit to the GitLab repository.\nAfter you have made a new commit to the repository, navigate to Your Project -&gt; Build -&gt; Pipelines to check the status of CI/CD pipelines connected to your repository. If you find a green message that says ‚Äúpassed‚Äù with a check mark then congratulations, your pipeline works! If you see a red message that says ‚Äúfailed‚Äù, check to see the error message associated with it - sometimes you need to reconfigure your .gitlab-ci.yml file to make sure it uses the correct formatting and defines the tests appropriately.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Continuous Integration with GitLab"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html",
    "href": "docs/infrastructure/apache_webserver.html",
    "title": "Set up an Apache web server",
    "section": "",
    "text": "If you want to host a website for your lab, or a web application of some sort, you are going to need to work with a webserver. Apache Web Server is a software package that turns a computer into an HTTP server. That is, it sends web pages ‚Äì stored as HTML files ‚Äì to people on the internet who request them. It is open-source software, which means it can be used and modified freely.\n\nThe job of a web server is to serve websites on the internet. To achieve that goal, it acts as a middleman between the server and client machines. It pulls content from the server on each user request and delivers it to the web‚Ä¶..One of the most popular web servers, Apache allows you to run a secure website without too much of a headache. It is free and open-source, making it a frequent choice of solopreneurs and small businesses who want a presence on the web‚Ä¶..The way Apache HTTP server works is that it will accept requests from the web browser, such as Google Chrome and Microsoft Edge, and turn programming scripts into web pages which contents are visible by the visitors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#background",
    "href": "docs/infrastructure/apache_webserver.html#background",
    "title": "Set up an Apache web server",
    "section": "",
    "text": "If you want to host a website for your lab, or a web application of some sort, you are going to need to work with a webserver. Apache Web Server is a software package that turns a computer into an HTTP server. That is, it sends web pages ‚Äì stored as HTML files ‚Äì to people on the internet who request them. It is open-source software, which means it can be used and modified freely.\n\nThe job of a web server is to serve websites on the internet. To achieve that goal, it acts as a middleman between the server and client machines. It pulls content from the server on each user request and delivers it to the web‚Ä¶..One of the most popular web servers, Apache allows you to run a secure website without too much of a headache. It is free and open-source, making it a frequent choice of solopreneurs and small businesses who want a presence on the web‚Ä¶..The way Apache HTTP server works is that it will accept requests from the web browser, such as Google Chrome and Microsoft Edge, and turn programming scripts into web pages which contents are visible by the visitors.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#what-this-documentation-will-help-achieve",
    "href": "docs/infrastructure/apache_webserver.html#what-this-documentation-will-help-achieve",
    "title": "Set up an Apache web server",
    "section": "What this documentation will help achieve",
    "text": "What this documentation will help achieve\nThis guide will help you install the Apache web server on Ubuntu Linux and configure a HTTPS secure connection for all incoming web traffic.\nThe steps outlined below will ensure that all incoming web traffic to your server from port 80 (HTTP) will be redirected to port 443 (HTTPS). Port 80 is still accessible but redirects automatically. Redirection is configured after the SSL certificate is in place.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#prerequisites",
    "href": "docs/infrastructure/apache_webserver.html#prerequisites",
    "title": "Set up an Apache web server",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA system running Ubuntu Server\nAn internet connection\nAccess to a user account with sudo privileges",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#tools-software",
    "href": "docs/infrastructure/apache_webserver.html#tools-software",
    "title": "Set up an Apache web server",
    "section": "Tools / Software",
    "text": "Tools / Software\n\nA command-line utility (Use keyboard shortcut CTRL-ALT-T, or right-click the desktop and left-click Open Terminal)\nA firewall ‚Äì the default UFW (Uncomplicated Firewall) in Ubuntu is fine\nThe APT package manager, installed by default on Ubuntu",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#steps",
    "href": "docs/infrastructure/apache_webserver.html#steps",
    "title": "Set up an Apache web server",
    "section": "Steps",
    "text": "Steps\n\nRequest a Virtual Private Server (VPS) from TU Delft ICT\nInstall Apache web server on the VPS\nRequest an SSL certificate from TU Delft ICT\nConfigure the SSL certificate file on your VPS\nRedirect all incoming web traffic to HTTPS\n\n\nStep 1. Request a Virtual Private Server from TU Delft ICT\nYou can make a request for a server via the TopDesk self service portal. If you would like more information, documentation can be found here ___\nNote: to run a web server, ports 80 (HTTP) and 443 (HTTPS) must be opened.\n\n\nStep 2. Install Apache web server on the VPS\nApache is an open source web server that‚Äôs available for Linux servers. It is one of the most commonly used softwares for creating a web server. And, it‚Äôs free!\nInstalling Apache can be done using commands in the terminal. These instructions are documented with further information at the link in the beginning of this paragraph. First, make sure your local software packages are up to date by running:\nsudo apt-get update\nWhen that has finished, install the Apache package using:\nsudo apt-get install apache2\nThe system prompts for confirmation ‚Äì do so, and allow the system to complete the installation.\nFind the IP address of your VPS by running:\nhostname -I | awk '{print $1}'\nUse this IP address to enter the following command in your terminal, replacing local.server.ip with the actual IP address of your server:\nhttp://local.server.ip\nIf the installation was completed successfully, you should see the Apache2 Ubuntu Default page in your web browser.\nAlthough the Apache installation process is complete, there is one more additional step. Configure the default UFW firewall to allow traffic on port 80.\nStart by displaying available app profiles on UFW:\nsudo ufw show app list\nUse the following command to allow normal web traffic on port 80:\nsudo ufw allow 'Apache Full'\nVerify the changes by checking UFW status:\nsudo ufw status\nAt this point, your Apache web server is serving over HTTP, which is a good first step! But remember, we need to secure the connection over HTTPS. So, we still have a few more steps.\n\n\nStep 3. Request an SSL certificate from TU Delft ICT\nDetailed directions on how to do this can be found here ___\nYou can create the .csr file directly on the VPS by first create a directory on /etc/apache2 called ssl:\nmkdir /etc/apache2/ssl\nThen, generate a CSR and private key using:\nopenssl req -x509 -newkey rsa:4096 -keyout &lt;server_domain&gt;.key -out &lt;server_domain&gt;.csr -nodes\nAfter successfully running the command it will ask for the information of certificate request. Complete it using the appropriate information and then .key and .csr files will be generated.\nThe .csr file must be sent to TU Delft ICT using this TopDesk form: TOPdesk SSL certificate server request.\n\n\nStep 4. Configure the SSL certificate on your VPS\nWhen you receive SSL certificate files from the signing authority via TU Delft ICT, you need to put the information from this certificate in a specific place on your VPS in order to securely expose the web server. These instructions come from here: - Configure ssl for https\nNavigate to the default Apache site config directory using the following command:\nsudo nano /etc/apache2/sites-available/default-ssl.conf\nThis config file tells the server where to find SSL certificate. It should look like this:\n                &lt;IfModule mod_ssl.c&gt;\n                &lt;VirtualHost _default_:443&gt;\n                ServerAdmin webmaster@localhost\n\n                DocumentRoot /var/www/html\n\n                ErrorLog ${APACHE_LOG_DIR}/error.log\n                CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n                SSLEngine on\n\n                SSLCertificateFile    /etc/ssl/certs/ssl-cert-snakeoil.pem\n                SSLCertificateKeyFile /etc/ssl/private/ssl-cert-snakeoil.key\n\n                &lt;FilesMatch \".(cgi|shtml|phtml|php)$\"&gt;\n                SSLOptions +StdEnvVars\n                &lt;/FilesMatch&gt;\n                &lt;Directory /usr/lib/cgi-bin&gt;\n                SSLOptions +StdEnvVars\n                &lt;/Directory&gt;\n\n                &lt;/VirtualHost&gt;\n                &lt;/IfModule&gt;\nEdit this: ServerAdmin webmaster@localhost to this: ServerAdmin email@example.net\nAdd this right below the ServerAdmin line:\n                ServerName ADD_YOUR_IP_OR_DOMAIN_NAME_HERE\nNow, edit these lines with our certificate location:\n                SSLCertificateFile    /etc/apache2/ssl/apache.crt\n                SSLCertificateKeyFile /etc/apache2/ssl/apache.key\nOur file should look like this:\n                &lt;IfModule mod_ssl.c&gt;\n                &lt;VirtualHost _default_:443&gt;\n                ServerAdmin email@example.net\n                ServerName 203.0.113.122\n\n                DocumentRoot /var/www/html\n\n                ErrorLog ${APACHE_LOG_DIR}/error.log\n                CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n                SSLEngine on\n\n                SSLCertificateFile    /etc/apache2/ssl/apache.crt\n                SSLCertificateKeyFile /etc/apache2/ssl/apache.key\n\n                &lt;FilesMatch \".(cgi|shtml|phtml|php)$\"&gt;\n                SSLOptions +StdEnvVars\n                &lt;/FilesMatch&gt;\n                &lt;Directory /usr/lib/cgi-bin&gt;\n                SSLOptions +StdEnvVars\n                &lt;/Directory&gt;\n\n                &lt;/VirtualHost&gt;\n                &lt;/IfModule&gt;\nSave the file, and close it.\nNote: If you are using something other than Apache Web Server (like, nginx for example) you can also create the SSL config file from scratch. Each application will have different syntax that should be used in this file. You can see how the syntax is set up by using this tool\nEnable the SSL module using following command:\nsudo a2enmod ssl\nNow enable the site we have just edited:\nsudo a2ensite default-ssl.conf\nRestart Apache:\nsudo service apache2 restart\nThe website is now secure, access it using following address in the browser\nhttps://YOUR_SERVER_IP\n\n\nStep 5. Redirect all incoming web traffic to HTTPS\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod rewrite\nAdd in apache conf:\n                ProxyPass /thredds http://localhost:8080/thredds\n                ProxyPassReverse /thredds http://localhost:8080/thredds\n                RedirectMatch ^/$ /thredds/",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/apache_webserver.html#notes-and-next-steps",
    "href": "docs/infrastructure/apache_webserver.html#notes-and-next-steps",
    "title": "Set up an Apache web server",
    "section": "Notes and Next Steps",
    "text": "Notes and Next Steps\nTest that your web server is secured by HTTPS by typing the Fully Qualified Domain Name (FQDN) of your server in a web browser. If HTTPS is enabled, the URL should begin with it - if it still says HTTP, something will need to be reconfigured.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up an Apache web server"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html",
    "href": "docs/infrastructure/VPS_SSL_Certs.html",
    "title": "Configure SSL certificates",
    "section": "",
    "text": "It is common practice to have web servers serving content over HTTPS, which is the secure version of HTTP. In order to do this and make the connection secure, you need an SSL certificate from a certificate signing authority.\nThe role of the SSL certificate is to indicate the encryption of user data from a server exposed on the web. SSL is a key for encrypting information. When a website‚Äôs certificate is expired or invalid, or if it using a self-signed (unofficial) certificate, as a user you either get a warning saying, ‚Äúgo back to safety,‚Äù or can‚Äôt access the website over https - instead you see a message that says ‚ÄúNot Secure‚Äù in your web browser. This means that the certificate is not signed by an authority, so it‚Äôs not trusted.\nIn theory, if your web server is universally accessible and doesn‚Äôt contain forms for users with personal or confidential information, there is no strict need for HTTPS connection. But, HTTPS is the modern standard and without it, visitors will have impression that the website is not safe. Therefore it is good practice to always have HTTPS on your web server.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#background",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#background",
    "title": "Configure SSL certificates",
    "section": "",
    "text": "It is common practice to have web servers serving content over HTTPS, which is the secure version of HTTP. In order to do this and make the connection secure, you need an SSL certificate from a certificate signing authority.\nThe role of the SSL certificate is to indicate the encryption of user data from a server exposed on the web. SSL is a key for encrypting information. When a website‚Äôs certificate is expired or invalid, or if it using a self-signed (unofficial) certificate, as a user you either get a warning saying, ‚Äúgo back to safety,‚Äù or can‚Äôt access the website over https - instead you see a message that says ‚ÄúNot Secure‚Äù in your web browser. This means that the certificate is not signed by an authority, so it‚Äôs not trusted.\nIn theory, if your web server is universally accessible and doesn‚Äôt contain forms for users with personal or confidential information, there is no strict need for HTTPS connection. But, HTTPS is the modern standard and without it, visitors will have impression that the website is not safe. Therefore it is good practice to always have HTTPS on your web server.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#what-this-documentation-will-help-achieve",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#what-this-documentation-will-help-achieve",
    "title": "Configure SSL certificates",
    "section": "What this documentation will help achieve",
    "text": "What this documentation will help achieve\nSSL certificates for TU Delft researchers and staff may be requested by contacting the ICT service desk. TU Delft ICT will order SSL certificates on your behalf from a trusted certificate signing authority.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#prerequisites",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#prerequisites",
    "title": "Configure SSL certificates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID\nLinux-based VPS provided by TU Delft ICT\n(optional) 1-step SSH connection established (see instructions ___)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#toolssoftware",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#toolssoftware",
    "title": "Configure SSL certificates",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nNone",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#steps",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#steps",
    "title": "Configure SSL certificates",
    "section": "Steps",
    "text": "Steps\n\nGenerate .csr file\nSecure-copy .csr file from server to local machine\nSubmit Certificate Server Request to TU Delft ICT via TopDesk form\n\n\nStep 1. Generate .csr file\nThe Certificate Signing Request (.csr) file is a file generated by you in a standard format that contains all the information the signing authority needs to create a signed certificate. You will need to include this .csr file in submitting your SSL certificate request form to TU Delft ICT, and you should generate it on your VPS directly. The instructions for generating a .csr file come from here.\nSSH to your VPS (____ link if you have followed the process to connect directly, you can use: username@localmachine ~ % ssh externalserveralias)\nEnter following command in the terminal: Note: replace mydomain with your actual domain name:\nusername@externalserver ~ % openssl req -new -newkey rsa:2048 -nodes -keyout mydomain.key -out mydomain.csr\nYou will be prompted to answer a series of questions:\n\nCountry name: 2 letter abbreviation for your country. Netherlands is NL.\nState or Province Name: this is where your org operates from. Zuid-Holland.\nLocality Name - name of the city your org operates from. Don‚Äôt use abbreviations in this field.\nOrganisation Name - use your (organisation‚Äôs) full name.\nOrganisational Unit Name - Use a department name ex. ‚ÄúIT Department‚Äù or ‚ÄúLibrary‚Äù\nCommon Name - the FQDN that you are requesting an SSL certificate for.\nEmail address\nOptional password (can skip step)\nOptional company name\n\nYour CSR file has now been generated. To find your CSR, take a look at the contents of your current working directory with the ls command. You should notice two new files ending with ‚Äú.key‚Äù and ‚Äú.csr‚Äù respectively. For example:\nusername@externalserver ~ % ls\n-rw-r--r--. 1 root root 1082 Jan 31 12:10 mydomain.csr\n-rw-------. 1 root root 1704 Jan 31 12:10 mydomain.key\nThe .key file should be kept private on your server. The .csr file is your certificate signing request, and can be sent to a Certificate Authority.\n\n\nStep 2. Secure-copy .csr file from server to local machine\nIn this step we use Secure Copy protocol (SCP) which is a means of securely transferring files between hosts on a network. This example will save the file in the Home directory, but you can also save it into any other project folder on your machine.\nNavigate to directory of choice. In this case, we‚Äôll use home.\nusername@localmachine .ssh % cd ~\nUse scp to secure copy .csr file from your external server. If you have followed ___these steps to enable 1-step SSH access to your VPS, you can do this using the alias you set, and the .csr file name which should be your external server FQDN.csr. Don‚Äôt forget to add the . at the end of the command.\nusername@localmachine ~ % scp externalserveralias:~/external-server-FQDN.nl.csr .\nCheck to see that it saved on your local machine using ls:\nusername@localmachine ~ % ls\nApplications        Movies\nDesktop             Music\nDocuments           Pictures\nDownloads           Public\nDropbox             external-server-FQDN.nl.csr\nLibrary             surfdrive\nIf you have not set up 1-step SSH connection to your VPS, the file transfer procedure from the VPS to your local computer is composed of two steps:\n\nFrom the VPS to the intermediary server, and\nFrom the intermediary server to the local computer.\n\nFor the first step use:\nscp &lt;path to the csr file&gt; &lt;netid&gt;@&lt;intermediary_server_address&gt;:&lt;a path in the intermediary_server&gt; (e.g., scp thredds.tudelft.nl.csr mynetid@linux-bastion-ex.tudelft.nl:~).\nIn the second step, you need to copy the file from the intermediary server to the local computer using the same command but with a different source and destination:\nscp &lt;netid&gt;@&lt;intermediary_server_address&gt;:&lt;the path in the intermediary_server to the selected file&gt; &lt;a path in the local computer&gt; (e.g., scp mynetid@linux-bastion-ex.tudelft.nl:~/thredds.tudelft.nl.csr .)\nPlease note, if you are a Windows user, for the second step you need to install cygwin and ssh to the intermediary server using:\nssh &lt;netid&gt;@&lt;intermediary_server_address&gt; (e.g., ssh mynetid@linux-bastion-ex.tudelft.nl).\n\n\nStep 3. Submit Certificate Server Request to TU Delft ICT via TopDesk form\nTU Delft ICT will use the information stored in your .csr file to get the SSL certificate from the signing authority and send the SSL certificate to you. In order to make this request, you must attach your .csr file from the previous step.\nNavigate to TopDesk form for TU Delft. TOPdesk SSL certificate server request.\nChoose ‚ÄúAttach file‚Äù and navigate to directory where .csr file is stored** (in this example, it is in ‚ÄúHome‚Äù). Select ‚Äúexternal-server-FQDN.nl.csr‚Äù.\nSubmit request. You can delete this file from your home directory after you submit the form.\nICT will respond with a SSL certificate (with the extension .crt, .cer, and/or .pem) that comes from the signing authority. When you have this, you can configure the SSL certificate to work with the web server on your VPS.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_SSL_Certs.html#notes-and-next-steps",
    "href": "docs/infrastructure/VPS_SSL_Certs.html#notes-and-next-steps",
    "title": "Configure SSL certificates",
    "section": "Notes and Next Steps",
    "text": "Notes and Next Steps\nSSL certificates can expire - TU Delft ICT will let you know when this is about to happen. To renew, you will need a new .csr file. You can send this to TU Delft ICT via the original TopDesk form and they will forward to the signing authority.\nTo use your SSL certificate with your web server, you need to change some configuration settings based on the web server you are using (e.g., Apache, nginx). See ___Set up an Apache web server for more information.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Configure SSL certificates"
    ]
  },
  {
    "objectID": "docs/img/licences.html",
    "href": "docs/img/licences.html",
    "title": "",
    "section": "",
    "text": "testing-pyramid.jpg\n\n\n\n\ntesting-pyramid.jpg\n\n\nPermission: ‚ÄúAll I ask is that you provide attribution to me (e.g., credit @SketchingDev and link back to the original source).‚Äù See https://github.com/SketchingDev/sketchingdev.github.io/issues/20#issuecomment-2690689502"
  },
  {
    "objectID": "docs/data/planning/privacy.html",
    "href": "docs/data/planning/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "In case you are working with sensitive data, it is important to consider the privacy of the individuals involved. This includes ensuring that personal data is collected, stored, and processed in compliance with relevant laws and regulations.\nYou might need to obtain informed consent from participants before collecting their data, and you should also need to anonymize or pseudonymize data to protect the privacy of individuals. Additionally, you should be aware of the ethical implications of your research and ensure that you are conducting it in a responsible manner.\n\n\n\n\n\n\n Important\n\n\n\n\nReach out to your faculty data steward for more information.\nExplore TU Delft privacy related resources:\n\nTU Delft Personal Data page\nTU Delft Personal Research Data Workflow Guide\nTU Delft Privacy SharePoint site\nPhD Supervisors guide - Personal data and human subjects in research\n\nAdditionally, Utrecht University has a Data Privacy handbook that covers and explains many privacy topics in detail!",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning",
      "Privacy"
    ]
  },
  {
    "objectID": "docs/data/getting_started.html",
    "href": "docs/data/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This section provides resources to support data owners and users at various stages of the data lifecycle. It highlights important information and available tooling at TU Delft that can help you implement the Findable, Accessible, Interoperable, and Reproducible (FAIR) principles in your research data management practices.\nTU Delft emphasizes researchers‚Äô adherence to best practices in Research Data and Software Management (RDSM) as crucial to research quality. The university has maintained policies for a structured framework clarifying roles, responsibilities, and procedural standards for RDSM.\nResearch data includes any information observed, generated or created for use in research projects. Though often tied to specific research projects, research data follows its own lifecycle, which is distinct from the project itself. After initial data collection, data may be processed, analyzed, published, shared, archived, reused or destroyed.\nEffective research data management by following FAIR principles ensures efficiency and reproducibility at every step of the data lifecycle. At TU Delft, various tools and resources are available to meet diverse data management needs at any stage in its lifecycle.\n\n\n\n\n\n\n Important note\n\n\n\nPlease note that useful and practical information on research data management (RDM) is covered by other TU Delft resources, such as the RDM 101 book, PhD Supervisors guide, the RDM and TU Delft library pages. We want to refer you to these resources throughout this section, and where applicable, an admonition (like this one) will be placed at the beginning of a page.\n\n\n\n\n\n\n\n\n Further reading\n\n\n\n\nTU Delft Research Data Management landing page\nTU Delft - The goal of data management\nTU Delft Research Data framework policy\nResearch data management policies per faculty",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html",
    "href": "docs/data/data_storage/storage_options.html",
    "title": "Storage options",
    "section": "",
    "text": "Storing your data in a secure location is a key element of a successful project with a data component. As a TU Delft researcher, you have several options. Below you can find an overview of the available storage options, depending on whether they are local, networked, or cloud-based, how secure they are, whether they can be shared with internal or external partners, and whether they are automatically backed up or not. Please visit the sections on Data Security, Data Sharing and Data Backup for more information on these topics.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html#overview-of-storage-options",
    "href": "docs/data/data_storage/storage_options.html#overview-of-storage-options",
    "title": "Storage options",
    "section": "Overview of storage options",
    "text": "Overview of storage options\n\n\n\n\n\n\n Important\n\n\n\nThese options have been curated from:\n\nThe intranet section on Data Storage\nThe TOPdesk ‚ÄúOverview of data storage and file sharing‚Äù (Home &gt; ICT services &gt; IT for Research &gt; Overview of data storage and file sharing)\nAnd the Storage Finder tool\nSURF\n\nPlease note that the information is subject to change, and you should always refer to the official TU Delft documentation for the most up-to-date information.\n\n\n\n\n\nStorage Option\nLocation\nSecurity\nSharing\nBackup\nSize\nAditional info\n\n\n\n\nPrivate laptop\nLocal\nLow\nDefined by owner\nDefined by owner\nCheck device specifications\nSecurity listed as low as the device is prone to loss and theft, among other incidents\n\n\nPrivate desktop\nLocal\nMedium\nDefined by owner\nDefined by owner\nCheck device specifications\nSame as above, perhaps more secure than a laptop as it is not necessarily mobile\n\n\nTU Delft managed laptop\nTU Delft network\nMedium\nInternal to TU Delft\nDefined by owner (unless using options listed below)\nCheck device specifications\nAll TU Delft managed devices are password protected. Security is therefore slightly higer than private counterparts\n\n\nTU Delft managed desktop\nTU Delft network\nMedium\nInternal to TU Delft\nDefined by owner (unless using options listed below)\nCheck device specifications\nAll TU Delft managed devices are password protected. Security is therefore slightly higer than private counterparts\n\n\nTU Delft Personal Data Storage (H:)\nTU Delft network\nHigh\nInternal to TU Delft\nYes\n8 GB\nStorage Finder\n\n\nTU Delft Group Data Storage (M:)\nTU Delft network\nHigh\nInternal to TU Delft\nYes\n50 GB\nStorage Finder\n\n\nTU Delft Project Data Storage (U:)\nTU Delft network\nHigh\nInternal to TU Delft. External access can be enabled with a TU Delft guest account.\nYes\n&gt; 250 GB\nStorage Finder\n\n\nMicrosoft Teams\nCloud\nHigh\nInternal to TU Delft\nYes\n&lt; 250 GB\nStorage Finder\n\n\nMicrosoft Sharepoint\nCloud\nHigh\nInternal to TU Delft\nYes\n&lt; 250 GB\nStorage Finder\n\n\nMicrosoft OneDrive for Business\nCloud\nHigh\nExternal access can be enabled\nYes\n1 TB\nStorage Finder\n\n\nSURFdrive\nCloud\nHigh\nExternal access can be enabled\nYes\n500 GB\nSURFdrive documentation",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage_options.html#project-drive-operations",
    "href": "docs/data/data_storage/storage_options.html#project-drive-operations",
    "title": "Storage options",
    "section": "Project Drive operations",
    "text": "Project Drive operations\nA Project Drive is a TU Delft managed network drive that allows you to store and share data with your project team members. It is a secure and reliable option for storing research data, and it is automatically backed up.\n\n\n\n\n\n\n For most situations, the Project Data Storage (U:) drive is the optimal choice for storing research data.\n\n\n\n\n\n\nBelow you can see how to access a Project Data Storage (U:) drive, request space for it, and how to mount it on a server (please refer to the section on Remote Servers for more details).\n\n\n\n Access and request space for Project Data Storage\n\nLearn more ¬ª\n\n\n\n Mount Project Drive on server\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html",
    "href": "docs/data/data_storage/sharing.html",
    "title": "Data sharing",
    "section": "",
    "text": "This section covers aspects of data sharing, for ongoing work, throughout the active phase of a research project. The use of data repositories, the importance of data licenses, and the significance of metadata in sharing completed work are discussed in the section on Archiving and publishing data.\nBoth sections are closely related, as data sharing is a key component of the broader data management lifecycle. As such, these sections include shared concepts of security and privacy also covered in sections of their own.\nAs shown in the overview of storage options section, TU Delft provides several options for data storage, which can be used to share data with internal and external partners. The choice of storage solution depends on the nature of the data, the level of security required, and the intended audience for the shared data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html#tu-delft-internal",
    "href": "docs/data/data_storage/sharing.html#tu-delft-internal",
    "title": "Data sharing",
    "section": "TU Delft internal",
    "text": "TU Delft internal\n\nNetworked storage\n\nPersonal Data Storage (H:)\nThis is the default storage location for personal data, such as documents, spreadsheets, and presentations. It is accessible only to the user and is not suitable for sharing with others. However, it enables you to share your data across TU Delft-managed devices connected to the TU Delft network.\n\n\nGroup Data Storage (M:)\nThe folder structure on this network drive follows a similar structure to that of the faculties, departments, and research groups at TU Delft. It is typically accessible to members of a research group and can be used to share data with colleagues within the same department and faculty. Access to this storage is managed by your department secretary and/or the Faculty ICT Manager.\n\n\nProject Data Storage (U:)\nAccess to this storage is requested by a contact person ‚Äúcaller/owner‚Äù which can in turn provide access via a NetID. Its data can be accessed through CIFS and NFS (with kerberos authentication), meaning it can be mounted in different systems. Please visit the project drive request and project drive mounting sections for more information.\n\n\n\nCloud-based storage\nCloud services are not recommended as primary storage locations for research data. A critical drawback is that access to data stored on these platforms can be lost upon the creator‚Äôs departure from TU Delft, posing a significant risk to data continuity and ownership. Furthermore, they ‚Äúshould not be used for highly confidential data such as state secrets, sensitive personal data or highly sensitive IP material‚Äù.\nTU Delft provides a few collaboration tools through the Microsoft Office 365 platform, including Microsoft Teams, SharePoint and OneDrive.\n\nMicrosoft Teams\nThis platform is primarily used for communication and collaboration within research groups and projects. It allows for file sharing, real-time collaboration, and integration with other Microsoft 365 applications. However, it is not a dedicated data storage solution and should be used in conjunction with other storage options.\n\n\n\n\n\n\n Find more details on TopDesk Home ‚áæ ICT services ‚áæ Collaboration Tools ‚áæ Microsoft Teams.\n\n\n\n\n\n\n\n\nMicrosoft SharePoint\nConsiderations for this platform are similar to those for Microsoft Teams. It is primarily used for document management and collaboration within research groups and projects. SharePoint allows for file sharing, version control, and integration with other Microsoft 365 applications.\n\n\n\n\n\n\n Find more details on TopDesk Home ‚áæ ICT services ‚áæ Collaboration Tools ‚áæ SharePoint 2016 support",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/sharing.html#external",
    "href": "docs/data/data_storage/sharing.html#external",
    "title": "Data sharing",
    "section": "External",
    "text": "External\n\nMicrosoft OneDrive for Business\nOneDrive is installed by default on the laptops and desktops supplied by TU Delft. TU Delft OneDrive is recognisable by the name: ‚ÄúOneDrive - Delft University of Technology‚Äù. Web based access within and outside TUDelft, sharing and working together is possible with TU Delft colleagues and also external users. OneDrive is suitable for sharing data with external partners, as it allows for controlled access and collaboration. However, it is important to ensure that sensitive data is not shared without proper security measures in place.\n\n\n\n\n\n\n Find more details on TopDesk Home ‚áæ OneDrive for Business\n\n\n\n\n\n\n\n\nSURFdrive\nSURFdrive is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. SURFdrive offers staff, researchers and students an easy way to share and synchronise files within a secure community cloud with ample storage capacity.\n\n\n\n\n\n\n Find more details on SURFdrive.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data sharing"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html",
    "href": "docs/data/data_storage/project_drive_request.html",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "",
    "text": "TU Delft network drives are automatically mounted on TU Delft-managed computers (running Windows) when connected to the TU Delft network (dastud, eduVPN).\nOn macOS and Linux, there are a few additional steps needed to access Project Drives. The instructions can be found here. (Last updated: August 2024)\nIt can also be accessed through webdata.tudelft.nl using a WebDAV web link staff-umbrella. Some systems also support the client WebDrive. To mount on a TU Delft Virtual Private Server, first follow the instructions here, and then the instructions in the next guide Mount Project Drive on server.\n\n\n\n\n\n\n Reminder\n\n\n\nIf you are accessing the Project Data Storage (U:) drive from outside the TU Delft network (e.g., from home), you need to connect to eduVPN beforehand.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html#accessing-the-project-data-storage-u-drive",
    "href": "docs/data/data_storage/project_drive_request.html#accessing-the-project-data-storage-u-drive",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "",
    "text": "TU Delft network drives are automatically mounted on TU Delft-managed computers (running Windows) when connected to the TU Delft network (dastud, eduVPN).\nOn macOS and Linux, there are a few additional steps needed to access Project Drives. The instructions can be found here. (Last updated: August 2024)\nIt can also be accessed through webdata.tudelft.nl using a WebDAV web link staff-umbrella. Some systems also support the client WebDrive. To mount on a TU Delft Virtual Private Server, first follow the instructions here, and then the instructions in the next guide Mount Project Drive on server.\n\n\n\n\n\n\n Reminder\n\n\n\nIf you are accessing the Project Data Storage (U:) drive from outside the TU Delft network (e.g., from home), you need to connect to eduVPN beforehand.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_request.html#requesting-project-data-storage-u-space",
    "href": "docs/data/data_storage/project_drive_request.html#requesting-project-data-storage-u-space",
    "title": "Accessing and requesting Project Data Storage space",
    "section": "Requesting Project Data Storage (U:) space",
    "text": "Requesting Project Data Storage (U:) space\n\nPrerequisites\n\nTU Delft NetID\n(Optional) A list of TU Delft collaborators who should have read/write access to it\n\n\n\nSteps\n\nRequest the storage via the TU Delft ICT form on TopDesk.\nFill out and send the form according to your data storage preferences and requirements.\nAccess your data storage.\n\n\nStep 1. Request storage via the TU Delft ICT form on TopDesk\nYou can make a request for data storage via a form available on the TopDesk self-service portal (requires NetID sign-in); navigate to:\nHome ‚áæ ICT services ‚áæ IT for Research ‚áæ Data storage for Research: Project data (U:) ‚áæ ICT: Request Research Data Storage.\n\n\nStep 2. Fill out and send the form according to your data storage preferences and requirements\nThe form is divided into three sections: ‚ÄúCaller‚Äù, ‚ÄúInformation about requester and data‚Äù, and ‚ÄúData for a research project‚Äù.\nThe Caller section should contain the contact information of the main administrator of this server. If you select your name, the fields below should be auto-populated with your building, phone number, email, department/program, organizational unit, and (sometimes) room.\n\nIn the next part, Information about requester and data, you choose your preferences about data preservation. The first question asks whether you are setting up new storage or want to change existing storage (or end it). The next questions are about the availability, classification and retention. There are information bubbles next to the choices and clarifications at the beginning of the form.\nLastly, in the section Data for a research project you need to provide information about your project. You will need to provide a description of the project, the name of the project, initial and future storage needs, and a corresponding person for handover if you leave TU Delft. Additionally, you have the option to choose whether you want to be able to share the data with external users.\n\n\n\nStep 3. Access your data storage\nAfter submitting the request, you will receive a response from ICT. Once approval is granted, you can access your Project Data Storage as described above.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Access and request space for *Project Data Storage*"
    ]
  },
  {
    "objectID": "docs/data/data_storage/backup.html",
    "href": "docs/data/data_storage/backup.html",
    "title": "Data backup",
    "section": "",
    "text": "An effective system for backing up research data is crucial to the success and reproducibility of any research project. Backups ensure that irreplaceable files are not lost due to hardware failure, accidental deletion, or other unforeseen events.\nA widely recommended strategy for data backup is the 3-2-1 Rule:\nWhen deciding on backup frequency, you should consider how often the data changes, the amount of work that would be lost between backups, the cost (time and money) to replace lost work, and the effort required for the backup process. Automated backup solutions are often more efficient than manual processes. Please refer to the section on Storage options for more information on which options involve automated backup or not.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data backup"
    ]
  },
  {
    "objectID": "docs/data/data_storage/backup.html#using-rsync-in-the-command-line-for-data-backup",
    "href": "docs/data/data_storage/backup.html#using-rsync-in-the-command-line-for-data-backup",
    "title": "Data backup",
    "section": "Using rsync in the command line for data backup",
    "text": "Using rsync in the command line for data backup\nIf you are comfortable with the command line, rsync is a powerful and versatile utility available on Linux (including Windows Subsystem for Linux: WSL) and macOS systems that can efficiently synchronize and back up files and directories. It is particularly useful for incremental backups, as it only transfers the differences between the source and destination, saving time and bandwidth.\nHere are simple steps to use rsync for data backup:\n\nBasic Local Backup: To copy files from a source directory to a destination directory on the same machine (e.g., to an external hard drive), use the following command:\n\n   rsync -av /path/to/your/source_data/ /path/to/your/backup_destination/\n\n-a (archive mode): This is a composite flag that preserves permissions, timestamps, symbolic links, and recursively copies directories, making it ideal for backups.\n-v (verbose): This flag provides detailed output, showing which files are being copied or skipped.\n\n\nIncremental Backups: To ensure that only new or changed files are copied, and to remove files from the backup destination if they‚Äôve been deleted from the source, add the --delete flag:\n\n   rsync -av --delete /path/to/your/source_data/ /path/to/your/backup_destination/\nThis makes the backup an exact mirror of the source, reflecting deletions as well as additions and modifications.\n\nRemote Backups via SSH: For secure backups to a remote server (e.g., a research server or another machine), rsync can use SSH. Ensure the remote machine has an SSH server running and you have SSH access.\n\n   rsync -avz /path/to/your/source_data/ username@remote_host:/path/to/remote/backup_destination/\n\n-z (compress): This flag compresses file data during transfer, which can speed up transfers over a network connection.\n\n\nExcluding Specific Files or Directories: If you need to exclude certain files or folders from your backup (e.g., temporary files or large datasets that are not critical to back up frequently), use the --exclude option:\n\n   rsync -av --exclude 'temp_files/' --exclude 'logs/*.log' /path/to/your/source_data/ /path/to/your/backup_destination/\nYou can specify multiple --exclude options for different patterns or paths.\n\n\n\n\n\n\n Important\n\n\n\nrsync is a robust tool for maintaining up-to-date copies of your research data, supporting both local and remote backup strategies as part of a comprehensive data management plan. For more advanced usage, refer to the rsync manual by running man rsync in your terminal or consult the rsync documentation.\n\n\n\n\n\n\n\n\n Important\n\n\n\nThese steps are similar to those recommended in Moving data to your server and in the Transfer Data section of the DelftBlue documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data backup"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/offboarding.html",
    "href": "docs/data/data_publishing/offboarding.html",
    "title": "Offboarding and ownership",
    "section": "",
    "text": "Offboarding procedures are necessary for ensuring the continuity and integrity of research data when team members leave or transition roles. Establishing clear ownership protocols and conducting thorough offboarding processes help maintain data accessibility, prevent loss, and ensure compliance with institutional policies. To achieve effective offboarding, document data responsibilities, transfer access rights, and conduct exit interviews to capture essential knowledge and insights.\nThis guide serves primarily as a reminder that there should be a plan in place for offboarding. Different research groups will have different practices, and there is no single definitive checklist or procedure. However, we would like to refer you to the Data Management Offboarding by Harvard RDM site for ideas on how to approach offboarding and preserve knowledge when colleagues leave.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Offboarding and ownership"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/archival_publishing_index.html",
    "href": "docs/data/data_publishing/archival_publishing_index.html",
    "title": "Archiving and publishing data",
    "section": "",
    "text": "Towards the end of the research cycle, data archiving (and publishing, when possible) is an essential step for the longevity and accessibility of your research. Archiving data means that it is preserved and remains accessible for future use. This process not only helps with the visibility of your work but also contributes to the advancement of knowledge in your field. Other researchers have the data available and can build upon your work. Learning how to effectively preserve and share your research isn‚Äôt just good scientific practice - it‚Äôs how your work makes a lasting impact.\n\n\n\n Licensing\nChoose a license for your data.\n\nLearn more ¬ª\n\n\n\n Publishing data\nBroaden the reach of your research.\n\nLearn more ¬ª\n\n\n\n Offboarding and ownership\nEnsures accountability and longevity.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish"
    ]
  },
  {
    "objectID": "docs/data/data_collection/elab_rspace.html",
    "href": "docs/data/data_collection/elab_rspace.html",
    "title": "eLabJournal and RSpace",
    "section": "",
    "text": "Jones recounts her frustration with relying on paper notebooks to record experiments. ‚ÄúIt becomes problematic when you need to replicate experiments or continue a research project conducted by a researcher who has since left your institution without documenting their work properly.‚Äù Jones identifies some difficulties of understanding handwritten information (including her own!) ‚ÄúLocating and interpreting data from past experiments can be challenging using paper notebooks‚Äù, she admits. ‚ÄúFinally, after rifling through pages of scribbled diagrams, photocopies and post-it notes you find the experiment you‚Äôve been searching for and it can be impossible to decipher the handwriting and bridge the gaps of missing information‚Äù. These problems sound all too familiar to most lab-based scientists and have increased the demand for digital solutions, such as ELNs, that can improve the rigour, robustness and reproducibility of scientific research.\nDr.¬†Si√¢n Jones, quote taken from Keep calm and go paperless: Electronic lab notebooks can improve your research. CC-BY-4.0\n\nAn electronic laboratory notebook (ELN), also called a digital lab notebook, offers a text editor for writing notes similar to a paper notebook, along with spreadsheet tools for calculations and formatting tables and graphs. They include protocol templates to record standard procedures and laboratory inventories to keep track of samples, reagents, and equipment. Additionally, ELNs provide collaboration tools to share experimental information with others, making them a straightforward solution to research documentation.\nTU Delft has a subscription to eLABJournal and RSpace ELNs, and both offer similar functionalities.\n\n\n\n\n\n\n\n Learn more\n\n\n\nFor instructions on getting started with RSpace and eLabJournal, along with other essential information about ELNs, visit the TU Delft Library website‚Äôs Electronic Lab Notebook page.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "eLabJournal and RSpace"
    ]
  },
  {
    "objectID": "docs/data/data_collection/collection.html",
    "href": "docs/data/data_collection/collection.html",
    "title": "Data collection",
    "section": "",
    "text": "Data collection is one of the first steps in the data lifecycle. It involves gathering and recording data from various sources, which can include (but are not limited to) experiments, observations, simulations or existing datasets. These can come from various sources being both public and private. The data collected can be quantitative or qualitative, and it can be structured or unstructured. The choice of data collection methods and tools depends on the research objectives, the type of data needed, and the resources available.\nProper data collection practices are essential for ensuring the integrity and reliability of the data, as well as for its use later analysis and interpretation. This section provides an overview of important aspects of data collection, including data conventions, data access and reuse, and the use of eLabJournal and RSpace. These elements are essential for maintaining the quality and usability of research data, and they play a significant role in the overall research process.\n\n\n\n\n\n\n Learn more\n\n\n\n\nWe would like to refer you to the Discover & Reuse and Gather/Create & Analyse and sections of the TU Delft Navigating Research Data and Software: A Practical Guide for PhD Supervisors guide for more information.\nAdditionally, we would like to refer you to the RDM 101 book for more information on this topic.\n\n\n\n\n\n\n Data conventions\nData standards and types.\n\nLearn more ¬ª\n\n\n\n Data access and reuse\nMaking your data accessible and reusable.\n\nLearn more ¬ª\n\n\n\n eLabJournal and RSpace\nTools made available by TU Delft to document the research process.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection"
    ]
  },
  {
    "objectID": "docs/community/maintainers.html",
    "href": "docs/community/maintainers.html",
    "title": "Guide maintainers",
    "section": "",
    "text": "Guide maintainers\nThis content is automatically generated, all changes made will be lost.\n\n\n\n\n\n\n\n\n\nSection\nTitle\nLead maintainer\nBackup maintainer\n\n\n\n\ncontainers\nDocker users\nMaurits Kok\n\n\n\ndata\nRequestProject Drive\nAshley Cryan\n\n\n\ndata\nSync with Unison\nAshley Cryan\n\n\n\ndata\nMount Project Drive\nRa√∫l Ortiz Merino\nMaurits Kok\n\n\ndata\nData publishing\nAleksandra Wilczynska\n\n\n\ngitlab\nTransfer ownership of a GitLab repository\nLora Armstrong\nMaurits Kok\n\n\ngitlab\nCI with Gitlab\nAshley Cryan\nMaurits Kok\n\n\ngitlab\nGitLab runner for MATLAB\nMaurits Kok\n\n\n\ninfrastructure\nOverview\nMaurits Kok\nJose Urra\n\n\ninfrastructure\nRequest SSL Certificate\nAshley Cryan\n\n\n\ninfrastructure\nRequest VPS\nAshley Cryan\n\n\n\ninfrastructure\nSet up an Apache web server\nAshley Cryan\n\n\n\nresources\nCurriculum\nAshley Cryan\nMaurits Kok\n\n\nsoftware\nFAIR4RS checklist\nMaurits Kok\nElviss Dvinskis\n\n\nsoftware\nSoftware Management Plan\nMaurits Kok\n\n\n\nsoftware\nBranch management\nMaurits Kok\n\n\n\ntesting\nTesting with MATLAB\nMaurits Kok"
  },
  {
    "objectID": "docs/community/community.html",
    "href": "docs/community/community.html",
    "title": "Community",
    "section": "",
    "text": "Teams channel - DCC Community\nData Stewards\nOpen Science Program\nOpen Science Community\nDelft HPC\nData Champions\nICT Innovation\nStatistical Helpdesk\n\n\n\n\n4TU.ResearchData is an international data repository for science, engineering and design. Its services include curation, sharing, long-term access and preservation of research datasets. These services are available to anyone around the world. In addition, 4TU.ResearchData also offers training and resources to researchers to support them in making research data findable, accessible, interoperable and reproducible (FAIR).\n\n\n\nFounded in 2012 as an independent foundation by NWO and SURF, the Netherlands eScience Center is the national centre with the digital skills to create innovative software solutions in academic research. They award research projects based on calls for proposals, and train researchers in the use of research software. They offer our expertise in the form of research software engineers (RSEs), the technology specialists with expert digital skills who work with us at the Center.\n\n\n\nNL-RSE brings together the community of people writing and contributing to research software from Dutch universities, knowledge institutes, companies and other organizations to share knowledge, to organize meetings, and raise awareness for the scientific recognition of research software."
  },
  {
    "objectID": "docs/community/community.html#partners-within-the-tu-delft",
    "href": "docs/community/community.html#partners-within-the-tu-delft",
    "title": "Community",
    "section": "",
    "text": "Teams channel - DCC Community\nData Stewards\nOpen Science Program\nOpen Science Community\nDelft HPC\nData Champions\nICT Innovation\nStatistical Helpdesk"
  },
  {
    "objectID": "docs/community/community.html#tu.researchdata",
    "href": "docs/community/community.html#tu.researchdata",
    "title": "Community",
    "section": "",
    "text": "4TU.ResearchData is an international data repository for science, engineering and design. Its services include curation, sharing, long-term access and preservation of research datasets. These services are available to anyone around the world. In addition, 4TU.ResearchData also offers training and resources to researchers to support them in making research data findable, accessible, interoperable and reproducible (FAIR)."
  },
  {
    "objectID": "docs/community/community.html#escience-center",
    "href": "docs/community/community.html#escience-center",
    "title": "Community",
    "section": "",
    "text": "Founded in 2012 as an independent foundation by NWO and SURF, the Netherlands eScience Center is the national centre with the digital skills to create innovative software solutions in academic research. They award research projects based on calls for proposals, and train researchers in the use of research software. They offer our expertise in the form of research software engineers (RSEs), the technology specialists with expert digital skills who work with us at the Center."
  },
  {
    "objectID": "docs/community/community.html#nl-rse",
    "href": "docs/community/community.html#nl-rse",
    "title": "Community",
    "section": "",
    "text": "NL-RSE brings together the community of people writing and contributing to research software from Dutch universities, knowledge institutes, companies and other organizations to share knowledge, to organize meetings, and raise awareness for the scientific recognition of research software."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dcc@tudelft.nl. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dcc@tudelft.nl. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the DCC Guides!",
    "section": "",
    "text": "These guides provide a starting point for Research Software, Research Data, and Research Computing at TU Delft. This is an initiative from the TU Delft Digital Competence Centre.\n Shared solutions for research support staff  Practical guides for researchers\n\n\n   Research Software \n   Data Management \n   Computing Infrastructure \n   Courses and Workshops \n\n\n\n\n\n\n\n\n Want to get involved? \n Join the community We welcome anyone to join us in improving our guides! Find out how in our contributing guide.\n Join the discussion We welcome community discussions, ideas, and general questions to develop solutions and receive feedback in our community forum.\n Open an issue We track topic requests and bug-reports via GitHub issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "<span style=\"filter:grayscale(100%);\">‚åÇ</span> **Home**"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have issues, tips, ideas, events, or questions related to Research Computing, Research Data, and Research Software at TU Delft?\nAre you a researcher interested in these topics?\nDo you work and collaborate with researchers on these topics?\n\nWe‚Äôre using GitHub Discussions as a place to connect with other members of our community. We hope that you:\n\nAsk questions about challenges you encounter\nShare ideas and solutions\nEngage with other community members\nBe welcoming and open-minded. Remember that this is a community we build together. See our Code of Conduct for more information.\n\n\n\n\nDo you have questions, ideas or ongoing developments on FAIR related aspects, Open Science, training, etc? Would you like to point to specific resources and potential solutions or ideas?\n\nUse the Q&A to ask a question on a specific topic, such as\n\nHow do I generate a reproducible software development environment?\nWhere can I find information on creating a Python package?\nHow should I archive my software?\nHow do I get started with git?\n\nUse Ideas to make proposals, for instance\n\nUsing jupyter hub for a workshop\nUsing jupyter books to create educational resources\nRunning monthly webinars for the community to transfer FAIR practices\n\nUse Solutions to point people to existing solutions or share your own. These solutions might end up on our Guides website to share with our community.\nShow and tell things you have done that you are proud of and like other to be aware of.\n\n\n\n\n\n\n\nFork the repository to your own GitHub profile.\nClone the repository.\nNavigate to the root of this repository in your terminal.\nInstall Quarto if you don‚Äôt already have it installed on your machine. You can find the installation instructions here.\n\nAlternative: Install Quarto within a virtual environment using the environment.yml file by following the below steps:\n\nRun conda env create -f environment.yml in the terminal to create a conda environment with Quarto pre-installed.\nActivate the environment by running conda activate dcc_guides.\n\n\n\nRun quarto preview.\nYou will see the rendered version in a browser window.\n\n\n\n\n\nFork the repository to your own GitHub profile.\nEither commit a new change to the repository to trigger the build action or manually trigger the action. To manually trigger the action, go to Actions -&gt; Quarto Publish Guides and press Run workflow and Run workflow.\nIn your forked repository, under Settings -&gt; Pages set Source to gh-pages and /(root) and press Save.\n\n\n\n\n\n(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;\n(important) wait until some kind of consensus is reached about your idea being a good idea;\nif needed, fork the repository to your own GitHub profile and create your own feature branch off of the latest main commit. While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes, possibly from the ‚Äòupstream‚Äô repository (follow the instructions here and here);\npush your feature branch to (your fork of) the DCC guides repository on GitHub;\ncreate the pull request, e.g.¬†following the instructions here. If needed, provide a link to the gh-pages in your forked repository: https://&lt;your-username&gt;.github.io/TU-Delft-DCC.github.io/."
  },
  {
    "objectID": "CONTRIBUTING.html#welcome",
    "href": "CONTRIBUTING.html#welcome",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have issues, tips, ideas, events, or questions related to Research Computing, Research Data, and Research Software at TU Delft?\nAre you a researcher interested in these topics?\nDo you work and collaborate with researchers on these topics?\n\nWe‚Äôre using GitHub Discussions as a place to connect with other members of our community. We hope that you:\n\nAsk questions about challenges you encounter\nShare ideas and solutions\nEngage with other community members\nBe welcoming and open-minded. Remember that this is a community we build together. See our Code of Conduct for more information."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-participate",
    "href": "CONTRIBUTING.html#how-to-participate",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Do you have questions, ideas or ongoing developments on FAIR related aspects, Open Science, training, etc? Would you like to point to specific resources and potential solutions or ideas?\n\nUse the Q&A to ask a question on a specific topic, such as\n\nHow do I generate a reproducible software development environment?\nWhere can I find information on creating a Python package?\nHow should I archive my software?\nHow do I get started with git?\n\nUse Ideas to make proposals, for instance\n\nUsing jupyter hub for a workshop\nUsing jupyter books to create educational resources\nRunning monthly webinars for the community to transfer FAIR practices\n\nUse Solutions to point people to existing solutions or share your own. These solutions might end up on our Guides website to share with our community.\nShow and tell things you have done that you are proud of and like other to be aware of."
  },
  {
    "objectID": "CONTRIBUTING.html#for-developers",
    "href": "CONTRIBUTING.html#for-developers",
    "title": "Contributing guidelines",
    "section": "",
    "text": "Fork the repository to your own GitHub profile.\nClone the repository.\nNavigate to the root of this repository in your terminal.\nInstall Quarto if you don‚Äôt already have it installed on your machine. You can find the installation instructions here.\n\nAlternative: Install Quarto within a virtual environment using the environment.yml file by following the below steps:\n\nRun conda env create -f environment.yml in the terminal to create a conda environment with Quarto pre-installed.\nActivate the environment by running conda activate dcc_guides.\n\n\n\nRun quarto preview.\nYou will see the rendered version in a browser window.\n\n\n\n\n\nFork the repository to your own GitHub profile.\nEither commit a new change to the repository to trigger the build action or manually trigger the action. To manually trigger the action, go to Actions -&gt; Quarto Publish Guides and press Run workflow and Run workflow.\nIn your forked repository, under Settings -&gt; Pages set Source to gh-pages and /(root) and press Save.\n\n\n\n\n\n(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;\n(important) wait until some kind of consensus is reached about your idea being a good idea;\nif needed, fork the repository to your own GitHub profile and create your own feature branch off of the latest main commit. While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes, possibly from the ‚Äòupstream‚Äô repository (follow the instructions here and here);\npush your feature branch to (your fork of) the DCC guides repository on GitHub;\ncreate the pull request, e.g.¬†following the instructions here. If needed, provide a link to the gh-pages in your forked repository: https://&lt;your-username&gt;.github.io/TU-Delft-DCC.github.io/."
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About these guides",
    "section": "",
    "text": "The DCC Guides are a curated, well-maintained online resource tailored for research support staff and researchers at TU Delft. They provide guidance on data management, research software development, and computing practices. By incorporating best practices from both internal and external sources, the guides will offer a centralized, comprehensive collection of solutions and resources specific to the TU Delft research environment.\nThe DCC Guides support research support staff by:\n\nEnabling the collection, curation, and sharing of developed solutions and relevant resources.\nPromoting a consistent, unified approach to delivering research support.\n\nThe DCC Guides support researchers by:\n\nHelping them understand the key challenges and requirements of digital research.\nGuiding researchers towards compliance with TU Delft policies on data management and research software.\nProviding standalone documentation for resolving digital research challenges independently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "docs/about.html#purpose-and-benefits",
    "href": "docs/about.html#purpose-and-benefits",
    "title": "About these guides",
    "section": "",
    "text": "The DCC Guides are a curated, well-maintained online resource tailored for research support staff and researchers at TU Delft. They provide guidance on data management, research software development, and computing practices. By incorporating best practices from both internal and external sources, the guides will offer a centralized, comprehensive collection of solutions and resources specific to the TU Delft research environment.\nThe DCC Guides support research support staff by:\n\nEnabling the collection, curation, and sharing of developed solutions and relevant resources.\nPromoting a consistent, unified approach to delivering research support.\n\nThe DCC Guides support researchers by:\n\nHelping them understand the key challenges and requirements of digital research.\nGuiding researchers towards compliance with TU Delft policies on data management and research software.\nProviding standalone documentation for resolving digital research challenges independently.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "docs/about.html#audience",
    "href": "docs/about.html#audience",
    "title": "About these guides",
    "section": "Audience",
    "text": "Audience\nThe DCC Guides are primarily designed for the DCC and TU Delft research support staff, with researchers as a secondary audience. Usability for support processes is a core focus. Success is assessed through increased user engagement (measured by feedback and site analytics) and improved support outcomes (evaluated internally).\n\n1. DCC team members\nAs the main contributors and maintainers, the DCC team is the primary audience. A key goal is to document and preserve developed solutions for reuse, avoiding redundant effort. The team is responsible for regularly updating the guides, integrating user feedback, and ensuring content quality.\n\n\n2. TU Delft research support staff\nThis group includes Faculty Data Stewards, ICT Innovation, and the Library‚Äôs Research and Data Services team. These staff members can use the guides as a trusted resource to support researchers more effectively and stay up to date with best practices.\n\n\n3. TU Delft researchers\nResearchers can benefit from easy access to current, reliable information to support their research workflows. The guides are not intended as a complete training pathway and will link to trustworthy external resources where appropriate.",
    "crumbs": [
      "<ins>Guides</ins>",
      "About the guides"
    ]
  },
  {
    "objectID": "docs/community/dcc.html",
    "href": "docs/community/dcc.html",
    "title": "About the DCC",
    "section": "",
    "text": "About the DCC\nThe TU Delft Digital Competence Centre (DCC) is an on-campus initiative to help researchers make research data FAIR, improve research software, and apply computing practices to increase the efficiency of the research process. The DCC is an initiative of the Open Science Programme at TU Delft designed to benefit researchers at all levels.\nFor more information, please visit our website at dcc.tudelft.nl or email us at dcc@tudelft.nl."
  },
  {
    "objectID": "docs/data/data_collection/access_reuse.html",
    "href": "docs/data/data_collection/access_reuse.html",
    "title": "Access and reuse",
    "section": "",
    "text": "Access and reuse depends on a variety of factors, including the format in which your data is stored, the tools used to create and manage it, and the policies governing its use. The key consideration is planning who, and under what conditions, can access your data.\n\n\n\n\n\n\n Learn more\n\n\n\nAgain, we would like to refer you to modules from the RDM 101 course and links from the PhD Supervisors guide for more information on this topic:\n\nRDM 101 - Module 3 - Access to Data\nRDM 101 - Module 4 - Data access and Data publication\nPhD Supervisors guide - Discover & Reuse\n\n\n\nTo ensure that your data can be accessed and reused in the future, consider the following practices:\n\nChoose the right format: Opting for widely accepted, non-proprietary, and open formats ensures that your data remains accessible and usable over time, even as technology evolves. Formats such as CSV for tabular data, TXT or Markdown for text, and PNG for images are examples of formats that are more likely to be supported in the long term. This increases the likelihood that your data will be accessible in the future, regardless of changes in technology or software.\nDocument your data: Documenting the context and structure of your data to enhance its reusability. Providing metadata, clear file naming conventions, and a README file explaining the dataset can significantly improve the chances of your data being understood and reused by others, including your future self.\nFollow data management best practices: Implement data management best practices, such as version control, regular backups, and secure storage. This will help ensure that your data remains accessible and usable over time.\nConsider data sharing policies: Be aware of any data sharing policies or regulations that may apply to your data. This includes understanding the rights and permissions associated with your data, as well as any restrictions on its use or distribution.\n\n\n\n\n\n\n\n Tip: Explore our Data sharing guide for more information.\n\n\n\n\nPlan for long-term storage: Consider how your data will be stored and accessed in the long term. This may involve using cloud storage solutions, institutional repositories, or other platforms that provide reliable access to your data over time.\n\n\n\n\n\n\n\n Tip: Explore our section Data storage for more information on data storage options.\n\n\n\n\nArchive in a trusted repository: Deposit your data in a trusted data repository. Repositories like 4TU.ResearchData provide long-term storage, preservation, and access services, making it easier for others to find and reuse your data.\n\n\n\n\n\n\n\n Tip: Explore our section Archive and publish for information on archiving your research data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Access and reuse"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html",
    "href": "docs/data/data_collection/data_conventions.html",
    "title": "Data conventions",
    "section": "",
    "text": "By adhering to domain-aligned data conventions, you can enhance the quality and reproducibility of your work, promote collaboration, data sharing and reuse. While flexibility exists to accommodate the specific needs of your research project, it is important to establish a consistent framework for data collection and management. This section outlines some key considerations for data conventions, including naming conventions, folder structure, and file standards.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#data-conventions",
    "href": "docs/data/data_collection/data_conventions.html#data-conventions",
    "title": "Data conventions",
    "section": "Data conventions",
    "text": "Data conventions\nEstablishing consistent naming conventions and metadata practices for your data files is essential for long-term project maintainability. Similarly, as mentioned in the Project structure guide in the Research Software section, some essential principles apply, like:\n\nuse descriptive, concise and consistent names\navoid special characters and spaces (instead use underscores _ or hyphens -)\nfollow ISO 8601 date formatting (YYYY-MM-DD)\nthe order matters - order elements in the name from least to most specific\ninclude versioning in the name (where applicable)\n\nHowever, the specific conventions you choose should align with the needs of your research project, collaborators, and established practices within your field.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#folder-structure",
    "href": "docs/data/data_collection/data_conventions.html#folder-structure",
    "title": "Data conventions",
    "section": "Folder structure",
    "text": "Folder structure\nMaintaining a consistent folder structure across your research group and project phases prevents confusion, reduces errors in data analysis, and significantly improves research reproducibility. Remember that folder structure should complement your file naming (data) conventions to create a cohesive data organization system. Some practices to consider include:\n\nUse a consistent and descriptive naming convention for folders, similar to file naming conventions\nAvoid using spaces or special characters in folder names\nCreate a hierarchical structure that moves from general to specific, making navigation intuitive\nEstablish consistent naming patterns across all levels of your folder hierarchy\nSeparate raw and processed data into distinct folders to preserve original data integrity\nLimit folder nesting to 3-5 levels to prevent excessive complexity\nGroup related files into logical categories rather than creating folders for individual files\nDocument your folder structure in your data management plan for reference\nFor time-series or versioned data, consider incorporating date-based folder organization\nConsider adding a README file in your data/ folder to provide an overview of the folder structure and its contents",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_collection/data_conventions.html#file-standards",
    "href": "docs/data/data_collection/data_conventions.html#file-standards",
    "title": "Data conventions",
    "section": "File standards",
    "text": "File standards\nFiles commonly use standardised formats depending on their knowledge domain, and/or the platfrom through which they are made available. For example, FASTA format files are widely used in bioinformatics workflows for sequence alignment, database searches, genome assembly and more. Several major biological databases and tooling use and support the FASTA format, like NCBI, EMBL-EBI, and UniProt.\nIn the field of geosciences, the NetCDF format is widely used for array-oriented scientific data. It is particularly useful for storing multidimensional data such as temperature, pressure, and humidity in a single file. NetCDF files are self-describing and can be compressed to save space, making them ideal for large datasets.\nIn the field of engineering, data is often stored in formats such as HDF5. It is widely used for storing large amounts of numerical data and are supported by many programming languages and software tools.\nIn addition, 4TU.ResearchData has a list of preferred file formats, and they have a data collection policy that outlines the preferred file formats for data submission.\n\n\n\n\n\n\n Further reading\n\n\n\n\nThe FAIR Guiding Principles for scientific data management and stewardship",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Collection",
      "Data conventions"
    ]
  },
  {
    "objectID": "docs/data/data_processing.html",
    "href": "docs/data/data_processing.html",
    "title": "Data processing",
    "section": "",
    "text": "Data processing is a broad term and is often conflated with related terms. It typically includes steps like data cleaning and data transformation. Some workflows may extend this to preliminary analysis, though visualizations are generally considered a separate stage. The term data wrangling is frequently used interchangeably with early processing stages. The specific steps involved can vary depending on the type of data and the goals of the analysis.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Data processing"
    ]
  },
  {
    "objectID": "docs/data/data_processing.html#data-versioning",
    "href": "docs/data/data_processing.html#data-versioning",
    "title": "Data processing",
    "section": "Data versioning",
    "text": "Data versioning\nData versioning is the process of assigning unique version numbers to different iterations of a dataset. This practice is important when working with machine learning models, as it allows to track changes made to the data over time and ensure that the correct version of the data is used for training and testing. Data versioning can also help with reproducibility, because it allows to recreate previous versions of the data and compare results across different iterations.\nThere are a couple of techniques and tools for data versioning. For example, Git is typically used to track source code and software versions, but is not well-suited for large datasets. However, Git LFS (Large File Storage) can be used to manage large files in a Git repository. Other tools like DVC (Data Version Control) are specifically designed for data versioning and can be used to track changes to datasets and models over time. DVC integrates with Git and allows to version control data files, models, and experiments in a way that is similar to how Git works for source code.\n\nDVC\nDVC has very comprehensive documentation, tutorials and videos available, most of the resources are available on their homepage. It covers how to get started with data versioning, how to integrate it with Git, and how to use DVC with different cloud storage providers (e.g.¬†AWS S3).",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Data processing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html",
    "href": "docs/data/data_publishing/licensing_data.html",
    "title": "Data licensing",
    "section": "",
    "text": "When depositing data, selecting an appropriate license cannot be overlooked, as it defines the permissions and restrictions for others using your data. Clear labelling of licensing terms ensures that data can be shared and reused legally and ethically.\nSimilarly to software licenses, data licenses can be divided depending on their restrictiveness. The most common licenses are Creative Commons (CC) licenses, which are widely used for data and other creative works. These licenses can be categorized into permissive and restrictive types.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html#cc-licenses",
    "href": "docs/data/data_publishing/licensing_data.html#cc-licenses",
    "title": "Data licensing",
    "section": "CC licenses",
    "text": "CC licenses\nPermissive licenses allow for broad reuse with minimal restrictions, while restrictive licenses impose limitations on how the data can be used. CC licenses are further divided into the following categories:\n\nPublic Domain (CC0): No restrictions, data can be used for any purpose.\nAttribution (CC-BY): Requires attribution to the original creator, allows for sharing and adaptation with appropriate citations.\nAttribution-ShareAlike (CC-BY-SA): Requires attribution and allows for derivatives under the same license.\nNon-Commercial (CC-BY-NC): Allows use for non-commercial purposes only.\nNo Derivatives (CC-BY-ND): No derivatives allowed.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCreative Commons Licenses",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/licensing_data.html#other-data-licenses",
    "href": "docs/data/data_publishing/licensing_data.html#other-data-licenses",
    "title": "Data licensing",
    "section": "Other data licenses",
    "text": "Other data licenses\nIn addition to CC licenses, Open Data Commons provides a set of licenses that are specifically designed for data. These licenses are similar to CC licenses but are tailored for data and databases.\n\nPublic Domain Dedication and License (PDDL): Similar to CC0, it allows for unrestricted use of the data.\nOpen Data Commons Attribution License (ODC-BY): Requires attribution to the original creator, allows for sharing and adaptation with appropriate citations.\nOpen Data Commons Open Database License (ODbL): Allows for sharing, modification, and use of the database, but requires attribution and share-alike for derivatives. Similar to CC-BY-SA, but specifically designed for databases.\nRestrictive Licenses: Limits usage, often prohibiting commercial use or modifications.\n\nWhile Creative Commons and Open Data Commons license might seem similar, the difference between them is that CC licenses are more general and can be applied to any type of work, while Open Data Commons licenses are specifically designed for data and databases and therefore cover different rights.\n\n\n\n\n\n\n Learn more\n\n\n\n\nTuring Way - Data Licenses",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Licensing"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html",
    "href": "docs/data/data_publishing/publishing.html",
    "title": "Publishing data",
    "section": "",
    "text": "Data repositories are online locations for storing research data long term. When data is published in a repository, it is (often) assigned a persistent digital object identifier (DOI), which ensures that the data is discoverable and accessible over time. When published, the data should be accompanied by an appropriate license and metadata describing your data.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html#tu.researchdata",
    "href": "docs/data/data_publishing/publishing.html#tu.researchdata",
    "title": "Publishing data",
    "section": "4TU.ResearchData",
    "text": "4TU.ResearchData\nChoosing a repository for publishing your data depends on factors such as storage costs, funding or publication requirements, and security needs. TU Delft provides an in-house data repository, 4TU.ResearchData, supporting versioning and a GitHub remote as part of their services. Additionally, when publishing your data, your submission is reviewed to ensure that repository requirements are met. You will receive a DOI for your dataset, which can be used to cite your data in publications. All data is stored in the Netherlands, with two locations in Delft and a backup in Leiden.\n\n\n\n\n\n\n Learn more\n\n\n\nGetting started with 4TU.ResearchData",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_publishing/publishing.html#zenodo",
    "href": "docs/data/data_publishing/publishing.html#zenodo",
    "title": "Publishing data",
    "section": "Zenodo",
    "text": "Zenodo\nDeveloped by CERN, Zenodo is an open repository that allows researchers to upload and share their research outputs, including datasets, publications, and software. Similarly to 4TU.ResearchData you will receive a DOI for your dataset. Zenodo supports versioning and has GitHub integration.\n\n\n\nSelecting a data repository. The Turing Way Community. This illustration is created by Scriberia with The Turing Way community, used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\nRegardless of the repository you choose, it is important to ensure that your data is properly documented. This includes describing the content, structure, and context of your data, as well as any relevant documentation that is needed to understand and use the data.\n\n\n\n\n\n\n Learn more\n\n\n\nFor a more comprehensive list of repositories and how to select one, you can check the Turing Way book or the TU Delft Library Guidelines",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Archive and publish",
      "Publishing data"
    ]
  },
  {
    "objectID": "docs/data/data_storage/project_drive_mounting.html",
    "href": "docs/data/data_storage/project_drive_mounting.html",
    "title": "Mount Project Drive on server",
    "section": "",
    "text": "Project drive storage can be mounted and made accessible in your TU Delft Virtual Private Server (VPS).\n\nPrerequisites\n\nA TU Delft VPS\nA TU Delft Project Data Storage drive\n\n\n\nSteps\n\nLocate the URL of your project storage.\nConnect to your TU Delft VPS via SSH.\nCreate a new directory as a mounting point.\nRetrieve your Linux user and group details.\nEdit the fstab file to include project storage technical details.\nMount the project drive.\n\n\nStep 1. Locate the URL of your project storage\nThe URL for your project drive can be obtained from either:\n\nthe email from TU Delft ICT with the confirmation of your project drive request, or\nby using a web browser to navigate into https://webdata.tudelft.nl/\n\n\n\n\n\n\n\n Note\n\n\n\nhttps://webdata.tudelft.nl/ can only be accessed on campus or using eduVPN\n\n\n\nChrome/Brave/Firefox/Vivaldi/SafariEdge\n\n\n\nNavigate into https://webdata.tudelft.nl/staff-umbrella/\n\nA pop-up should appear asking for your username and password.\n\nProvide your NetID in the username field, provide your password accordingly.\n\nYou should now see a list of your project drives.\n\nClick on the project drive of your choice.\n\nCopy everything after ‚Äúhttps://webdata.tudelft.nl/‚Äù\n\n\n\nContent within webdata is under password protection. Typing your username and password is only possible with a pop-up message which is disabled in an Edge browser on systems managed by TU Delft.\n\n\n\n\n\nStep 2. Connect to your TU Delft VPS via SSH\nFollow instructions in the TU Delft ICT email from initial server setup or follow our guide to configure via SSH.\n\n\nStep 3. Create a new directory as the mounting point\nThe convention is to create mounting points in the folder /media. Navigate to the directory and create a new folder with:\ncd /media\nmkdir &lt;server_mount_point&gt;\nReplace &lt;server_mount_point&gt; with the name of your choice. This will be the name of the folder where your project drive will be mounted.\n\n\nStep 4. Find and save your user and group details\nIn the terminal, you can retrieve your local user and group details with:\nid -u &lt;your_netID&gt; # User ID\nid -g &lt;your_netID&gt; # Group ID\nYou may need the values for uid and gid for step 5.\n\n\n\n\n\n\n Note\n\n\n\nThese commands are server-specific, so make sure to execute them on the server where the project drives will be mounted.\n\n\n\n\nStep 5. Edit the fstab file to include project storage technical details\nThe fstab file contains a list of the addresses of external file systems. In this file, the details of your project drive will need to be added in a single line. This line consists of four parts:\n\nfilesystem - the address of the project drive\nmount point - the location in the VPS where you want to mount the project drive\ntype - the type of the filesystem\noptions - additional option such as user privileges\n\nThe fstab file must be in the /etc/ directory and can be opened with the vi or nano editor:\n\nvinano\n\n\nIn the terminal, enter the following command to open the fstab file in the vi editor:\nsudo vi /etc/fstab\nThen, switch to the insert mode (hit ‚Äúi‚Äù to switch to insert mode and be able to type)\n\n\nIn the terminal, enter the following command to open the fstab file in the nano editor:\nsudo nano /etc/fstab\n\n\n\nAdd the following line to the file:\n&lt;your_netID&gt;@sftp.tudelft.nl:/staff-umbrella/&lt;project_drive_name&gt;  /media/&lt;server_mount_point&gt; fuse.sshfs  rw,noauto,users,_netdev  0  0\nreplacing the values between &lt; and &gt; with your NetID, the name of your project drive, and the name of the folder you created in step 3.\n\n\n\n\n\n\n Note\n\n\n\nIf this configuration throws a permission error during mounting, try:\n//tudelft.net/staff-umbrella/&lt;project_drive_name&gt;/ /media/&lt;server_mount_point&gt; cifs username=&lt;your_netID&gt;,noauto,uid=&lt;your_uid&gt;,gid=&lt;your_gid&gt;,forcegid,rw,_netdev\nUse the values for uid and gid from step 4.\n\n\nClose the file editor and save the changes:\n\nvinano\n\n\nUse Control+C followed by :wq to save the file and close it to get back to your terminal.\n\n\nAs indicated by the nano interface, use Control+O to write out the file. Then, confirm your choice of filename by hitting Enter. Finally, exit the file with Control+X\n\n\n\n\n\nStep 6. Mount the project drive\nTo mount the project drive, execute the command:\nsudo mount /media/&lt;server_mount_point&gt;\nYou can also unmount the drive with:\nfusermount -u /media/&lt;server_mount_point&gt;\nThe project drive will not mount automatically, so you will need to remount it manually each time you restart the server.\n\n\n\n\n\n\n Note\n\n\n\nIf the step above does not work, it probably means that the packages for mounting cifs-type filesystems haven‚Äôt been installed. Depending on your Linux flavour you will need to install them using:\n\nUbuntu/DebianRedhat/Centos/Fedora\n\n\nsudo apt install cifs-utils\n\n\nsudo yum install cifs-utils\n\n\n\n\n\n\n\n\nNotes and next steps\nThe steps above can also be used to mount any storage offered by TU Delft with a WebDav link (staff-homes, staff-groups, staff-bulk, student-homes, student-groups and apps). Simply use the latter half of the URL from the WebDav web link of your storage drive, which will change from staff-umbrella (Project Data Storage drive) to something else, depending on the storage drive you would like to mount.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Storage options",
      "Mount *Project Drive* on server"
    ]
  },
  {
    "objectID": "docs/data/data_storage/security.html",
    "href": "docs/data/data_storage/security.html",
    "title": "Data security",
    "section": "",
    "text": "Data security is a critical aspect of research data management, ensuring that sensitive information is protected from unauthorized access, loss, or corruption. It involves implementing measures to safeguard data throughout its lifecycle, from collection and storage to sharing and archiving.\nTU Delft implements robust authentication and authorization mechanisms, such as strong passwords and two-factor authentication, to restrict data access only to approved individuals and systems.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data security"
    ]
  },
  {
    "objectID": "docs/data/data_storage/security.html#core-principles-of-research-data-security",
    "href": "docs/data/data_storage/security.html#core-principles-of-research-data-security",
    "title": "Data security",
    "section": "Core principles of research data security",
    "text": "Core principles of research data security\nKey principles include:\n\nAvailability: Ensuring that authorized users can access the data and associated systems when needed for their research activities.\nConfidentiality: Ensuring that data is accessible only to individuals who are explicitly authorized to view or use it. ¬†\nIntegrity: Maintaining the accuracy, completeness, and consistency of data throughout its entire lifecycle, preventing unauthorized modification. Regularly backing up data to prevent loss due to accidental deletion, hardware failure, or cyberattacks, and having a recovery plan in place to restore data in case of incidents.\n\n\n\n\n\n\n\n Tip: Explore our Data backup guide for more information.\n\n\n\n\nVerifying data integrity\nA checksum is a small-sized block of data derived from another block of digital data. By comparing the checksum of a file you have downloaded, backed-up, or transferred with the one provided by the source, you can verify if the file is an exact and untampered copy.\n\nStep 1: Obtain an original checksum\nBefore you can verify a file, you need an original checksum value. This is typically provided by a website or source from where you downloaded the file. It could also have been generated by yourself from Step 2. It usually is a long string of alphanumeric characters and is often labeled as MD5, SHA-1, or SHA-256.\nFor example, you might see something like this on a download page:\nSHA-256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\n\nStep 2: Generate a checksum for your file\nYou can use different command-line tools to calculate the checksum of the file you have downloaded, transferred or backed-up.\n\nWindows PowerShellmacOS terminalLinux terminal\n\n\nOpen PowerShell (press Win + X and select ‚ÄúPowerShell‚Äù).\nUse the Get-FileHash command. By default, it uses SHA-256.\nGet-FileHash C:\\path\\to\\your\\file\nTo use a different algorithm like MD5, specify it:\nGet-FileHash C:\\path\\to\\your\\file -Algorithm MD5\n\n\nOpen your terminal (you can find it in Applications &gt; Utilities).\nFor SHA-256, use the shasum -a 256 command:\nshasum -a 256 /path/to/your/file\nFor MD5, use the md5 command:\nmd5 /path/to/your/file\n\n\nOpen your terminal.\nFor SHA-256, use the sha256sum command:\nsha256sum /path/to/your/file\nFor MD5, use the md5sum command:\nmd5sum /path/to/your/file\n\n\n\n\n\nStep 3: Compare the checksums\nFinally, compare the checksum you generated in Step 2 with the original checksum from Step 1. There might be cases where you need to do Step 2 in two different systems, such as a local machine and a remote server.\nIf they match exactly, your file is a perfect copy.\nIf they do not match, the file was likely corrupted during download or has been tampered with. You should delete the file and download it again.\n\n\n\nOther considerations\nOur Data privacy guide has a list of TU Delft privacy related resources that should be helpful if you are working with highly sensitive data, such as personal information, health data, or copyrighted material.\n\n\n\n\n\n\n Important\n\n\n\n\nReach out to your faculty data steward for more information.\nExplore TU Delft security and privacy related resources:\n\nTU Delft Security and Privacy SharePoint site",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage",
      "Data security"
    ]
  },
  {
    "objectID": "docs/data/data_storage/storage.html",
    "href": "docs/data/data_storage/storage.html",
    "title": "Data storage",
    "section": "",
    "text": "In the context of research data management, ‚ÄúData Storage‚Äù refers to securely housing research data on various digital media or infrastructure, having it readily accessible for ongoing work, analysis, and collaboration throughout the active phase of a research project. It involves the systems and practices used to maintain the availability, integrity, and accessibility of data while it is actively being used, processed, or shared by the research team. This includes both local and networked solutions, serving as the persistent layer for active research operations involving collaborators internal to TU Delft, or external.\nData storage, rather than being confined to a single step, functions as a persistent foundation, supporting activities from initial data acquisition through active analysis. As a researcher, you may frequently move between phases, such as returning to data collection after initial analysis reveals gaps, or refining processing methods based on analytical outcomes. This interconnected and iterative nature is crucial for effective research data management, as it means that planning for later stages, like archiving and publishing, must occur early, and ongoing management activities, such as storage and security, are not isolated tasks but integral to every step of active research.\nThis section details the recommended storage solutions available to TU Delft researchers, emphasizing institutional and national (Dutch/European) services and providing guidance on their appropriate use. It also highlights the importance of data security, sharing, and backup as essential components of effective data storage practices.\n\n\n\n Storage options\nLocal and networked, institutional and national.\n\nLearn more ¬ª\n\n\n\n Data security\nEnsuring confidentiality, integrity, and availability.\n\nLearn more ¬ª\n\n\n\n Data sharing\nMaking active data available to internal and/or external collaborators.\n\nLearn more ¬ª\n\n\n\n Data backup\nStrategies to prevent data loss.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Storage"
    ]
  },
  {
    "objectID": "docs/data/fair_data/fair.html",
    "href": "docs/data/fair_data/fair.html",
    "title": "FAIR data",
    "section": "",
    "text": "The goal of the FAIR principles is to improve research transparency, reproducibility and reusability. FAIR data enhances research reliability, impact, and visibility, creating new collaboration opportunities for researchers. The acronym FAIR stands for:\n\nFindable: Data should be findable by users. A straightforward and reliable way to achieve this is by depositing data in a repository with appropriate metadata, tags, and identifiers to improve searchability.\nAccessible: Data should be accessible to authorized users. This does not mean all data must be publicly available; rather, it should be ‚Äúas open as possible, as closed as necessary‚Äù. If data cannot be made publicly available due to sensitive information or commercial restrictions, the metadata should still be made public to indicate where and how the data can be accessed if needed.\nInteroperable: Data should be in standardized formats with a clear structure to allow it to be interoperable across different systems, enabling data to be used in various applications by both the data owners and other users.\nReusable: For data to be reusable, users must understand what the data represents, the information it contains, and how to interpret its structure and format. Good documentation and an appropriate license are key in reusability, as these enable others to understand and work with your dataset, especially upon publication.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "FAIR Data"
    ]
  },
  {
    "objectID": "docs/data/planning/planning.html",
    "href": "docs/data/planning/planning.html",
    "title": "Planning",
    "section": "",
    "text": "When planning your research, it is important to consider how you will manage your data. This is the very first step in the research data lifecycle; you define your research question, and pinpoint relevant data sources. This includes planning how you will collect, store, and share your data, as well as how you will ensure its security and privacy. You formulate a plan for your data management, which is a living document that you can update as your research progresses.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/data/planning/planning.html#data-management-plan",
    "href": "docs/data/planning/planning.html#data-management-plan",
    "title": "Planning",
    "section": "Data management plan",
    "text": "Data management plan\nTU Delft emphasizes the importance of data management planning and asks researchers to create a data management plan (DMP) at the start of their research project. In many cases this is also a formal requirement. TU Delft provides a dedicated tool called DMPonline to help you write a DMP.\n\n\n\n\n\n\n Important\n\n\n\nPlease consult your faculty data steward for more information regarding DMPs and the specific requirements for your faculty.\nAlso, you can find more information on DMPs:\n\nPhD Supervisors guide - Data management plan\nTU Delft RDM page - ‚ÄúData Management Plans‚Äù\nTU Delft RDM page - ‚ÄúDMPonline‚Äù\nRDM 101 course - Module 5 - DMP",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/data/planning/planning.html#what-to-do-if-you-have-sensitive-data",
    "href": "docs/data/planning/planning.html#what-to-do-if-you-have-sensitive-data",
    "title": "Planning",
    "section": "What to do if you have sensitive data?",
    "text": "What to do if you have sensitive data?\nIf your datasets contains personal, confidential, or otherwise sensitive information, you need to consider how to handle this data appropriately.\n\n\n\n Privacy\nHandle sensitive data.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/folder-regular.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Data Management**",
      "Planning"
    ]
  },
  {
    "objectID": "docs/guides.html",
    "href": "docs/guides.html",
    "title": "How-to Guides",
    "section": "",
    "text": "The guides are split into a few major sections:\n\nComputing Infrastructure contains information and guides on TU Delft ICT infrastructure.\nData Management contains information and guides on using TU Delft data storage options and best-practices for FAIR data.\nResearch Software contains information for creating FAIR research software.\nResources contains a collection of courses, workshops, and references."
  },
  {
    "objectID": "docs/infrastructure/VPS_SSH.html",
    "href": "docs/infrastructure/VPS_SSH.html",
    "title": "Set up SSH tunneling for a VPS",
    "section": "",
    "text": "This guide explains how to set up a secure, single-step SSH connection to a Virtual Private Server (VPS) at TU Delft using SSH tunneling. By default, connecting to a VPS requires first accessing a Bastion Host (an intermediary server controlling access), making it a two-step process. Therefore, it is a two-step process: to reach a remote host, a user has to connect first to the bastion host and from there to the VPS. However, by using SSH tunneling and SSH keys, you can connect to your VPS and other remote hosts in a single step.\nWith the method described below, you will be able to connect directly from your local machine to your VPS, bypassing the need to log in to the bastion host separately. This setup also simplifies secure file transfers between your local machine and the VPS.\n\n\n\n\n\n\n Accessing a VPS\n\n\n\nDepending on your geogaphic location, access to a VPS via SSH may be blocked by the TU Delft firewall. In such cases, you must use a VPN connection via eduVPN. Access if usally blocked if you are connecting from your home network.\n\n\n\nPrerequisites\nBefore starting, you need:\n\nA TU Delft NetID.\nAccess to a VPS provided by TU Delft ICT, including a username and password.\nAn SSH client installed on your local machine (usually included with most Linux and macOS distributions; for Windows, you can use a third-party SSH client like PuTTY).\nALinux or macOS terminal\n\n\n\nSteps for Linux and macOS\n\n\n\n\n\n\n Summary of steps\n\n\n\n\nCreate SSH keys.\nCopy SSH keys to bastion host and remote server.\nCreate a new host for SSH connection.\nTest connection\n\n\n\n\nSet up SSH tunneling for a host (Linux Terminal)\n\nIf you do not have an SSH key-pair, create one on the local machine. Go to the terminal and enter the following command. Replace &lt;my-keyname&gt; with a name of your choice for the SSH key, e.g., id_rsa or id_ed25519.\n\n$  ssh-keygen -t ed25519 -f ~/.ssh/&lt;my-keyname&gt;\nYou will be promted to crate a passphrase. We recommend you to add one to make the connection more secure. The passphrase will be asked every time you connect to the VPS. To skip the passphrase, press Enter when prompted. You should see something like this:\nGenerating public/private ed25519 key pair.\nEnter passphrase for \"ed25519\" (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in ~/.ssh/&lt;my-keyname&gt;\nYour public key has been saved in ~/.ssh/&lt;my-keyname&gt;.pub\nThe key fingerprint is:\nSHA256:6j06srvun06gJ5UCmD+MVq6RsPuytCO5mF4hTELnWTg root@local-machine\nThe key's randomart image is:\n+--[ED25519 256]--+\n| . ...           |\n|o.oEo            |\n|*. +.            |\n|=*+  .           |\n|o*=o+   S        |\n|..+=.. .         |\n|.+o.. o          |\n|*+oo.o.o.        |\n|B*oo*B*o..       |\n+----[SHA256]-----+\nA private and public key will be added to ~/.ssh.\nThe public key is the file with the .pub extension, e.g., &lt;my-keyname&gt;.pub\n\n\n\n\n\n\n Tip\n\n\n\nSimilar to passwords, it is advised to rotate your SSH keys regularly (e.g., every 6 months). You can do this by generating a new key pair and replacing the old one on your local machine and VPS.\n\n\n\nLog in to your VPS and, copy the content of your public key into the VPS ~/.ssh/authorized_keys file. You can achieve this by copying the content of the public key file to the clipboard and pasting it into the authorized_keys file on the VPS. Be mindful and not remove anything from this file, or other SSH connections might stop working. Finally, save the file.\nCreate a new host for SSH connection. On your local machine, edit the ~/.ssh/config file and add the following configuration. If the file does not exist, create it.\n\nHost &lt;host-nickname&gt;\n    HostName &lt;target-host&gt;\n    User &lt;target-username&gt;\n    ProxyJump &lt;target-username&gt;@linux-bastion-ex.tudelft.nl\n    IdentityFile ~/.ssh/&lt;my-keyname&gt;\nReplace: : a name of your choice for the target host, e.g., my-server : the actual name of the target host (FQDM), e.g, server.tudelft.nl : the username used to log in to the target host, usually your NetID : the username used to log in to the bastion server : the name of the SSH private key you created, e.g., id_rsa. If your private key is stored in a different location, replace the path accordingly.\n\nTest the SSH tunneling connection. Connect to the VPS using SSH tunneling by typing the command below. Use your bastion-password when asked. This is usually the password associated with your NetID.\n\n$ ssh &lt;host-nickname&gt;\nIf you encounter problems with the connection, use the debug mode ssh -vvv &lt;host-nickname&gt; to find out what might have gone wrong. This command will provide detailed information about the connection process and can help you troubleshoot any issues.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Setting up SSH tunneling"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html",
    "href": "docs/infrastructure/VPS_request.html",
    "title": "Request a VPS",
    "section": "",
    "text": "This guide describes the essentials for requesting and setting up a TU Delft managed server. A server is a computer that can handle requests. Servers are often a critical component of architectural solutions for data management. There are many reasons why you as a researcher may need to request a server, for example:\n\nAutomation of a process in your data collection\nSet up runners for the TU Delft Gitlab\nA part of your analysis should be running continuously, and you cannot do it with your own machine\nSpecific functionality (web server)\nYou need a machine to handle large amounts of requests\nYou want to outsource the maintenance of a server to TU Delft ICT\nRely on safety and security administrated by the University, including backups\nSecuring/controlling access\nMounting storage drive",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#background",
    "href": "docs/infrastructure/VPS_request.html#background",
    "title": "Request a VPS",
    "section": "",
    "text": "This guide describes the essentials for requesting and setting up a TU Delft managed server. A server is a computer that can handle requests. Servers are often a critical component of architectural solutions for data management. There are many reasons why you as a researcher may need to request a server, for example:\n\nAutomation of a process in your data collection\nSet up runners for the TU Delft Gitlab\nA part of your analysis should be running continuously, and you cannot do it with your own machine\nSpecific functionality (web server)\nYou need a machine to handle large amounts of requests\nYou want to outsource the maintenance of a server to TU Delft ICT\nRely on safety and security administrated by the University, including backups\nSecuring/controlling access\nMounting storage drive",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#what-this-documentation-will-help-you-achieve",
    "href": "docs/infrastructure/VPS_request.html#what-this-documentation-will-help-you-achieve",
    "title": "Request a VPS",
    "section": "What this documentation will help you achieve",
    "text": "What this documentation will help you achieve\nThis documentation helps researchers to request a VPS and data storage on the Project Drive. There are other forms of storage available, but Project Drive storage is often recommended for expandable and secure data preservation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#prerequisites",
    "href": "docs/infrastructure/VPS_request.html#prerequisites",
    "title": "Request a VPS",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID\nBasic knowledge of Linux (if requesting Linux server, recommended)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#toolssoftware",
    "href": "docs/infrastructure/VPS_request.html#toolssoftware",
    "title": "Request a VPS",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nFor Windows users, you will need a programming and runtime environment like Cygwin or SSH client like PuTTY in order to access the VPS running Linux",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#steps",
    "href": "docs/infrastructure/VPS_request.html#steps",
    "title": "Request a VPS",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the TU Delft server request form\nFill and send the form according to your preferences and needs\nReceive confirmation of server deployment from TU Delft ICT\nLogin to your server for the first time\n\n\nStep 1: Navigate to the TU Delft server request form\nYou can make a request for a server via the TopDesk self service portal.\n\n\nStep 2: Fill and send the form according to your preferences and needs\nThe form is divided into three sections: Caller Details, General Questions, and Technical Questions.\nCaller Details should contain the contact information of the main administrator of this server. If you select your name, the fields below should be auto-populated with your building, phone number, email, department/program, organizational unit, and (sometimes) room.\n\nThe last question in the Caller Details section access to the server by external users. Generally speaking, granting access to TU Delft-managed servers is not recommended, but if it is necessary you can add the contact details of the external party and the reason(s) for which they should have access. You will need to provide a company-affiliated email address for the external user, and the request may or may not be granted by ICT.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that a server provides access to the backend of your application. If for example you want to deploy a web server to share your data widely, users do not need direct access to the server in order to access the data itself.\n\n\nThe next section contains General Questions about the name and purpose of your server. If you plan to use this server ongoing into the future, you can either leave the field ‚ÄúExpiration Date‚Äù blank or add a date in 10+ years. TU Delft ICT will alert you when the expiration date you select is nearing.\n\n\n\n436ab820-8194-4212-8622-93758ba56f56\n\n\nYou can add the netIDs of your collaborators who should have read/write access to the server, and optionally the netID of your faculty Data Steward or of a DCC Team member if you would like their help. You can find your faculty Data Steward contact information here.\nThe Technical Questions section asks you to specify an operating system and some other technical details about your server configuration. You can choose between four basic configuration options - you can of course consider which one best fits your needs. If you are new to working with servers, generally the best choice is Basic configuration 4.\nThe next question deals with opening ports to the server through the TU Delft firewall. Ports are essentially gateways to the server that are specific to different purposes. For example, port 80 is opened to handle HTTP requests, port 20 is opened to handle SSH requests, port 3306 is opened to allow access to a MySQL database, and port 443 is opened to handle HTTPS requests. If you are planning to use your VPS as a webserver, ports 80 and 443 should be open. You can use this space to ask ICT to do so.\nThe next section, FQDN, is the way you can refer to your server on the internet. The recommendation is a format like &lt;servername&gt;.&lt;facultyabbreviation&gt;.tudelt.nl. In general, it‚Äôs best to keep names relatively short and informative to make it easy to reference and remember.\nYou should also be sure to check the instructions in the form and contact your faculty Data Steward or Faculty IT Manager if you need further explanation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#initial-configuration-of-your-vps",
    "href": "docs/infrastructure/VPS_request.html#initial-configuration-of-your-vps",
    "title": "Request a VPS",
    "section": "Initial Configuration of your VPS",
    "text": "Initial Configuration of your VPS\nA few days after submitting the request, you will receive an email from ICT with login details. You can connect to your VPS via ssh. If you are in the windows environment, it is recommended to install Cygwin and its packages to be able to use the ssh command in a non-unix environment. The unix based systems (e.g., mac) contain ssh by default. In order to login to your VPS, you need to first ssh to the bastion server with ssh &lt;username&gt;@linux-bastion-ex.tudelft.nl and then from there login to your server ssh &lt;servername&gt;. The first thing we recommend to do after logging into the server is to update the pre-installed packages:\n\nDebian (Ubuntu)RedHat\n\n\nsudo apt-get update && sudo apt-get upgrade\n\n\nsudo yum update\n\n\n\nIt would be also useful to set a password for the VPS when you log in. You can do that by passwd command.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/VPS_request.html#next-steps",
    "href": "docs/infrastructure/VPS_request.html#next-steps",
    "title": "Request a VPS",
    "section": "Next Steps",
    "text": "Next Steps\nCommon next steps after obtaining a VPS and storage include initial configuration steps such as establishing a connection via SSH and mounting Project Drive storage; and also software installation steps for tools like Docker, and setting up Apache Web Server. We will add more documentation when common installations come to our attention, so please reach out to us with your questions or suggestions.\n\nInstall Docker\nConfigure Docker for use as non-root\nConfigure a runner for the TU Delft Gitlab\nApache Web Server",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers",
      "Request a VPS"
    ]
  },
  {
    "objectID": "docs/infrastructure/getting_started.html",
    "href": "docs/infrastructure/getting_started.html",
    "title": "Overview",
    "section": "",
    "text": "Under construction! üèóÔ∏è",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_groups.html",
    "href": "docs/infrastructure/gitlab/gitlab_groups.html",
    "title": "Creating GitLab groups",
    "section": "",
    "text": "Groups and subgroups are similar to directories in the operating systems. In windows or Mac, we create directories to organise files or other directories. For example, imagine a scenario where you want to keep your photos in an organised manner. To this end, you may organise your photos on multiple levels. In the first level, perhaps you broadly classify them and then in the next levels you narrow it down to more specific subjects. Similarly, in the GitLab, a group is used as a binder to put together projects or even other groups. For example, if three themes are researched in a lab, each of those themes could be a group, and all the research in a certain theme fall into its corresponding group. Sometimes there are also sub-themes. For every sub-theme you can create a subgroup (within the group corresponding to the broader theme) and assign the research close to the sub-theme there.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Creating GitLab groups"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_groups.html#background",
    "href": "docs/infrastructure/gitlab/gitlab_groups.html#background",
    "title": "Creating GitLab groups",
    "section": "",
    "text": "Groups and subgroups are similar to directories in the operating systems. In windows or Mac, we create directories to organise files or other directories. For example, imagine a scenario where you want to keep your photos in an organised manner. To this end, you may organise your photos on multiple levels. In the first level, perhaps you broadly classify them and then in the next levels you narrow it down to more specific subjects. Similarly, in the GitLab, a group is used as a binder to put together projects or even other groups. For example, if three themes are researched in a lab, each of those themes could be a group, and all the research in a certain theme fall into its corresponding group. Sometimes there are also sub-themes. For every sub-theme you can create a subgroup (within the group corresponding to the broader theme) and assign the research close to the sub-theme there.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Creating GitLab groups"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_groups.html#what-this-documentation-will-help-achieve",
    "href": "docs/infrastructure/gitlab/gitlab_groups.html#what-this-documentation-will-help-achieve",
    "title": "Creating GitLab groups",
    "section": "What this documentation will help achieve",
    "text": "What this documentation will help achieve\nThis documentation helps researchers to set up a group and assign contributors to the projects within the GitLab.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Creating GitLab groups"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_groups.html#prerequisites",
    "href": "docs/infrastructure/gitlab/gitlab_groups.html#prerequisites",
    "title": "Creating GitLab groups",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Creating GitLab groups"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_groups.html#steps",
    "href": "docs/infrastructure/gitlab/gitlab_groups.html#steps",
    "title": "Creating GitLab groups",
    "section": "Steps",
    "text": "Steps\n\nLog-in to the GitLab with your netid and password.\nEnter to the Groups section.\nCreate a group.\nAssign contributors to the group.\n\n\nStep 1. Log-in to the GitLab with your netid and password\nTo create a group in GitLab, first you need to log in to the TU Delft‚Äôs instance of GitLab.\n Figure 1: The login page of TU Delft‚Äôs GitLab.\n\n\nStep 2. Enter to the Groups section\nAfter the log-in, from the top bar click on groups and then Your groups. I\n Figure 2: The Group section of the GitLab.\n\n\nStep 3. Create a group\nIn the new web page, click on the New group and then fill in the form by choosing a suitable group name and then selecting the visibility level. You can later on change both group name and visibility level.\n Figure 3: Creation of a new Group in the GitLab.\n\n\nStep 4. Assign contributors to the group.\nAfter creating a group, it appears in the Groups tab. The group owner can add different people to the projects. This can be done by entering a project and then clicking on the Members on the left panel and filling in the form (Figure 4). A very important part of this form is permissions which are explained here in detail.\n Figure 4: Assigning members to a project.\nYou can also create subgroups within a group. To create a subgroup, you need to enter an existing group and press the New subgroup button. Then, similar to creating a group, you fill in the subgroup name and select the degree of visibility for the subgroup. The subgroup visibility level is always a subset of its (parent) group visibility, hence if the parent visibility is private, there is no choice except private for the subgroup visibility but if the parent visibility is public, subgroup visibility could be either public, internal, or private. The same logic applies when you create a subgroup within another subgroup.\n Figure 5: Creating a subgroup.\nAfter setting up the group, (sub groups,) projects, and assigning the collaborators to the projects, they can start working with the remote repository and transfer their scripts there.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Creating GitLab groups"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html",
    "href": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html",
    "title": "Transfer ownership of a GitLab repository",
    "section": "",
    "text": "GitLab repositories belonging to employees leaving the TU Delft might be deleted in future.\nFrom the TU Delft GitLab help documentation, we read the following:\nCurrent situation\nFirst, if you use Git on your computer, you will have the entire history also locally on your machine. Without a valid TU Delft account, your GitLab access will become inactive. There are currently no plans to delete any content when an account becomes inactive.\nFuture situation\nAt some point in the future, repositories of former TU Delft employees may be deleted. To avoid losing information, it is recommended to transfer ownership of your repositories to a current TU Delft employee when you leave.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Transfer ownership of a GitLab repository"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#background",
    "href": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#background",
    "title": "Transfer ownership of a GitLab repository",
    "section": "",
    "text": "GitLab repositories belonging to employees leaving the TU Delft might be deleted in future.\nFrom the TU Delft GitLab help documentation, we read the following:\nCurrent situation\nFirst, if you use Git on your computer, you will have the entire history also locally on your machine. Without a valid TU Delft account, your GitLab access will become inactive. There are currently no plans to delete any content when an account becomes inactive.\nFuture situation\nAt some point in the future, repositories of former TU Delft employees may be deleted. To avoid losing information, it is recommended to transfer ownership of your repositories to a current TU Delft employee when you leave.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Transfer ownership of a GitLab repository"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#purpose-of-this-guide",
    "href": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#purpose-of-this-guide",
    "title": "Transfer ownership of a GitLab repository",
    "section": "Purpose of this guide",
    "text": "Purpose of this guide\nThis guide provides the steps required to secure access to repositories of employees who will leave the TU Delft.\nIf they have access to your projects, they will still have access after you leave, as long as the projects still exist. You can control who has access to your projects by going to Project Information &gt; Members in the sidebar of your repository. To be safe, transfer the ownership of the projects to a current TU Delft employee when you leave.\nYou can transfer a project to another user‚Äôs GitLab namespace. Read what a namespace is here.\n\n\n\n\n\n\nNote\n\n\n\nProviding a more straightforward way to transfer ownership in GitLab was raised as an issue in 2016 but the issue is still open; you can follow the progress here if interested.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Transfer ownership of a GitLab repository"
    ]
  },
  {
    "objectID": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#steps",
    "href": "docs/infrastructure/gitlab/gitlab_transfer_ownership.html#steps",
    "title": "Transfer ownership of a GitLab repository",
    "section": "Steps",
    "text": "Steps\nThe steps will guide you through transferring repository ownership between TU Delft employees through an intermediary GitLab group:\n\n\n\n\n\n\n\nNote\n\n\n\nSummary (based on this Stack Overflow post): ‚Äã‚ÄãMove your project from your namespace to a group where both you and the other user are owners, then the other user can transfer it to their own namespace\n\n\n\nStep 1. Create a new group\nCreate a new group if you don‚Äôt have one that you want to use (Menu &gt; Groups &gt; Create group)\n\n\nStep 2. Invite members\nMake sure that both the maintainer of the project repository, and the person who it will be transferred to, are members of the group and have the Owner role (to add a new owner: go to the group namespace, then from the sidebar choose Group information &gt; Members &gt; Invite members.\n\n\nEnter the username or email of the person you want to invite and change the role to ‚ÄòOwner‚Äô. Click ‚ÄòInvite‚Äô.)\n\n\n\nStep 3. Transfer project to group\nHave a maintainer of the project repository transfer it to the group namespace (go to the project namespace, then from the sidebar go to Settings &gt; General &gt; Advanced &gt; Transfer Project)\n\n\n\nStep 4. Check email\nAfter doing this, the maintainer(s) of the project will get an email:\n\n\n\nStep 5. Transfer project to user\nNow the person who the project is being transferred to can move it to their own namespace (go to the project namespace, then from the sidebar, go to Settings &gt; General &gt; Advanced &gt; Transfer Project like before).\n\n\nStep 6. Optional: remove group\nIf desired, the group can be deleted after the transfer is complete (go to the group namespace, then from the sidebar go to Settings &gt; General &gt; Advanced &gt; Remove Group)",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "TU Delft GitLab",
      "Transfer ownership of a GitLab repository"
    ]
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html",
    "href": "docs/infrastructure/giving_sudo_privilege.html",
    "title": "Giving sudo privilege to a user",
    "section": "",
    "text": "There are some tasks in Linux that requires superuser (sudo) permission. For instance editing the /etc/fstab file, rebooting the system, mounting a drive, viewing /etc/passwd file and many other tasks cannot be done without sudo privilege."
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html#background",
    "href": "docs/infrastructure/giving_sudo_privilege.html#background",
    "title": "Giving sudo privilege to a user",
    "section": "",
    "text": "There are some tasks in Linux that requires superuser (sudo) permission. For instance editing the /etc/fstab file, rebooting the system, mounting a drive, viewing /etc/passwd file and many other tasks cannot be done without sudo privilege."
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html#what-this-documentation-will-help-achieve",
    "href": "docs/infrastructure/giving_sudo_privilege.html#what-this-documentation-will-help-achieve",
    "title": "Giving sudo privilege to a user",
    "section": "What this documentation will help achieve",
    "text": "What this documentation will help achieve\nThis documentation helps you to give sudo privilege to a user."
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html#prerequisites",
    "href": "docs/infrastructure/giving_sudo_privilege.html#prerequisites",
    "title": "Giving sudo privilege to a user",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTU Delft netID\nBasic knowledge of Linux\nThose who grant sudo permission need to have sudo permission themselves."
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html#toolssoftware",
    "href": "docs/infrastructure/giving_sudo_privilege.html#toolssoftware",
    "title": "Giving sudo privilege to a user",
    "section": "Tools/Software",
    "text": "Tools/Software\n\nFor Windows users, you will need a programming and runtime environment like Cygwin or SSH client like PuTTY in order to access the VPS running Linux."
  },
  {
    "objectID": "docs/infrastructure/giving_sudo_privilege.html#steps",
    "href": "docs/infrastructure/giving_sudo_privilege.html#steps",
    "title": "Giving sudo privilege to a user",
    "section": "Steps",
    "text": "Steps\n\nGetting the username(s) of sudo access candidates\nNavigating to the sudoers directory\nCreate an access file for each of the candidates\nEditing the access files\n\n\nStep 1. Getting the username(s) of sudo access candidates\nYou can get the usernames by checking the /etc/passwd file. This file contains all the usernames and their login information.\n\n\nStep 2. Navigating to the sudoers directory\nThe list of users who posses the sudo permission is in the /etc/sudoers.d/ folder, so navigate to that folder.\n\n\nStep 3. Create an access file for each of the candidates\nIf you use ls command to check the available files in this folder, you can see there are some files that their name start with a number and then a dash and then a user name (&lt;number&gt;-&lt;username&gt;). In Linux for every user with a sudo permission, there is a file like that, so we duplicate one of the existing files and rename them for each of the sudo candidates (in fact, name of those file doesn‚Äôt matter; however, it helps us to understand which users have sudo privilege without opening them).\n\n\nStep 4. Editing the access files\nIn this step you need to open each of the newly duplicated files and replace the old username in the second line with the sudo candidate username. After the edit, you just save the changes and exit the file."
  },
  {
    "objectID": "docs/infrastructure/remote_servers.html",
    "href": "docs/infrastructure/remote_servers.html",
    "title": "Servers",
    "section": "",
    "text": "TU Delft offers its employees the use of physical or virtual servers. These servers are known as faculty managed servers and can be requested to conduct work related to a specific project within a faculty.\n\n\n\n\n\n\n Note\n\n\n\nIt is not possible to use the servers to set up services that are already provided by the ICT department.\n\n\n\nVirtual vs physical servers\nThere is a choice between virtual and physical servers. Virtual servers are provided free of charge and can be requested via TOPdesk. Physical servers can be requested by contacting the faculty‚Äôs IT manager, and any associated costs are paid by the purchasing department.\nIn most cases, a virtual server is the most suitable option. However, a physical server may be necessary when it is intended for use as a GPU or computing cluster.\n\n\nServer configuration\nWhen requesting a virtual server, users can choose from a range of predefined hardware and operating system configurations. The following operating systems are available: Windows Server 2019, Windows Server 2022, Red Hat Enterprise Linux (latest supported version), and Ubuntu (latest LTS version).\nIf additional capacity is needed, it can be requested via the ‚ÄòICT malfunction‚Äô or ‚ÄòRequest ICT service‚Äô forms in TOPdesk. This includes options such as increasing the number of processors, cores per processor, RAM, or disk storage.\nSome considerations:\n\nICT provides the server, operating system, and network access.\nUsers are granted administrator privileges, allowing them to install any required software, provided it complies with the conditions specified in the request form.\nAccess can be granted to both TU Delft members and external users.\nICT provides daily backups, restoration services, and virus scanning for Windows servers.\nICT ensures that the server operating system remains up to date (e.g., security patches), except for Linux systems.\n\n\n\n\n\n\n\n Tip\n\n\n\nDetailed information on managing the server, including network and firewall settings, is provided in the TOPdesk application form.\n\n\n\n\nExample use cases\n\nPerforming computational or data processing tasks that require a dedicated server environment.\nRunning an instance of a service or application, such as ABAQUS, COMSOL, or other specialized tools for a lab or research group.\nHosting a static website, a web application, or an API for a project.\nHosting databases, such as MySQL, PostgreSQL, MongoDB, or other database management systems.\nDeploying and managing TU Delft GitLab runners for CI/CD pipelines.\n\n\n\nRelevant links\n\nIntranet page for faculty managed servers\nTOPdesk form to request a new virtual server\nIntranet page for faculty IT managers",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/gear.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Computing Infrastructure**",
      "Remote servers"
    ]
  },
  {
    "objectID": "docs/resources/courses.html",
    "href": "docs/resources/courses.html",
    "title": "Courses and workshops",
    "section": "",
    "text": "The TU Delft organises training for researchers on data management, research software development, and open science. For an overview of available courses, please visit the TU Delft Library website.\n\n\n\nThe Carpentries teaches foundational coding and data science skills to researchers worldwide.\n Software Carpentry Lesson  Data Carpentry Lesson\n\nCourses overview\n\n\n\n\nThe Delft Institute for Computational Science and Engineering offers courses on supercomputing through the Delft High Performance Computing Center.\n\nCourses overview\n\n\n\n\nCodeRefinery teaches good practices for writing and maintaining research software, focussed on open source software. Their lessons cover version control, testing, continuous integration, documentation, and more.\n\nUpcoming workshops\n\n\n\n\nThe Research Engineering and Infrastructure Team offers support on software engineering, data science, and high-performance computing. They offer courses on Rust for Research, Python best practices, and Working with a cluster.\n\nCourses overview\n\n\n\n\nThe TU Delft offers a variety of Massive Open Online Courses.\n Open Science  Automated Software Testing  AI, Data & Digitalization  Unix Tools\n\nMOOC overview"
  },
  {
    "objectID": "docs/resources/courses.html#training-for-researchers-at-the-tu-delft",
    "href": "docs/resources/courses.html#training-for-researchers-at-the-tu-delft",
    "title": "Courses and workshops",
    "section": "",
    "text": "The TU Delft organises training for researchers on data management, research software development, and open science. For an overview of available courses, please visit the TU Delft Library website.\n\n\n\nThe Carpentries teaches foundational coding and data science skills to researchers worldwide.\n Software Carpentry Lesson  Data Carpentry Lesson\n\nCourses overview\n\n\n\n\nThe Delft Institute for Computational Science and Engineering offers courses on supercomputing through the Delft High Performance Computing Center.\n\nCourses overview\n\n\n\n\nCodeRefinery teaches good practices for writing and maintaining research software, focussed on open source software. Their lessons cover version control, testing, continuous integration, documentation, and more.\n\nUpcoming workshops\n\n\n\n\nThe Research Engineering and Infrastructure Team offers support on software engineering, data science, and high-performance computing. They offer courses on Rust for Research, Python best practices, and Working with a cluster.\n\nCourses overview\n\n\n\n\nThe TU Delft offers a variety of Massive Open Online Courses.\n Open Science  Automated Software Testing  AI, Data & Digitalization  Unix Tools\n\nMOOC overview"
  },
  {
    "objectID": "docs/resources/courses.html#external-training-opportunities",
    "href": "docs/resources/courses.html#external-training-opportunities",
    "title": "Courses and workshops",
    "section": "External training opportunities",
    "text": "External training opportunities\n\n\n\nThe eScience Center offers regular workshops on good practices for research software development and on intermediate-level topics such as GPU programming, Deep Learning, and Image Processing. Check out the overview of the training materials.\n\nUpcoming workshops\n\n\n\n\nSURF is the IT cooperative of Dutch education and research institutions and offers various workshops on using the supercomputers and storage solutions.\n\nUpcoming events\n\n\n\n\nThe Software Sustainability Institute in the UK provides training and resources to improve the quality of research software.\n\nTraining Hub\n\n\n\n\nTaxila provides an overview of training, learning, and teaching materials for trainers and trainees in the Netherlands.\n\nUpcoming workshops"
  },
  {
    "objectID": "docs/resources/courses.html#community-training-opportunities",
    "href": "docs/resources/courses.html#community-training-opportunities",
    "title": "Courses and workshops",
    "section": "Community training opportunities",
    "text": "Community training opportunities\n\n\n\nAre you learning R or looking for a friendly community to practice with? The R Caf√© is a welcoming space for R users of all levels - from absolute beginners to experienced programmers.\n\nUpcoming events\n\n\n\n\n4TU.ResearchData offers training resources and community engagement to research and research-support professionals working to make their research data FAIR.\n\nUpcoming events"
  },
  {
    "objectID": "docs/software/code_quality/code_smells.html",
    "href": "docs/software/code_quality/code_smells.html",
    "title": "Code smells",
    "section": "",
    "text": "CC-BY-4.0 ¬© 2021 Balaban et al.\n\n\nCode smells are software characteristics that suggest there might be an issue with the code‚Äôs design or implementation. While code smells themselves might not always indicate a bug or malfunction, they can make the code harder to understand and extend, which can lead to bugs and other issues down the line. Code smells are usually noticed and addressed during code reviews, when writing tests, adding new features, fixing bugs, and during automated code analysis.\n\n\n\n\n\n\n How to use these cards?\n\n\n\nEach guide provides an overview of a code smell, its symptoms and an example on how to refactor it. We don‚Äôt intend to cover all refactoring techniques, but we aim to provide a starting point for identifying and addressing common code smells.\n\n\n\n\n\nLong Method\nProblem: A function is very long and hard to understand.\n\n Refactor long methods\n\n\n\n\n\nLarge Classes\nProblem: A class contains too many responsibilities or functionalities.\n\n Refactor large classes\n\n\n\n\n\nCode Duplication\nProblem: The same or very similar code appears in multiple places.\n\n Refactor duplicate code\n\n\n\n\n\nHard-Coded Values\nProblem: Literal values (e.g., numeric values or strings) are directly embedded in the code.\n\n Refactor hard-coded values\n\n\n\n\n\nDeep Nesting\nProblem: There are excessive levels of nested for-loops or if-statements.\n\n Refactor nested logic\n\n\n\n\n\nMany Inputs\nProblem: Functions require a long list of parameters.\n\n Refactor argument lists\n\n\n\n\n\nInappropriate Intimacy\nProblem: Two classes or methods depend too much on each other‚Äôs internals.\n\n Refactor coupling\n\n\n\n\n\nSide Effects\nProblem: Changes in one part of the code cause unexpected behavior in another.\n\n Refactor side effects\n\n\n\n\n\nCommented out Code\nProblem: There is a significant amount of outdated or commented-out code.\n\n Refactor commented code\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nTen simple rules for quick and dirty scientific programming\nGood enough practices in scientific computing",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html",
    "title": "Deep Nesting",
    "section": "",
    "text": "‚ÄúCode is like humor. When you have to explain it, it‚Äôs bad.‚Äù\nCory House\nDeep nesting occurs when there are too many levels of indentation in the code, making it harder to understand, maintain, and debug. It can lead to reduced readability, and increases cognitive load for developers.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#symptoms",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#symptoms",
    "title": "Deep Nesting",
    "section": "Symptoms",
    "text": "Symptoms\n\nExcessive indentation makes it hard to track logic.\nMany nested if statements or for loops.\nHard-to-follow branching logic.\nSlow performance due to inefficient code.\nIncreased likelihood of bugs due to complexity.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-conditional-statements",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-conditional-statements",
    "title": "Deep Nesting",
    "section": "Example - Deeply nested conditional statements",
    "text": "Example - Deeply nested conditional statements\ndef validate_model_convergence(model):\n    if model.convergence &gt; 1:\n        if model.convergence &lt; 0.1:\n            if model.secondary_condition == True\n                return True\n            else:\n                return False\n        else:\n            return False\n    else:\n        return False\n\nSolutions\nRefactoring deep nesting improves readability and maintainability. Techniques to reduce deep nesting include:\n\nUsing early returns to eliminate unnecessary indentation.\nExtracting complex logic into helper functions for better modularity.\nUsing built-in functions like any and all to simplify conditions.\n\n\nSolution 1: Using early returns\ndef validate_model_convergence(model):\n    if model.convergence &lt;= 1:\n        return False\n    if model.convergence &gt;= 0.1:\n        return False\n    if not model.secondary_condition:\n        return False\n    return True\n        \nThia solution uses early returns to reduce the nesting level and make the code more readable. Each condition is checked separately, and if it fails, the function returns immediately, avoiding further nesting and evaluation of unnecessary conditions.\n\n\nSolution 2: Using all for conciseness\nAlternatively, we can use the all function to check multiple conditions in a single line, which can make the code more concise and easier to read.\ndef validate_model_convergence(model):\n    return all([\n        model.convergence &gt; 1,\n        model.convergence &lt; 0.1,\n        model.secondary_condition,\n    ])",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-loops",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#example---deeply-nested-loops",
    "title": "Deep Nesting",
    "section": "Example - Deeply nested loops",
    "text": "Example - Deeply nested loops\nIn this example, we have three nested loops to iterate over three 3D arrays and sum their corresponding elements. This code can be refactored using NumPy to improve performance and readability.\n# Create three random 10x10x10 arrays\nA = np.random.rand(10, 10, 10)\nB = np.random.rand(10, 10, 10)\nC = np.random.rand(10, 10, 10)\n\n# Using nested loops (inefficient)\nresult = np.zeros((10, 10, 10))\nfor i in range(10):\n    for j in range(10):\n        for k in range(10):\n            result[i, j, k] = A[i, j, k] + B[i, j, k] + C[i, j, k]\n\nSolution\nUsing NumPy, we can perform the same operation without nested loops, which is more efficient and easier to read.\n# Create three random 10x10x10 arrays\nA = np.random.rand(10, 10, 10)\nB = np.random.rand(10, 10, 10)\nC = np.random.rand(10, 10, 10)\n\n# Vectorized solution (fast & efficient)\nresult = A + B + C",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/deep_nesting.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/deep_nesting.html#key-takeaways",
    "title": "Deep Nesting",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nDeep nesting makes code harder to read and maintain.\nTechniques like early returns, helper functions, and built-in functions can simplify complex logic.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRealPython - ‚ÄúLook Ma, No For-Loops‚Äù",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Deep nesting"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html",
    "title": "Hard coding",
    "section": "",
    "text": "Hard-coding variables occurs when constants, configuration values, or logic are directly embedded into the code, making changes difficult. Hard-coding leads to rigid systems that require modifying the source code itself to change behavior, rather than adjusting parameters, settings, or external configurations.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#symptoms",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#symptoms",
    "title": "Hard coding",
    "section": "Symptoms",
    "text": "Symptoms\n\nMagic numbers or string literals appear directly in the code.\nYou find yourself searching the codebase for specific values to tweak behavior for different executions.\nThe same constant value appears multiple times, making updates error-prone.\nThe logic is less readable, since magic numbers don‚Äôt indicate what they represent.\nA small behavior change requires altering the core code, instead of adjusting an input parameters of config file.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#example---hard-coding-and-magic-numbers",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#example---hard-coding-and-magic-numbers",
    "title": "Hard coding",
    "section": "Example - Hard coding and magic numbers",
    "text": "Example - Hard coding and magic numbers\ndef calculate_area(radius):\n    # Hard-coded value of pi\n    return 3.14 * radius * radius # What if you need more precision?\n\ndef check_temperature(temperature):\n    # Hard-coded temperature values for thresholding\n    if temperature &gt; 30: # What does 30 represent\n        print(\"It's too hot!\")\n    elif temperature &lt; 10:\n        print(\"It's too cold!\")\n\nIssues\n\nThe value of pi is hard-coded as 3.14, which can lead to precision issues.\nThe temperature thresholds (30, 10) are buried in the logic, making them difficult to modify.\nThe meaning of 30 and 10 is unclear - are they for a specific region, season, or use case?\n\n\n\nSolution\nUsing named constants and configurable parameters makes the code more readable, maintainable, and flexible.\nimport numpy as np  # Use a library constant\n\nHOT_THRESHOLD = 30  # Defined constant for readability\nCOLD_THRESHOLD = 10  # Defined constant for readability\n\ndef calculate_area(radius):  # Default parameter allows customization\n    return np.pi * radius * radius # Use library constant for pi\n\ndef check_temperature(temperature, hot_threshold=HOT_THRESHOLD, cold_threshold=COLD_THRESHOLD):\n    if temperature &gt; hot_threshold: # Use named constants for readability\n        print(\"It's too hot!\")\n    elif temperature &lt; cold_threshold:\n        print(\"It's too cold!\")",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#example---rigid-code",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#example---rigid-code",
    "title": "Hard coding",
    "section": "Example - Rigid code",
    "text": "Example - Rigid code\nThis simulation hard-codes the time step and duration, making it rigid.\ndef run_simulation():\n    step_size = 0.01  # Fixed timestep\n    total_time = 10  # Fixed total duration\n    for t in range(0, total_time, step_size):\n        update_system(t)\n\nIssues\n\nChange the step size of total duration required modifying the source code.\nThe code is not reusable across different simulations.\n\n\n\nSolution\nIntroduce function parameters or external configuration files for flexibility and reproducibility.\ndef run_simulation(step_size=0.01, total_time=10):\n    for t in range(0, total_time, step_size):\n        update_system(t)\n\n# Calling with different configurations\nrun_simulation(step_size=0.05, total_time=20)  # Adjust without modifying the underlying code\nFor larger projects, moving configuration values to a separate file or class can further improve reproducibility and maintainability. Users would then only need to adjust the (text-based) configuration file without touching the core code.\n# config.yaml\nsimulation:\n  step_size: 0.01\n  total_time: 10\nimport yaml\n\ndef load_config(file_path=\"config.yaml\"):\n    with open(file_path, 'r') as file:\n        return yaml.safe_load(file)\n\nconfig = load_config()\nrun_simulation(config['simulation']['step_size'], config['simulation']['total_time'])",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/hardcoded_values.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/hardcoded_values.html#key-takeaways",
    "title": "Hard coding",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nUse named constants for improved readability.\nExternalize configuration values to allow easy adjustments without modifying the source code.\nConfiguration files or classes can further improve maintainability and reproducibility.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Hard-coded values"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html",
    "href": "docs/software/code_quality/code_smells/large_class.html",
    "title": "Large Classes",
    "section": "",
    "text": "A monolithic design is where an entire system is built as a single, tightly coupled unit without clear separation of responsibilities or modularization. This often leads to large, complex classes that handle multiple responsibilities, making the codebase harder to understand, modify, and maintain.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#symptoms",
    "href": "docs/software/code_quality/code_smells/large_class.html#symptoms",
    "title": "Large Classes",
    "section": "Symptoms",
    "text": "Symptoms\n\nLarge classes that try to handle too many responsibilities.\nCode duplication across multiple parts of the system.\nDifficulties in testing because changes in one part of the code affect others.\nLimited reusability of components due to tight coupling.\nSmall modifications require extensive changes across the codebase.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#example---violating-the-single-responsibility-principle",
    "href": "docs/software/code_quality/code_smells/large_class.html#example---violating-the-single-responsibility-principle",
    "title": "Large Classes",
    "section": "Example - Violating the Single Responsibility Principle",
    "text": "Example - Violating the Single Responsibility Principle\nIn this example, we are writing code for a temperature monitoring system. A bad design would be putting everything inside one big class:\nclass SensorSystem:\n    def __init__(self):\n        self.temperature = 0\n\n    def read_temperature(self):\n        # Simulated temperature reading\n        self.temperature = 25  \n        print(f\"Temperature: {self.temperature}¬∞C\")\n\n    def log_temperature(self):\n        # Simulated logging\n        print(f\"Logging temperature: {self.temperature}¬∞C\")\n\n    def send_alert(self):\n        if self.temperature &gt; 30:\n            print(\"ALERT: High temperature detected!\")\n\ndef main():\n    sensor_system = SensorSystem()\n    sensor_system.read_temperature()\n    sensor_system.log_temperature()\n    sensor_system.send_alert()\n\nif __name__ == \"__main__\":\n    main()\n\nSolution\nWe should split this class into smaller, focused classes:\n\nTemperatureSensor ‚Äì Handles sensor readings.\nLogger ‚Äì Handles logging.\nAlertSystem ‚Äì Handles alerts.\n\n\nFollow the Single Responsibility Principle (SRP) - Ensure that each class has only one job. If a class is doing too much, split its responsibilities into separate classes.\nUse dependency injection: Reduce class coupling by calling dependencies as arguments (injecting dependencies) rather than hard-coding them. This promotes modularity and testability, as well as making it easier to swap out components.\n\nclass TemperatureSensor:\n    def read_temperature(self):\n        # Simulated sensor reading\n        return 25  \n\nclass Logger:\n    def log(self, message):\n        print(f\"LOG: {message}\")\n\nclass AlertSystem:\n    def send_alert(self, temperature):\n        temperature_threshold = 30\n        if temperature &gt; temperature_threshold:\n            print(\"ALERT: High temperature detected!\")\n\nclass SensorSystem:\n    def __init__(self, sensor, logger, alert_system):\n        self.sensor = sensor\n        self.logger = logger\n        self.alert_system = alert_system\n\n    def monitor_temperature(self):\n        temperature = self.sensor.read_temperature()\n        self.logger.log(f\"Temperature: {temperature}¬∞C\")\n        self.alert_system.send_alert(temperature)\n\n# Dependency Injection\ndef main():\n    sensor = TemperatureSensor()\n    logger = Logger()\n    alert_system = AlertSystem()\n    sensor_system = SensorSystem(sensor, logger, alert_system) # dependencies injected\n\n    sensor_system.monitor_temperature()\n\nif __name__ == \"__main__\":\n    main()\nWhy is this better?\n\nNo unnecessary mixing of concerns.\nEasily swap different logging or alerting mechanisms.\nEach component can be tested in isolation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/large_class.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/large_class.html#key-takeaways",
    "title": "Large Classes",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nIf your class is doing too many things, split it into smaller, focused classes.\nUse dependency injection to keep components flexible and testable.\nFollowing modular design makes your code easier to understand, modify, and reuse.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nArjanCodes - Dependency Injection Best Practices",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Large classes"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html",
    "href": "docs/software/code_quality/code_smells/many_arguments.html",
    "title": "Many arguments",
    "section": "",
    "text": "When a function or method takes many parameters (inputs), it can become difficult to understand, maintain, and test. If a function needs a lot of information to work, it might be doing too many things at once, and this can confuse programmers or lead to mistakes. Refactoring the code to reduce the number of parameters or organizing the data in a more logical way can make the code easier to read and work with.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many inputs"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#symptoms",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#symptoms",
    "title": "Many arguments",
    "section": "Symptoms",
    "text": "Symptoms\n\nFunctions or methods with many parameters, especially if some of them are not used within the function.\nFunctions with long and confusing parameter lists, which are hard to remember or use correctly.\nCode that‚Äôs hard to change or update because of too many parameters being passed around.\nFunctions often require the same set of parameters, which can be grouped together logically.\n\n\n\n\n\n\n\nA good rule of thumb\n\n\n\n\n1-3 parameters: Generally fine.\n4-5 parameters: Might be acceptable if needed, but review if they can be grouped.\n6+ parameters: Strongly consider refactoring the function or method.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many inputs"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#example---long-parameter-list",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#example---long-parameter-list",
    "title": "Many arguments",
    "section": "Example - Long parameter list",
    "text": "Example - Long parameter list\nHere‚Äôs an example of a function that takes many parameters:\ndef process_machine_operation(machine_id, temperature, pressure, speed, duration):\n    # Perform machine operation\n    print(\"Machine ID:\", machine_id)\n    print(\"Temperature:\", temperature)\n    print(\"Pressure:\", pressure)\n    print(\"Speed:\", speed)\n    print(\"Operation Duration:\", duration)\n\n# Usage\nprocess_machine_operation(\n    machine_id=\"M001\", \n    temperature=100.5, \n    pressure=200.0, \n    speed=1500.0, \n    duration=5.0)\nThis function takes a lot of information at once: the machine ID, temperature, pressure, speed, and duration. If the function grows even more complex, it will become very hard to keep track of what each parameter means, and it could make the code difficult to maintain.\n\nSolutions\nTo solve this problem, we can do one or both of the following:\n\nSimplify the Function: Break the function into smaller parts that do one thing each.\nUse Objects to Group Related Data: Instead of passing many individual pieces of information, we can group them together into one object or structure that holds related information.\n\n\n1. Using a Dataclass\nfrom dataclasses import dataclass\n\n# Create a dataclass to group the machine operation parameters\n@dataclass\nclass MachineOperationData:\n    machine_id: str\n    temperature: float\n    pressure: float\n    speed: float\n    duration: float\nNow, instead of passing five separate parameters to our function, we‚Äôll just pass one object that holds everything. Next, we change our process_machine_operation function to accept this new object, making it simpler and cleaner.\ndef process_machine_operation(operation_data):\n    # Perform machine operation\n    print(\"Machine ID:\", operation_data.machine_id)\n    print(\"Temperature:\", operation_data.temperature)\n    print(\"Pressure:\", operation_data.pressure)\n    print(\"Speed:\", operation_data.speed)\n    print(\"Operation Duration:\", operation_data.duration)\n\n# Usage \noperation_data = MachineOperationData(\n    machine_id=\"M001\", \n    temperature=100.5, \n    pressure=200.0, \n    speed=1500.0, \n    duration=5.0)\nprocess_machine_operation(operation_data)\n\n\n\n\n\n\nTip\n\n\n\nYou can combine dataclasses with data validation through Pydantic.\n\n\n\n\n2. Divide and conquer\nAlthough using a single dataclass is a good start, we don‚Äôt want our data structure to become too big and complicated. If the dataclass starts holding too much data, it can make the code harder to understand. Instead, we can break it into smaller, simpler data classes that work together. For example:\n@dataclass\nclass Machine:\n    machine_id: str\n    manufacturer: str\n\n@dataclass\nclass OperationParameters:\n    temperature: float\n    pressure: float\n    speed: float\n    duration: float\n\n@dataclass\nclass EnvironmentalConditions:\n    humidity: float\n    altitude: float\n\n@dataclass\nclass MachineOperationData:\n    machine: Machine\n    operation_parameters: OperationParameters\n    environmental_conditions: EnvironmentalConditions\n\ndef process_machine_operation(operation_data):\n    print(\"Machine ID:\", operation_data.machine.machine_id)\n    \n    # Implement machine operation\nHere, instead of having one large MachineOperationData dataclass, we‚Äôve divided it into smaller pieces. Each class now represents a specific part of the data, which can then be used individually as smaller classes or grouped together as needed. This approach keeps everything organized and easy to work with.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many inputs"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_smells/many_arguments.html#key-takeaways",
    "href": "docs/software/code_quality/code_smells/many_arguments.html#key-takeaways",
    "title": "Many arguments",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nDon‚Äôt pass too many parameters. If a function requires many parameters, it‚Äôs a sign that the function might be doing too much. Group related data together to reduce the number of parameters or break the function into smaller parts.\nUse classes or dataclasses to help organize related data into neat packages that are easy to pass around in your code.\nKeep things simple: Don‚Äôt let your classes become too big. If necessary, break them down into smaller parts, but keep them organized and easy to understand.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRealPython - Data Classes",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code smells",
      "Many inputs"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html",
    "href": "docs/software/code_quality/code_style.html",
    "title": "Code style and tools",
    "section": "",
    "text": "‚ÄúPrograms must be written for people to read, and only incidentally for machines to execute.‚Äù\nHarold Abelson",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#style-guide",
    "href": "docs/software/code_quality/code_style.html#style-guide",
    "title": "Code style and tools",
    "section": "Style guide",
    "text": "Style guide\nStyle guides are a set of rules and conventions that define how code should be written in a particular programming language. They cover aspects such as naming conventions, indentation, line length, and other formatting rules. Style guides help to ensure that code is consistent, readable, and maintainable, and they are often enforced by static analysis tools and formatters. Many programming languages have official style guides, and there are also community-driven style guides that provide additional recommendations and best practices.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#static-analysis-tools",
    "href": "docs/software/code_quality/code_style.html#static-analysis-tools",
    "title": "Code style and tools",
    "section": "Static analysis tools",
    "text": "Static analysis tools\nStatic analysis tools are used to analyze source code without executing it. They can identify potential issues in the code, such as syntax errors, bugs, and code smells. Static analysis tools can also enforce coding standards and best practices, and help to identify security vulnerabilities. They are often integrated into Continuous Integration workflows to ensure that code quality is maintained throughout the development process.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#formatters",
    "href": "docs/software/code_quality/code_style.html#formatters",
    "title": "Code style and tools",
    "section": "Formatters",
    "text": "Formatters\nFormatters are tools that automatically adjust the formatting of your code to make it consistent and readable according to predefined style guidelines. They do not identify errors in the logic of the code but instead can restructure the whitespace, line breaks, and indentation so that the code is more uniform across a project.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/code_style.html#overview-of-programming-languages",
    "href": "docs/software/code_quality/code_style.html#overview-of-programming-languages",
    "title": "Code style and tools",
    "section": "Overview of programming languages",
    "text": "Overview of programming languages\n\n\n\nLanguage\nStyle Guide\nStatic Analysis Tools\nFormatters\n\n\n\n\nPython\nPEP 8\npylint, flake8, prospector\nblack, autopep8, yapf\n\n\nR\nTidyverse Style Guide\nlintr\nstyler\n\n\nMATLAB\nMATLAB Style Guidelines 2.0\ncheckcode\nCode Analyzer\n\n\nC/C++\nGoogle C++ Style Guide\ncppcheck, clang-tidy\nclang-format, astyle\n\n\nJulia\nJulia Style Guide, Blue Style Guide\nJET.jl, Aqua.jl\nJuliaFormatter.jl\n\n\nFortran\nFortran Best Practices\nfortran-linter\nfprettify\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nThe Turing Way - Code Quality\nThe Turing Way - Code Style\nRealPython - Python Code Quality\nRealPython - PEP8",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Code style"
    ]
  },
  {
    "objectID": "docs/software/code_quality/online_services.html",
    "href": "docs/software/code_quality/online_services.html",
    "title": "Online services",
    "section": "",
    "text": "Sonar\nSonar is a cloud-based service that provides inspection of code quality to perform automatic reviews with static code analysis to detect bugs, code smells and security vulnerabilities in a project. It supports many programming languages and integrates with GitHub (and GitLab and Bitbucket) as part of the Continuous Integration workflows. Sonar is particularly useful for projects that require compliance with coding standards or need regular feedback on the quality of the code.\n Learn more: SonarQube Cloud documentation\n\n\n\n\n\n\nConsideration\n\n\n\nWhile Sonar offers valuable features for code quality analysis, be aware that for non open-source projects it is a paid service, and pricing model depends on how many lines of code you want to check.\n\n\n\n\nGitHub CodeQL\nGitHub CodeQL is a semantic code analysis engine that allows you to write queries to find security vulnerabilities in your codebase. It is particularly useful for identifying security vulnerabilities in open-source projects, and it can be integrated into your GitHub Actions workflow to automatically scan your code for vulnerabilities. CodeQL is available for a variety of programming languages, and it can help you identify and fix security issues before they become a problem.\n\n\n\n\n\n\n Learn more\n\n\n\n\nGitHub CodeQL\nIntroduction to code scanning\n\n\n\n\n\nCode coverage\nCode coverage quantifies the proportion of source code that is run by a software program‚Äôs (unit) test suite. It helps to identify which parts of the codebase have been tested, and achieving a high code coverage generally indicates a lower likelihood of hidden bugs. However, it is important to note that high code coverage does not necessarily translate to high code quality - it simply tells us how much of the codebase is being tested.\n\n\n\n\n\n\n Learn more\n\n\n\n\nSonar - Test coverage\nCodecov - Test coverage\n\n\n\n\n\nDependabot\nDependabot is a GitHub app that helps you keep your dependencies up to date. It checks for outdated dependencies in your project and automatically creates pull requests to update them. This can help you stay on top of security vulnerabilities and ensure that your project is using the latest features and bug fixes.\n Enabling Dependabot for your repository\n\n\nOpenSSF\nThe Open Source Security Foundation (OpenSSF) Best Practices badge provides a way for Free/Libre and Open Source Software (FLOSS) projects to demonstrate their adherence to best practices. Projects can choose to self-certify for free. Inspired by the numerous badges available on GitHub, the OpenSSF Best Practices Badge allows to quickly identify which FLOSS projects are committed to best practices and are therefore more likely to deliver high-quality and secure software.\nThe criteria for earning the passing badge and additional details about the OpenSSF Best Practices Badging program can be found on GitHub.\n\n\n\n\n\n\n Learn more\n\n\n\n\nOpenSSF - Best Practices\nGitHub - Best Practices Badge",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Code quality",
      "Online services"
    ]
  },
  {
    "objectID": "docs/software/containers/docker_gui.html",
    "href": "docs/software/containers/docker_gui.html",
    "title": "Using a docker container with a GUI",
    "section": "",
    "text": "Docker is an open platform for developing, shipping, and running applications. Docker provides the ability to package and run an application in a loosely isolated environment called a container. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host system. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.\nDockerfile ‚Äì is a text document that contains all the commands you would normally execute manually in order to build a Docker image. The instructions include a choice of operating system and all the libraries we need to install into it. Docker can build images automatically by reading the instructions from a Dockerfile.\nDocker Images ‚Äì are the basis of containers. A Docker image is an immutable (unchangeable) file that contains the source code, libraries, dependencies, tools, and other files needed for an application to run.\nDocker Container ‚Äì A container is, ultimately, just a running image."
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#docker-installation",
    "href": "docs/software/containers/docker_gui.html#docker-installation",
    "title": "Using a docker container with a GUI",
    "section": "Docker installation",
    "text": "Docker installation\nDocker can be installed on Windows, macOS, and Linux. Please visit the Docker website for downloading and installation instructions. Note, you will need admin access to your system.\nPlease check the Issues/troubleshooting session at the end of this page if you encounter some problems during installation. If your problem is not listed you can add it as an issue in the main repository.\n\nVerify Docker installation\nRun the following commands in the terminal (see below) to verify your installation:\n\ndocker --version\nWill output the version number\ndocker run hello-world\nWill output a welcome message. If you haven‚Äôt run this command before, you will receive the message Unable to find image: ‚Äòhello-world:latest‚Äô locally. Docker will then proceed by downloading and running the latest version from DockerHub."
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#terminal-access",
    "href": "docs/software/containers/docker_gui.html#terminal-access",
    "title": "Using a docker container with a GUI",
    "section": "Terminal access",
    "text": "Terminal access\nLinux\nThe default Unix Shell for Linux operating systems is usually Bash. On most versions of Linux, it is accessible by running the (Gnome) Terminal or (KDE) Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than bash, you can run it by opening a terminal and typing bash.\nmacOS\nFor a Mac computer, the default Unix Shell is Bash, and it is available via the Terminal Utilities program within your Applications folder. To open Terminal, try one or both of the following:\n\nGo to your Applications. Within Applications, open the Utilities folder. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‚ÄòSpotlight‚Äô computer search function. Search for: Terminal and press Return.\n\nFor more info: How To use a terminal on Mac\nWindows\nComputers with Windows operating systems do not automatically have a Unix Shell program installed. We encourage you to use an emulator included in Git for Windows, which gives you access to both Bash shell commands and Git. To install, please follow these instructions."
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#x-windows-system",
    "href": "docs/software/containers/docker_gui.html#x-windows-system",
    "title": "Using a docker container with a GUI",
    "section": "X Windows System",
    "text": "X Windows System\nDocker doesn‚Äôt have any build-in graphics, which means it cannot run desktop applications by default. For this, we require the X Windows System. The X Window System (X11, or simply X) is a windowing system for bitmap displays, common on Unix-like operating systems. X provides the basic framework for a GUI environment: drawing and moving windows on the display device and interacting with a mouse and a keyboard.\nIf you are on a desktop Linux, you already have one. For macOS, you can download XQuartz, and for Windows, we tested VcXsrv.\nDesktop applications will run in Docker and will try to communicate with the X server you‚Äôre running on your PC. They don‚Äôt need to know anything but the location of the X server and an optional display that they target. This is denoted by an environmental variable named DISPLAY, with the following syntax: DISPLAY=xserver-host:0. The number you see after the : is the display number; for the intents and purpose of this article, we will consider this to be equivalent to 0 is the primary display attached to the X server.\nIn order to set up the environment variable, we need to add the following code to the docker run command in the terminal:\n\nWindowsmacOSLinux\n\n\n-e DISPLAY=host.docker.internal:0\n\n\n-e DISPLAY=docker.for.mac.host.internal:0\n\n\n--net=host -e DISPLAY=:0\n\n\n\nWith these commands (and an active X server on the host system), any graphical output inside the container will be shown on your own desktop."
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#mount-a-volume",
    "href": "docs/software/containers/docker_gui.html#mount-a-volume",
    "title": "Using a docker container with a GUI",
    "section": "Mount a volume",
    "text": "Mount a volume\nThe docker image with which you can spawn a container contains all the software and general datafiles. However, we still need to give the container access to your dataset. To do so, we can mount a directory on your own system inside the container with the following command structure: -v &lt;abs_path_host&gt;:&lt;abs_path_container&gt;. Assuming your terminal is opened inside the data folder on your system, the specific commands for the different operating systems mount this folder as the /data folder inside the container, are:\nFor Windows in GitBash: -v /$(pwd):/data\nFor Windows in cmd: -v %cd%:/data\nFor Linux and macOS: -v $(pwd):/data\n$(pwd) can be replaced with the absolute path of the datafolder, or be used to access subdirectories (e.g.¬†$(pwd)/data:/data).\nFor more info about mounting volumes, check this StackOverflow question"
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#running-a-container-with-data-and-graphical-output",
    "href": "docs/software/containers/docker_gui.html#running-a-container-with-data-and-graphical-output",
    "title": "Using a docker container with a GUI",
    "section": "Running a container with data and graphical output",
    "text": "Running a container with data and graphical output\nTo start a container from an image, we use the command docker run &lt;image_name&gt;. We also pass the additional flags --rm to delete the container after closing and -it to be able to interact with the container. Combining all arguments then leads to the following commands to run (and automatically close) the container:\n\nWindowsmacOSLinux\n\n\ndocker run --rm -it -e DISPLAY=host.docker.internal:0 -v /$(pwd):/data &lt;image_name&gt;:&lt;image_version&gt;\n\n\ndocker run --rm -it -e DISPLAY=docker.for.mac.host.internal:0 -v $(pwd):/data &lt;image_name&gt;:&lt;image_version&gt;\n\n\ndocker run --rm -it --net=host -e DISPLAY=:0 -v $(pwd):/data &lt;image_name&gt;:&lt;image_version&gt;"
  },
  {
    "objectID": "docs/software/containers/docker_gui.html#issuestroubleshooting",
    "href": "docs/software/containers/docker_gui.html#issuestroubleshooting",
    "title": "Using a docker container with a GUI",
    "section": "Issues/Troubleshooting",
    "text": "Issues/Troubleshooting\n\nFor Linux users encountering the error Unable to init server, please run xhost + in the terminal and rerun the docker run command. For more info, see here.\nWSL 2 installation incomplete for Windows users\n\nEnable the virtualization in the BIOS\nFollow ALL the steps described in: https://docs.microsoft.com/en-us/windows/wsl/install-manual\n\nFailing to port a display in the docker container for Mac users.\n\nSolution: Change the docker run command by this one , docker run --rm -it -e DISPLAY=IPADDRESS:0 -v $(pwd):/data &lt;image_name&gt;:&lt;image_version&gt;\nThe IPADDRESS is gotten from typing ifconfig in the terminal.\n\nFailing to run the pipeline once the GUI is open\n\nCheck that all documents are closed before run it , namely the Getting started and the adapter files documents.\n\nFailing to mount an external hard drive in Windows when running a docker container\n\nError:\n\n      libGL error: No matching fbConfigs or visuals found\n      libGL error: failed to load driver: swrast\n\nSolution (noy yet found):\n\nLook into this links:\n\nhttps://stackoverflow.com/questions/46586013/glxgears-not-working-inside-of-docker"
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies.html",
    "title": "Environment and dependency management",
    "section": "",
    "text": "Properly managing dependencies and your environment is a critical aspect of any software project. This ensures that your project can be reliably reproduced, simplifies setup for collaborators, and reduces conflicts between third-party libraries.\n\n\n\n Python\nEnvironment and dependency management in Python.\n\nLearn more ¬ª\n\n\n\n MATLAB\nEnvironment and dependency management in MATLAB.\n\nLearn more ¬ª\n\n\n\n R\nEnvironment and dependency management in R.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/envs_dependencies/python_envs_dependencies.html",
    "href": "docs/software/development_workflow/envs_dependencies/python_envs_dependencies.html",
    "title": "Environment and dependency management in Python",
    "section": "",
    "text": "When working with Python, managing dependencies and environments is important to ensure your project can be reproduced and shared.\n\n\n\n\n\n\n Definitions:\n\n\n\nA dependency is any external library your project needs, and a virtual environment is an isolated workspace where dependencies are installed.\n\n\nThere are several ways to manage dependencies and environments:\n\nConda environments\nVirtual environments (venv/virtualenv)\nDependency management tools (e.g.¬†poetry, pipenv)\n\n\nConda Environments\nConda is a package and environment manager popular in the research and data science community. It allows you to manage both Python and non-Python dependencies.\n\nBasic commands\n# Create a new environment, e.g. with python 3.12\nconda create -n your_env_name python=3.12\n\n# List all environments\nconda env list\n\n# Activate an environment\nconda activate your_env_name\n\n# Install packages in an environment\nconda install package_name\n\n# Remove a package\nconda remove package_name\n\n# Export an environment to a file\nconda env export &gt; environment.yml\n\n# Deactivate an environment\nconda deactivate\n\n# Remove an environment\nconda env remove -n your_env_name\n\n\nConda environment files\nConda environment files (environment.yml) are used to specify the dependencies of a project. They can be used to create an environment from scratch, or to update an existing environment.\n# Export an environment to a file\nconda env export &gt; environment.yml\n\n# Create an environment from a file\nconda env create -f environment.yml\n\n# Update an environment from a file\nconda env update -f environment.yml\n\n\n\nVirtual Environments (venv/virtualenv)\nPython provides venv as a buil-in tool for creating virtual environments. virtualenv is a third-party tool that provides similar functionality.\n\nBasic commands\n# Creating a virtual environment\n# Using venv (Python 3.3+ built-in)\npython -m venv your-env-name\n\n# Using virtualenv (must be installed first)\npip install virtualenv\nvirtualenv your-env-name\n\n#Activating the environment\n# Linux/macOS\nsource your-env-name/bin/activate\n# Windows\nyour-env-name\\Scripts\\activate\n\n# Installing a library (package)\npip install lib_name\n\n# Uninstalling a library (package)\npip uninstall lib_name\n\n# To deactivate\ndeactivate\n\n\nManaging dependencies with pip\nA requirements.txt file lists all dependencies with their specific versions.\n# Export requirements.txt from an activated environment\npip freeze &gt; requirements.txt\n\n# Install dependencies from requirements.txt\npip install -r requirements.txt\n\n\n\n\n\n\n Tip\n\n\n\nUse pip-chill or pipreqs instead of pip freeze to exclude unnecessary dependencies. pip-chill lists only packages you installed, while pipreqs lists packages your code actually uses.\n\n\n\n\n\nDependency Management Tools\nConsider using tools that offer more sophisticated dependency management by integrating virtual environment creation and dependency resolution. They maintain a project manifest (e.g., pyproject.toml for Poetry) that specifies primary dependencies and generate lock files to pin exact versions for reproducibility.\n\nPipenv: Combines pip and virtualenv into a single tool, with a focus on simplicity and ease of use.\nPoetry: Manages dependencies, environments, and package building in a streamlined way.\nPixi: A new tool that aims to provide a more user-friendly experience for managing Python environments and dependencies.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCodeRefinery - Recording dependencies\nThe Turing Way - Package Management Systems\nConda documentation\nvirtualenv documentation\nvirtualenvwrapper extension",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Environments and dependencies",
      "Python"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/index.html",
    "href": "docs/software/development_workflow/index.html",
    "title": "Development workflow",
    "section": "",
    "text": "A well-organized workspace and clear software development process make research easier and more effective. Organizing your project repository clearly and following steps for working alone or with others can help improve your research and make it easier to expand. This section provides guidance on how to manage your project, structure your project, reuse projects, manage environments and dependencies, choose a branching strategy, and collaborate with others.\n\n\n\n Project Management\nManaging your projects through version control platforms.\n\nLearn more ¬ª\n\n\n\n Project Structue\nStructuring your project.\n\nLearn more ¬ª\n\n\n\n Project Templates and Reusability\nReusing projects and repositories.\n\nLearn more ¬ª\n\n\n\n Environments and Dependencies\nManaging your environments and dependencies.\n\nLearn more ¬ª\n\n\n\n Workflow Management\nTools for writing and managing workflows.\n\nLearn more ¬ª\n\n\n\n Branch Management\nChoosing a branching strategy.\n\nLearn more ¬ª\n\n\n\n Collaboration\nCollaborative workflow.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_structure.html",
    "href": "docs/software/development_workflow/project_structure.html",
    "title": "Project structure",
    "section": "",
    "text": "In software development, the choices you make at the start will affect your project‚Äôs final outcome. One key decision is how to structure your project, as a well-organised setup is essential for reproducibility and long-term maintainability.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project structure"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/project_structure.html#repository-structures",
    "href": "docs/software/development_workflow/project_structure.html#repository-structures",
    "title": "Project structure",
    "section": "Repository structures",
    "text": "Repository structures\nThe following are recommendations of how you can structure your project repository for Python, MATLAB, and R projects.\n\nPythonMATLABR\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                     # Documentation directory\n‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks\n‚îú‚îÄ‚îÄ src/                      # Contains your main code\n‚îÇ   ‚îî‚îÄ‚îÄ your_project/            # A folder where your organized code lives\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # A marker file that indicates this folder is for Python code\n‚îÇ       ‚îú‚îÄ‚îÄ module            # A file or folder with specific functions or classes\n‚îÇ       ‚îî‚îÄ‚îÄ extras/           # A folder for additional, related code\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py   # A marker file for the additional code folder\n‚îú‚îÄ‚îÄ tests/                    # Your test directory  \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # Data files used in the project (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # Files from your analysis (if applicable)\n‚îú‚îÄ‚îÄ results/                  # Results (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # Untracked files \n‚îú‚îÄ‚îÄ requirements.txt          # Software dependencies (environment.yml if using Conda)\n‚îÇ                             # ‚Üë Even better to use a build system config (pyproject.toml)\n‚îÇ                             # ‚Üë which is becoming the new standard\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # License information\n Choosing between a src/ layout and a flat layout for Python\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                   # Documentation and user guides\n‚îú‚îÄ‚îÄ src/                    # Main MATLAB code\n‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Helper functions and scripts\n‚îÇ   ‚îú‚îÄ‚îÄ models/             # Core functions or classes implementing models/algorithms\n‚îÇ   ‚îî‚îÄ‚îÄ main_script.m       # Main script/-s or entry point for the project\n‚îÇ\n‚îú‚îÄ‚îÄ scripts/                # Scripts folder (e.g. for analysis and demo scripts)\n‚îú‚îÄ‚îÄ tests/                  # Tests folder (e.g. MATLAB unit tests)\n‚îú‚îÄ‚îÄ data/                   # Raw data files\n‚îú‚îÄ‚îÄ results/                # Output files (figures, processed data, etc.)\n‚îú‚îÄ‚îÄ examples/               # Example usage or tutorials\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore              # Specifies files/folders to ignore in version control\n‚îú‚îÄ‚îÄ README.md               # Project overview and instructions\n‚îî‚îÄ‚îÄ LICENSE                 # License information\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ R/                        # R scripts and functions (can also be called src/)\n‚îÇ   ‚îú‚îÄ‚îÄ function.R            # R functions used across analyses\n‚îÇ   ‚îî‚îÄ‚îÄ other_function.R      \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # raw data files (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # processed data files (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ doc/                      # project documentation\n‚îú‚îÄ‚îÄ man/                      # helper files for package functions generated from roxygen2 (if applicable)\n‚îÇ      \n‚îú‚îÄ‚îÄ vignettes/                # explanatory vignettes for the project (if applicable)\n‚îÇ   ‚îî‚îÄ‚îÄ function_vignette.Rmd # vignettes for each function\n‚îÇ\n‚îú‚îÄ‚îÄ tests/                    # test cases for your functions (highly recommended)\n‚îÇ   ‚îî‚îÄ‚îÄ testthat/             # using the testthat package\n‚îÇ\n‚îú‚îÄ‚îÄ results/                  # output from data analyses etc. (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ scripts/                  # high-level scripts for running analyses\n‚îÇ   ‚îî‚îÄ‚îÄ analysis_script.R     # script running the main analysis\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # gitignore\n‚îú‚îÄ‚îÄ DESCRIPTION               # package description file (if applicable)\n‚îú‚îÄ‚îÄ NAMESPACE                 # namespace file for package (if applicable)\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # license information\n\n\n\nThese structures are a starting point and can be adapted based on the specific needs and practices of your project. Some additional tips:\n\nParticular metadata files are often capitalized, such as README, LICENSE, CONTRIBUTING, CODE_OF_CONDUCT, CHANGELOG, CITATION.cff, NOTICE, and MANIFEST.\nGenerally, all content that is generated upon building or running your code should be added to .gitignore. This likely includes the content of processed_data and results folder.\nGit cannot track empty folders. If you want to add empty folders to enforce a folder structure, e.g., processed_data orresults, the convention is to add the file .gitkeep to the folder.\n\n\n\n\n\n\n\n Managing data\n\n\n\nIf your raw data files or any data assets are large (typically more than a few megabytes), it‚Äôs usually best not to include them directly in the repository. Instead:\n\nKeep such files externally (e.g.¬†cloud storage, Git LFS), and add only a reference or a small sample to the repository.\nAdding placeholder files or instructions in the README for how to obtain the complete datasets.\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCode Refinery - Organizing your projects\nArjanCodes guide to structuring Python projects\nA collection of .gitignore templates\nGit LFS",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Project structure"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html",
    "href": "docs/software/development_workflow/workflow_management.html",
    "title": "Workflow management",
    "section": "",
    "text": "Workflow management for research software is the practice of organizing series of tasks related to research, such as how scientific code, data and experiments are developed, executed and shared to ensure reproducible, collaborative and efficient research. An example research workflow might include data processing, model training or running simulations, as well as recording and publishing the results. The main objective is not only about making the code run successfully, but about making it reproducible, trackable and adaptable.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#workflow-management-tools",
    "href": "docs/software/development_workflow/workflow_management.html#workflow-management-tools",
    "title": "Workflow management",
    "section": "Workflow management tools",
    "text": "Workflow management tools\nA workflow management tool (or simply a workflow manager) is a tool to help researchers build and manage computational workflows in a structured and reproducible way.\n\nThese tools assist in defining, organizing, executing and tracking sequences of computational steps which are typically represented as a series of commands or scripts.\nBy explicitly recording how data is processed, including the parameters and software environments used, workflow managers make it easier to replicate analyses, debug issues, and share work with collaborators.\nThis level of structure helps avoid the all-too-common situation where a result is produced but the exact steps to generate it are lost or unclear.\n\nThe bioinformatics field is a prominent example where workflow management has become essential since research relies heavily on pipelines to analyze genomic data; usually you perform a sequence of analyses on several files. Hence, there are many workflow managers niche to the field.\n\nBut workflow management is a general need in many other applications involving data to support reproducible analysis.\nAny research field involving data analysis, simulations, or modeling can benefit from adopting workflow managers.\nFields like climate science, neuroscience, physics, and even digital humanities face similar challenges of organizing code, managing data dependencies, and ensuring that computational results are reproducible.\nAs scientific computing becomes more complex and collaborative, workflow management is emerging as a foundational practice to support open science, reproducibility, and long-term maintainability of research software.\n\n\nSome examples of workflow managers\nWorkflow managers come in many forms, from simple scripting approaches to full-featured platforms with graphical user interfaces (GUIs) or domain-specific languages (DSLs). Here are some examples, categorized by interface and complexity:\n\nScript-based or Classic Tools \n\nShell scripts / Python scripts (‚Äúgood old scripts‚Äù)\nMake ‚Äì a traditional build automation tool still used in research for its simplicity\n\nCode-Based Workflow Managers  These systems are primarily driven by writing code in general-purpose languages (like Python), often using functions, decorators, or object-oriented patterns:\n\nSnakemake (Python-based)\nNextflow (DSL based on Groovy)\nAirflow (Python-based)\nRuffus, SciPipe, and many others\n\nSnakemake, Nextflow, and Airflow are examples of DSL-based workflow managers that allow users to define workflows in a concise and human-readable syntax.\nGUI-Based Workflow Managers  These tools provide a visual interface for building and executing workflows, making them ideal for users with little or no programming background:\n\nGalaxy\nKNIME\n\n\n\n\n\n\n\n\nChoose the right workflow manager for you\n\n\n\n\nDSLs can make workflows easier to read, document, and debug - especially for complex pipelines.\nGUI-based workflow systems, with drag-and-drop interfaces and visual feedback, are often the most accessible option for beginners or interdisciplinary teams.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#snakemake-a-python-esque-make",
    "href": "docs/software/development_workflow/workflow_management.html#snakemake-a-python-esque-make",
    "title": "Workflow management",
    "section": "Snakemake: a Python-esque make",
    "text": "Snakemake: a Python-esque make\nSnakemake essentially builds on the implicit wildcard rule approach of Make, and it extends its capabilities by allowing the use of Python in a pipeline. Just like Make, its goal is to produce a set of requested output files based on predefined rules and steps.\nAlthough it was originally developed to create scalable bioinformatics and genomics pipelines, it can be generalized to other applications as well. It has become a standard tool in reproducible research; being cited more than 12 times per week in 2023, and has been used extensively in scientific publications in several different fields. Currently, it has over one million downloads on Conda.\n\nNoteworthy features of Snakemake\n\nIf Python and Make were to have a baby.\nYou can describe workflows using a human readable, Python based language.\nIt has built-in caching: if some steps of your workflow have already been run, Snakemake can recognize that and avoid rerunning the same analyses.\nIt can accommodate both serial and parallel jobs since each ‚Äúwork units‚Äù in a workflow can be run independently of one another.\nIt makes debugging easier since it keeps track of all files generated, you can identify which steps in your workflow have failed.\nIntegration with conda allows you to define conda environments for both the whole workflow and individual steps.\nYou can incorporate tools or methods written in different scripting languages.\nWorkflows can be scaled to server, cluster, grid and cloud environments, without modifying the workflow itself.\nIt has an active user and developer base in mainly bioinformatics, and other scientific research community. It‚Äôs development is driven by the needs of scientists and their needs of reproducible research.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/workflow_management.html#what-about-airflow",
    "href": "docs/software/development_workflow/workflow_management.html#what-about-airflow",
    "title": "Workflow management",
    "section": "What about Airflow?",
    "text": "What about Airflow?\nAirflow has become the go-to tool for building data pipelines, it started at Airbnb and was later adopted by the Apache project. Being backed by the Apache project, and a growing community of contributors, it‚Äôs the most popular workflow manager in software engineering.\n\n Pros:\n\nIt is the most popular choice of workflow management system in software engineering.\nIt has a bigger community support and hence more detailed tutorials and documentation.\nIt offers more features, especially enterprise integrations used in industry.\nIt integrates well with databases, APIs, cloud services and data warehouse like Google BigQuery, AWS S3 and Snowflake.\nIt is more production friendly with built-in scheduling and monitoring features.\n\n Cons:\n\nIt has a complex infrastructure and a steeper learning curve.\nIt is more difficult to share pipelines between different environments.\nIt lacks native support for conda.\nIt has a more convoluted approach to parallel computation.\nIt has limited support for HPC.\n\n\n\nA summary of workflow management tools discussed\n\n\n\n\n\n\n\n\n\nFeature\nSnakemake\nNextflow\nAirflow\n\n\n\n\nTarget Audience\nScientists, researchers\nBioinformaticians, scientists\nData engineers, DevOps\n\n\nEase of Use\nHigh\nMedium\nLow\n\n\nReproducibility Focus\nStrong\nMedium\nLow\n\n\nHPC Support\nExcellent\nGood\nMinimal\n\n\nCloud-Native Support\nModerate\nStrong\nExcellent\n\n\nCommunity\nScientific community\nBioinformatics\nData engineering, cloud-native\n\n\n\n\n\n\n\n\n\nSnakemake vs Airflow\n\n\n\nSnakemake is a gentle introduction to workflow management for most researchers. Airflow produces well defined production level workflows that are meant to be run continually, and hence Snakemake is much better suited for the needs of researchers who want to run reproducible analyses for a particular project or an application.\n\n\n\n\n\n\n\n\nSidenote: What is special about bioinformatics workflows?\n\n\n\n\n\nIn bioinformatics, a workflow is a collection of steps run in series to transform raw data input to processed results (figures, insights, decisions etc.). Each step can be made up of different tools, programs, parameters, databases and dependencies/requirements.\nThere are 3 main reasons why bioinformatics workflows are different than those data engineering workflows in the industry:\n1. Differences in data type, shape and scale\n\nBioinformatics datasets are typically very large and come from various sources (DNA sequences, RNA sequencing, proteomics data, imaging data)\nDifferent data types have different structures and different preprocessing/analysis needs\n\n2. Differences in programs and tooling\n\nBioinformatics pipelines often involve numerous steps with intricate dependencies\nDifferent steps use different, specialized tools\nOpen source, highly specialized tools that are not meant to be integrated natively, there are no software packages or standalone platforms to run analyses from start to finish\nNew algorithms, tools and reference databases are updated frequently, researchers need flexible pipelines that can be adapted easily\nMany bioinformatics tasks and tools are computationally expensive (genome assembly, alignment, sequence search) and require HPC\n\n3. Community support behind bioinformatics workflow managers and open source software\n\nField of bioinformatics has a strong emphasis on open science, reproducible and transparent research. All of which are achieved using workflow managers\n\n\n\n\n\n\nOther fields that already use or can benefit from Snakemake\n\nLarge, parallel deep learning experiments using Snakemake: The USGS demonstrates how Snakemake can be used to perform deep learning experiments on environmental data.\nWorkflow managers in high-energy physics - Enhancing analyses with Snakemake: Physics and high-energy particle research: Large scale HEP experiments with complex dependencies on cloud.\nNeuroscience: Using Snakemake to process and analyze MRI data.\nEcology and evolutionary research: Article outlining the specific benefits of using Snakemake to manage data analysis workflows in ecology and evolutionary research, and demonstrating its use through case studies.\nEarth and Climate Science: Preprocessing large datasets from climate models (NetCDF format) for regional climate projections.\nAutomating preprocessing EEG/MEG data, MRI and fMRI data analysis and predictive modeling for thousands of neuroimaging files.\nChemistry: Running large-scale molecular dynamics simulations with different parameter combinations and collecting results in a reproducible framework.\n\n\n\n\n\n\n\n Further reading\n\n\n\n\nAwesome pipeline repository: a curated list of tools for creating pipelines.\nSnakemake documentation: an extensive documentation that includes in depth tutorials and edge cases.\nAn Introduction to Snakemake with R for Economics\nSustainable data analysis with Snakemake: A general overview article about Snakemake published in F1000Research.\nA review of bioinformatic pipeline frameworks.: A review article published in Briefings in Bioinformatics, focused on Bioinformatics pipelines.\nAirflow vs Snakemake: A direct comparison between Airflow and Snakemake for data pipelines, while promoting FlowDeploy.\nWhy are bioinformatics workflows different?",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Workflow management"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/code_documentation.html",
    "href": "docs/software/documentation/code_documentation/code_documentation.html",
    "title": "Code documentation",
    "section": "",
    "text": "Good code documentation acts as the bridge between developers and users by clearly explaining the functionality and rationale behind your code. Whether you‚Äôre writing inline comments or structured documentation, the key is to make your code readable, maintainable, and accessible to both current and future contributors.\n\n\n\n Python projects\nCode comments, docstrings, API reference.\n\nLearn more ¬ª\n\n\n\n MATLAB projects\nDocumenting MATLAB projects.\n\nLearn more ¬ª\n\n\n\n R projects\nDocumenting R projects with roxygen2.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html",
    "href": "docs/software/documentation/code_documentation/python_documentation.html",
    "title": "Python code documentation",
    "section": "",
    "text": "Code readability is detailed in a coding style guide.\nCode comments are useful for clarifying complex parts of code, noting why certain decisions were made in specific blocks or lines.\nDocstrings provide a description of the function, class, or module that follows immediately after it is defined, and should contain all the relevant information needed for using them, rather than explaining how the code works. Ideally, every module should have a docstring, and so should every function and class that a module makes available.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html#code-comments",
    "href": "docs/software/documentation/code_documentation/python_documentation.html#code-comments",
    "title": "Python code documentation",
    "section": "Code comments",
    "text": "Code comments\nCode comments are inline annotations meant for developers who read or maintain the source code. They should:\n\nexplain parts that are not intuitive from the code itself\nexplain the purpose of a piece of code (why over how)\nneed to be kept up-to-date as wrong comments are not caught through testing\ndo not replace readable and structured code\ndo not turn old code into commented zombie code (see code smells)\ndo not repeat in natural language what is written in your code, e.g.\n\n# Now we check if the age of a patient is greater than 18\nif age_patient &gt; 18:",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_documentation/python_documentation.html#docstrings",
    "href": "docs/software/documentation/code_documentation/python_documentation.html#docstrings",
    "title": "Python code documentation",
    "section": "Docstrings",
    "text": "Docstrings\nDocstrings are structured comments, associated with segments (rather than lines) of code which can be used to generate documentation for users (and yourself!) of your project. They allow you to provide documentation to a segment (function, class, method) that is relevant for the user. Docstrings are placed in triple quotes \"\"\" and enable automated generation of API documentation.\nTwo docstring styles are commonly used for their readability:\n\nNumPy styleGoogle style\n\n\ndef func(arg1, arg2):\n    \"\"\"Summary line.\n\n    Extended description of function.\n\n    Parameters\n    ----------\n    arg1 : int\n        Description of arg1\n    arg2 : str\n        Description of arg2\n\n    Returns\n    -------\n    bool\n        Description of return value\n\n    \"\"\"\n    return True\n‚Æï Check out the NumPy style guide or a full example.\n\n\ndef func(arg1, arg2):\n    \"\"\"Summary line.\n\n    Extended description of function.\n\n    Args:\n        arg1 (int): Description of arg1\n        arg2 (str): Description of arg2\n\n    Returns:\n        bool: Description of return value\n\n    \"\"\"\n    return True\n‚Æï Check out the Google style guide or a full example.\n\n\n\n\nDocstring formatting\nPython‚Äôs PEP 257 provides guidelines on how to effectively write docstrings to ensure they are clear, concise, and useful. Some pointers:\n\nThe summary sentence of the docstring should appear on the same line as the opening triple quotes.\nThe closing triple quotes should be placed on a separate line, except for one-line docstrings.\nDocstrings for methods and functions should not have blank lines before or after them.\n\n\n\n\n\n\n\n Example\n\n\n\n\n\ndef find_max(numbers):\n    \"\"\"Find the maximum value in a list of numbers.\n\n    Parameters\n    ----------\n    numbers : iterable\n        A collection of numerical values from which the maximum will be determined.\n\n    Returns\n    -------\n    max_value : `float`\n        The highest number in the given list of numbers.\n    \"\"\"\n    pass\n\n\n\n\nDocstrings for classes should immediately follow the class definition without any preceding blank lines. However, a single blank line should follow the docstring, separating it from subsequent code such as class variables or the init method.\n\n\n\n\n\n\n\n Example\n\n\n\n\n\nclass Circle(object):\n    \"\"\"A circle defined by its radius.\n\n    Parameters\n    ----------\n    radius : `float`\n        The radius of the circle.\n    \"\"\"\n\n    def __init__(self, radius):\n        self.radius = radius\n\n\n\n\nThe content of a docstring must align with the indentation level of the code it documents.\n\n\n\n\n\n\n\n Correct and incorrect examples\n\n\n\n\n\n\n\n\n\ndef get_length(items):\n    \"\"\"Calculate the number of items in the list.\n\n    Parameters\n    ----------\n    items : list\n        A list whose length is to be determined.\n\n    Returns\n    -------\n    length : int\n        The number of items in the list.\n    \"\"\"\n    return len(items)\n\n\ndef get_length(items):\n    \"\"\"Calculate the number of items in the list.\n\nParameters\n----------\nitems : list\n    A list whose length is to be determined.\n\nReturns\n-------\nlength : int\n    The number of items in the list.\n\"\"\"\n    return len(items)\n\n\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nBuild API reference from docstrings\nNumpydoc style guide - best practices for docstrings\n\n\n\n\n\nDocstring contents\nFormatting conventions are important for clarity and readability across different APIs or libraries. Here we adhere to the numpydoc convention.\n\nSummaries\nDocstrings should start with a one-sentence summary and if additional clarification is needed, you could add an extended summary. For functions and methods, use imperative voice, framing its summary as a command or instruction that the user can execute through the API. For classes, the summary should clearly describe what the class represents or its primary responsibility.\n\n\nParameters and arguments\nThe Parameters section lists the input parameters of a class, function, or method. It should include the parameter name, type, and a brief description of what the parameter represents. Parameters are listed in the same order as they appear in the function definition.\n\n\n\n\n\n\n Full description and example\n\n\n\n\n\nDescribing parameters\nBasic example:\ndef calcDistance(x, y, x0=0., y0=0., **kwargs):\n    \"\"\"Calculate the distance between two points.\n\n    Parameters\n    ----------\n    x : `float`\n        X-axis coordinate.\n    y : `float`\n        Y-axis coordinate.\n    x0 : `float`, optional\n        X-axis coordinate for the second point (the origin,\n        by default).\n\n        Descriptions can have multiple paragraphs, and lists:\n\n        - First list item.\n        - Second list item.\n    y0 : `float`, optional\n        Y-axis coordinate for the second point (the origin,\n        by default).\n    **kwargs\n        Additional keyword arguments passed to\n        `calcExternalApi`.\n    \"\"\"\n\n\n\n\n\nReturns and Yields\nReturns is an explanation about the returned values and their types, following the same format as Parameters. This is applicable to functions and methods. Use Yields for generators.\n\n\n\n\n\n\n Returns and Yields examples\n\n\n\n\n\n\nDocumenting Returns\nDocumenting Yields\n\n\nBasic example for Returns:Basic example for Yields:\n\n\ndef getCoord(self):\n    \"\"\"Get the point's pixel coordinate.\n\n    Returns\n    -------\n    x : `int`\n        X-axis pixel coordinate.\n    y : `int`\n        Y-axis pixel coordinate.\n    \"\"\"\n    return self._x, self._y\n\n\ndef items(self):\n    \"\"\"Iterate over items in the container.\n\n    Yields\n    ------\n    key : `str`\n        An item's key.\n    value : obj\n        An item's value.\n    \"\"\"\n    for key, value in self._data.items():\n        yield key, value\n\n\n\n\n\n\n\n\nRaises\nFor classes, methods, and functions the Raises section is used to describe exceptions that are explicitly raised.\n\n\n\n\n\n\n Example\n\n\n\n\n\n\nDocumenting Raises\n\nRaises\n------\nIOError\n    Raised if the input file cannot be read.\nTypeError\n    Raised if parameter ``example`` is an invalid type.\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nDocumenting modules\nDocumenting classes\nDocumenting methods and functions\nDocumenting constants and class attributes\nDocumenting class properties\nComplete example module\nnumpydoc example",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code documentation",
      "Python projects"
    ]
  },
  {
    "objectID": "docs/software/documentation/code_of_conduct.html",
    "href": "docs/software/documentation/code_of_conduct.html",
    "title": "Code of conduct",
    "section": "",
    "text": "A code of conduct establishes expectations for behavior within your project‚Äôs community, creating an inclusive space that minimizes conflicts and promotes constructive collaboration.\nWhat to include:\n\nPurpose: State the reason for having a code of conduct and its importance for a positive community.\nExpected behavior: Clearly outline the standards of behavior expected from all participants.\nUnacceptable behavior: Specify actions and language that are not tolerated.\nEnforcement: Explain the consequences of violating the code of conduct and how incidents will be handled.\nContact information: Provide contact details for reporting concerns or issues.\n\nYou can add a CODE_OF_CONDUCT.md file at your repository‚Äôs root level. GitHub will automatically detect and display it in your community profile.\n\n\n\n\n\n\n Learn more\n\n\n\n\nContributor Covenant - A widely adopted template for open source communities.\nGitHub‚Äôs guide to adding a code of conduct",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Code of conduct"
    ]
  },
  {
    "objectID": "docs/software/documentation/hosting.html",
    "href": "docs/software/documentation/hosting.html",
    "title": "Hosting",
    "section": "",
    "text": "Once you have created your documentation either in Sphinx, Jupyter Book, MkDocs or Quarto, you can host it online.\n\nGitHub Pages\nGitHub Pages provides a simple way to host your documentation, especially if your project is already on GitHub.\nIt is straightforward to set up GitHub Pages:\n\nWithin your repository, go to the repository settings and find the GitHub Pages section.\nChoose your publishing source (you should have a docs folder or a dedicated branch).\n\nGitHub Pages also supports custom domains, which might be relevant to you. You can configure this by adding a CNAME file to your directory.\n\n\n\n\n\n\n Learn more\n\n\n\n\nGitHub Pages\nConfiguring a custom domain for your GitHub Pages site\nCoderefinery - Deploying Sphinx documentation to GitHub Pages\nCoderefinery - Hosting websites/homepages on GitHub Pages\n\n\n\n\n\nRead the Docs\nRead the Docs is a platform that simplifies the hosting of documentation. It integrates particularly well with Sphinx, allowing for the automatic building and hosting of your project documentation. Read the Docs supports automatic builds and version control, enabling users to switch between different versions of the documentation to match the version of the software they are using. Additionally, it offers support for custom domains.\nIt offers a free service for open-source projects, which includes features like version control and automatic builds. However, for private or commercial projects, Read the Docs requires a paid subscription.\nThe setup is also straightforward:\n\nSign up and import your documentation repository.\nConnect to your GitHub account.\nConfigure your project settings within their dashboard.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRead the Docs: documentation simplified\nRead the Docs tutorial",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Hosting"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html",
    "href": "docs/software/documentation/license.html",
    "title": "Software licenses",
    "section": "",
    "text": "Important\n\n\n\nFor questions about data and software licenses, please consult your Faculty Data Steward.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html#tu-delft-licensing-policy",
    "href": "docs/software/documentation/license.html#tu-delft-licensing-policy",
    "title": "Software licenses",
    "section": "TU Delft licensing policy",
    "text": "TU Delft licensing policy\nTU Delft, by default, holds the rights to the software created by its employees. In order to apply a pre-approved open source license, you have to follow the guidelines in TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. It states:\n\n\n\n\n\n\nIt is important to remember that in principle TU Delft holds the rights to the software created by its employees (i.e.¬†software developers, researchers and/or staff). So, some formal (legal) steps are needed to arrange matters properly.\nWhen these guidelines are followed and when the software is published, TU Delft disclaims its copyright, allowing software developers, researchers and staff to hold the copyright to their software and thereby having the right to apply one of the pre-approved licences when sharing software. The pre-approved open source software licences at TU Delft are Apache, MIT, BSD, EUPL, AGPL, LGPL, GPL, and CC0.\n\n\n\n\nSteps to apply a pre-approved open-source license\n\nDetermine if it is possible to apply an Open Source Software licence to your project (see diagram below).\nThe waiver of TU Delft should be added to the repository, either as a separate file or within the LICENSE file. The waiver text should be as follows:\nTechnische Universiteit Delft hereby disclaims all copyright interest in the program ‚ÄúName program‚Äù (one line description of the content or function) written by the Author(s). \n\n[Name Dean], Dean of [Name Faculty]\nAssert your own, personal copyright (¬© YEAR, [NAME], [REFERENCE project, grant or study if desired]. Waiving the copyright and having TU Delft staff assert the copyright in their own name facilitates the use of copyleft licences.\nApply one of the TU Delft pre-approved Open Source Software licences in the format and form described in the licence text after stating, ‚ÄúThis work is licensed under a [NAME and VERSION] OSS licence‚Äù.\nMake the software openly available (for instance in an online repository such as GitHub).\nPlease consider acknowledging support from TU Delft and/or your funding provider.\nRegister the software either in 4TU.ResearchData or in PURE. Registries in 4TU.ResearchData are automatically registered in PURE.\n\n\n\n\n\n\n\nThe steps above are mandatory, the TU Delft guidelines state:\nPlease note that if the software is not published, and/or if the guidelines have not been followed correctly and/or if the software is not registered in PURE, then this ‚Äòagreement‚Äô is invalid and the software automatically falls under the legal copyright of TU Delft. This instantly nullifies the right of the software developer or researcher to apply for a licence and thus the open source software licence applied never came into existence. This works retroactively.\n\n\n\n\n\nDecision tree for applying for a license\nTU Delft staff members can apply for an open-source license according to the decision tree found in TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation.\n\n\n\n\n\n\n\nDecision tree to guide software developers, researchers and staff on when they can apply an open source licence to their software (OSS: Open Source Software, IDF: Invention Disclosure Form). From: Bazuine, M. (2021). TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. Zenodo. https://doi.org/10.5281/zenodo.4629635",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/license.html#types-of-open-source-licenses",
    "href": "docs/software/documentation/license.html#types-of-open-source-licenses",
    "title": "Software licenses",
    "section": "Types of open source licenses",
    "text": "Types of open source licenses\n\nPermissive licenses aka copyright (e.g., MIT, BSD, Apache): These licenses allow users to do almost anything with the code, including using it in proprietary software.\nRestrictive licenses aka copyleft (e.g., GPL, AGPL, LGPL, EUPL): These licenses require any derivative works to be open source and distributed under the same license.\n\n\nCopyright licenses\n\nMIT: Very simple and permissive, allowing almost unrestricted reuse. The software can be freely used, modified, distributed, and sublicensed.\nBSD: Similar to the MIT license, but it may include additional attribution requirements.\n\n\n\n\n\n\n\n BSD license details\n\n\n\n\n\n\nAttribution: Requires that the copyright notice and list of conditions be included in all copies or substantial portions of the software (except for the BSD 0-Clause License, which does not require any attribution). There are different clause variants of the license. For example, a BSD 3-Clause license adds a clause preventing the use of the names of the project or its contributors to endorse or promote derived products without written permission.\nPatent Protection: Does not include explicit provisions for patent protection.\n\n\n\n\n\nApache: Allows for the use, modification, distribution, and sublicensing of the software under certain conditions. It is often used in open-source projects and is often the choice for its balance between permissiveness and the protection of patents. The Innovation and Impact Center recommends this license for industry collaborations.\n\n\n\n\n\n\n\n Apache license details\n\n\n\n\n\n\nAttribution: Requires preservation of the original copyright notice and a notice of any modifications made.\nPatent Protection: Includes a patent retaliation clause, which provides an additional layer of protection. This clause terminates the license if the user initiates patent litigation against any entity regarding the licensed software.\nNotice Requirement: Modifications to the original code must be documented, and a NOTICE file must be included with any substantial portions of the software.\n\n\n\n\n\n\n\n\n\n\n Key differences between a BSD and Apache license\n\n\n\n\nThe Apache license includes a patent retaliation clause to protect against patent litigation, but the BSD license does not explicitly address patent rights.\nThe BSD license does not require a specific notice file for modifications, but the Apache License requires a NOTICE file that documents any modifications made to the original code.\n\n\n\n\n\nCopyleft licenses\n\nGPL (GNU General Public License): One of the most widespread copyleft licenses. With the GPL license, any derivative work under this license automatically becomes subject to the same GPL terms, regardless of the size of the contribution. All future modifications and adaptions of code under this license is only compatible with this license and cannot be used in proprietary software.\nDerivatives from GPL (AGPL, LGPL, EUPL): From these, the EUPL license is somewhat more flexible compared to others as it can coexist with other open-source software licenses such as MIT, BSD, and Apache. For instance, if you integrate a portion of software that is licensed under Apache into a project governed by EUPL, that portion can retain its Apache license. In contrast, under the GPL, the entire codebase would need to be licensed under GPL.\n\n\n\n\n\n\n\n There are even instances when GPL licenses are incompatible with each other. For example, GPL-2.0 is incompatible with GPL-3.0. If a project uses GPL-2.0 you are essentially forced to use that license.\n\n\n\n\n\nLicense compatibility\n\n\n\n\n\n\n\nCompatibility of licenses. From: Bazuine, M. (2021). TU Delft Guidelines on Research Software: Licensing, Registration and Commercialisation. Zenodo. https://doi.org/10.5281/zenodo.4629635\n\n\n\n\n\n\n\n\n Tip\n\n\n\nDon‚Äôt forget to check whether your software‚Äôs dependencies have restrictions on re-use.\n\n\nIt is advisable to contact your faculty‚Äôs data steward regarding licensing questions. If your project involves complex legal considerations, particularly regarding intellectual property rights or compliance with licensing agreements, the Innovation and Impact Center should also be involved.\n\n\n\n\n\n\n Further reading\n\n\n\n\nTU Delft Research Software Policy\nTU Delft Guidelines on Research Software - the TU Delft Research Software Policy approved by the Executive Board\nChoose a license - a simplified guide on choosing an open-source license\ntl;drLegal - Plain English summaries of software licenses\nThe Turing Way - Licensing\nGitHub - Adding a license to a repository",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "Software licenses"
    ]
  },
  {
    "objectID": "docs/software/documentation/write_a_readme.html",
    "href": "docs/software/documentation/write_a_readme.html",
    "title": "README",
    "section": "",
    "text": "A README file is essential for your software project as it helps users understand the purpose of your project, how to install and use it, and how to contribute. While the specific content of a README can vary from project to project, a good README should always include the following sections:\n\nThe purpose of the project.\nHow to cite the project.\nInstallation and usage instructions.\nThe terms under which the software is distributed (license).\n\n\nProject purpose\nClearly explain the purpose of your project, including its motivation and objectives. This section should serve as an introductory overview, helping users and contributors understand the essence of your project.\nConsider adding:\n\nBackground information\nComparison with alternatives, highlighting what sets your project apart\nLinks to related references or documentation\n\n\n\nHow to cite\nIf you want users to cite your project when they use it, provide a citation in this section, referring to a publication or DOI of the software.\nFor more information about citing the project, see the citation guide.\n\n\nInstallation and usage\nThis section should explain the steps needed to set up and use the software. Before installation, users must be aware of any prerequisites or dependencies that are required. This can include specific versions of programming languages, libraries, operating systems, data, and hardware.\n\nInstallation steps\nProvide a step-by-step guide for installation. This can involve downloading the software from a repository, compiling code, or using a package manager. Consider using package managers, such as pip or conda, to simplify the installation.\nExample: The scikit-learn GitHub repository provides a good example of the Installation section of a README.\n\n\nUsage\nYou can include the simplest possible usage example directly in the README and provide more complex examples in additional files or links.\nExample: The TensorFlow GitHub repository gives a small usage sample after installation instructions and provides a link with additional examples and tutorials.\n\n\n\nLicense\nYour README should specify the licensing terms. For more information, see the License guide.\n\n\n\n\n\n\n Tip\n\n\n\nIt is recommended to write README‚Äôs in markdown (README.md) formatting and placed in the project‚Äôs top-level directory. If you find your README is becoming too long, consider incorporating additional documentation files instead of omitting important details.\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nMake a README - README 101\nMaking READMEs readable",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Documentation",
      "README"
    ]
  },
  {
    "objectID": "docs/software/fair_software/fair.html",
    "href": "docs/software/fair_software/fair.html",
    "title": "FAIR Software",
    "section": "",
    "text": "While originally targetting data management, the FAIR for Research Software (FAIR4RS) extends the FAIR principles to research software, which, unlike data, is executable and evolves over time. Ensuring the findability of software involves metadata, identifiers, and version control systems, while accessibility includes guidelines for obtaining, installing, and running the software. Interoperability involves adherence to community-driven standards or protocols, and reusability requires detailed documentation and user guides to effectively apply the software in new research projects.\n\n\n\n FAIR Software Checklist\nSet of recommendations for FAIR software. Add FAIR cards to your repository to track progress.\n\nLearn more ¬ª\n\n\n\n Software Management Plan\n\nLearn more ¬ª\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nFAIR Guiding Principles for scientific data management and stewardship to research software\nFAIR4RS community in Zenodo\nFAIR Software Checklist - five recommendations for FAIR (scientific) software",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "FAIR Software"
    ]
  },
  {
    "objectID": "docs/software/getting_started.html",
    "href": "docs/software/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "üèóÔ∏è Under construction",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Getting started"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/index.html",
    "href": "docs/software/releases_archiving/index.html",
    "title": "Packaging, releases and archiving",
    "section": "",
    "text": "Eventually your software will be at a sharable and publishable point. One way to distribute your software is to package it and release it on a platform like PyPI (Python Package Index), or CRAN (Comprehensive R Archive Network). Additionally, you want to archive your software for long-term preservation in repositories like Zenodo or 4TU.\n\n\n\n Packaging\nPackage your software.\n\nLearn more ¬ª\n\n\n\n Releases\nPublish your software.\n\nLearn more ¬ª\n\n\n\n Archiving\nArchive your software.\n\nLearn more ¬ª",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html",
    "title": "Creating a Python package",
    "section": "",
    "text": "By turning your code into a package and hosting it on a platform like PyPI (Python Package Index), you enhance the quality and sustainability of your software, promote reuse and embrace contributions from external collaborators.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#pyproject.toml",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#pyproject.toml",
    "title": "Creating a Python package",
    "section": "pyproject.toml",
    "text": "pyproject.toml\nIn the Development workflow section we looked at how to structure your project. Here we will focus on the pyproject.toml file, which is a configuration file used in Python projects to define build system requirements and project metadata. It is part of the PEP 517 and PEP 518 specifications, which aim to standardize the way Python projects are built and packaged. The pyproject.toml consists of TOML tables, and can include [build-system], [project], or [tools] tables.\n\n[build-system]\nThe [build-system] table is essential because it defines which build backend you will be using, and also which dependencies are required to build your project. This is needed because frontend tools like pip are not responsible for transforming your source code into a distributable package, and this is handled by one of the build backends (e.g.¬†setuptools, Hatchling).\n\n\n\n\n\n\n Example\n\n\n\n\n\n[build-system]\nrequires = [\"setuptools&gt;=64.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n\n\n\n\n[project]\nUnder the [project] table you can describe your metadata. It can become quite extensive, but this is where you would list the name of your project, version, authors, licensing, dependencies specific to your project, and other requirements, as well as other optional information. For a detailed list of what can be included under [project] check the Declaring project metadata section of Python Packaging Guide.\n\n\n\n\n\n\n Example\n\n\n\n\n\n[project]\n\nname = \"exampleproject\"\n# Define the name of your project here. This is mandatory. Once you publish your package for the first time,\n# this name will be locked and associated with your project. It affects how users will\n# install your package via pip, like so:\n#\n# $ pip install exampleproject\n#\n# Your project will be accessible at: https://pypi.org/project/exampleproject/\n#\nversion = \"2.0.0\"\n# Version numbers should conform to PEP 440, and are also mandatory (but they can be set dynamic)\n# https://www.python.org/dev/peps/pep-0440/\n#\ndescription = \"Short description of your project\"\n# Provide a short, one-line description of what your project does. This is known as the\n# \"Summary\" metadata field:\n# https://packaging.python.org/specifications/core-metadata/#summary\n#\nreadme = \"README.md\"\n# Here, you can include a longer description which often mirrors your README file.\n# This description will appear on PyPI when your project is published.\n# This corresponds to the \"Description\" metadata field:\n# https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#readme\n#\nrequires-python = \"&gt;=3.10\"\n# Indicate the versions of Python your project is compatible with. Unlike the\n# 'Programming Language' classifiers, 'pip install' will verify this field\n# and prevent installation if the Python version does not match.\n#\nlicense = {file = \"LICENSE.txt\"}\n# This specifies the license.\n# It can be a text (e.g. license = {text = \"MIT License\"}) or a reference to a file with the license text as shown above.\n#\nkeywords = [\"field_specific_keyword1\", \"field_specific_keyword2\"]\n# Keywords that describe your project. These assist users in discovering your project on PyPI searches.\n# These should be a comma-separated list reflecting the nature or domain of the project.\n#\nauthors = [\n  {name = \"A. Doe\", email = \"author@tudelft.nl\" }\n]\n# Information about the original authors of the project and their contact details.\n#\nmaintainers = [\n  {name = \"B. Smith\", email = \"maintainer@tudelft.nl\" }\n]\n# Information about the current maintainers of the project and their contact details.\n#\n#\n#\n# Classifiers help categorize the project on PyPI and aid in discoverability.\n# For a full list of valid classifiers, see https://pypi.org/classifiers/\nclassifiers = [\n  # Indicate the development status of your project (maturity). Commonly, this is\n  #   3 - Alpha\n  #   4 - Beta\n  #   5 - Stable\n  #.  6 - Mature\n  \"Development Status :: 4 - Beta\",\n\n  # Target audience\n  \"Intended Audience :: Developers\",\n  \"Topic :: Scientific/Engineering\",\n\n  # License type\n  \"License :: OSI Approved :: MIT License\",\n\n  # Python versions your software supports. This is not checked by pip install, and is different from \"requires-python\".\n  \"Programming Language :: Python :: 3\",\n  \"Programming Language :: Python :: 3.8\",\n  \"Programming Language :: Python :: 3.9\",\n  \"Programming Language :: Python :: 3.10\",\n  \"Programming Language :: Python :: 3.11\",\n  \"Programming Language :: Python :: 3 :: Only\",\n]\n\n# Dependencies needed by your project. These packages will be installed by pip when\n# your project is installed. Ensure these are existing, valid packages.\n#\n# For more on how this field compares to pip's requirements files, see:\n# https://packaging.python.org/discussions/install-requires-vs-requirements/\ndependencies = [\n  \"numpy\", \n  \"pandas&gt;=1.5.3\", \n  \"matplotlib&gt;=3.7.1\"\n]\n#\n# You can define additional groups of dependencies here (e.g., development dependencies).\n# These can be installed using the \"extras\" feature of pip, like so:\n#\n#   $ pip install exampleproject[dev]\n#\n# These are often referred to as \"extras\" and provide optional functionality.\n[project.optional-dependencies]\ntest = [\"coverage\"]\n#\n[project.urls]\n\"Homepage\" = \"https://github.com/your_handle_or_organisation\"\n\"Source\" = \"https://github.com/your_handle_or_organisation/exampleproject\"\n#\n# List of relevant URLs for your project. These are displayed on the left sidebar of your PyPI page.\n# This can include links to the homepage, source code, changelog, funding, etc.\n#\n#\n# This [project] example was adopted from https://github.com/pypa/sampleproject/blob/main/pyproject.toml\n\n\n\n\n\n[tools]\nThe [tool] table contains subtables specific to each tool. For example, Poetry uses the [tool.poetry] table instead of the [project] table.\n\n\n\n\n\n\n Example: Poetry project setup\n\n\n\n\n\n\n\n\n\n\n\n\n Difference between [build system] and [project]\n\n\n\n The [build-system] and [project] tables serve distinct roles. The [build-system] table must always be included, as it specifies the build tool used, regardless of the backend. On the other hand, the [project] table is recognized by most build backends for defining project metadata, though some backends may not and use a different format.\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nWriting your pyproject.toml\npyproject.toml specification\nConfiguring setuptools using pyproject.toml files\n\n\n\nBefore shifting to pyproject.toml, a common approach was to use a setup.py build script. You might encounter them in legacy projects.\n\n\n\n\n\n\n Learn more\n\n\n\n\nIs setup.py deprecated?\nHow to modernize a setup.py based project?",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#package-structuring",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#package-structuring",
    "title": "Creating a Python package",
    "section": "Package structuring",
    "text": "Package structuring\nIf you want to distribute your Python code as a package, you will need to have an __init__.py file in the root directory of your package. This allows Python to treat that directory as a package that can be imported. Every subfolder should also contain an __init__.py file.\nWhen importing a package, Python searches through the directories on sys.path looking for the package subdirectory. The presence of __init__.py files within these directories is crucial, as it tells Python that these directories should be treated as packages. This mechanism helps avoid the scenario where directories with commonplace names accidentally overshadow valid modules that appear later in the search path.\nWhile __init__.py can simply be an empty file, serving just to mark a directory as a package, it can also contain code that runs when the package is imported. This code can initialize package-level variables, import submodules, and other tasks.\nReferring to the project structure in our Development workflow guide, we can build on top of that structure.\n\n\n\n\n\n\n Reminder about flat vs src layout\n\n\n\n\n\nIn a flat layout, the project‚Äôs root directory directly contains the package directories and modules. This layout is straightforward and works well for simple projects.\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ your_pkg_name/ \n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ module.py\n‚îÇ   ‚îî‚îÄ‚îÄ subpkg1/\n‚îÇ       ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ\n...\nThe src layout places the package directory inside a top-level src directory. This layout helps prevent accidental imports from the current working directory, ensuring that you always import from the installed package rather than the source directory.\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ your_pkg_name/ \n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ module.py\n‚îÇ       ‚îî‚îÄ‚îÄ subpkg1/\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ\n...\n\n\n\nSo our example package structure would now look like this:\n\nPython\n\n\nyour_project/\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                     # Documentation directory\n‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks\n‚îú‚îÄ‚îÄ src/                      # Contains your main code\n‚îÇ   ‚îî‚îÄ‚îÄ your_pkg_name/        # A folder where your organized code lives - your package\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # A marker file for package initialization\n‚îÇ       ‚îú‚îÄ‚îÄ module.py         # A nested module\n‚îÇ       ‚îî‚îÄ‚îÄ subpkg1/          # A sub-package\n‚îÇ           ‚îî‚îÄ‚îÄ __init__.py   # A marker file for sub-package initialization\n‚îú‚îÄ‚îÄ tests/                    # Your test directory  \n‚îÇ\n‚îú‚îÄ‚îÄ data/                     # Data files used in the project (if applicable)\n‚îú‚îÄ‚îÄ processed_data/           # Files from your analysis (if applicable)\n‚îú‚îÄ‚îÄ results/                  # Results (if applicable)\n‚îÇ\n‚îú‚îÄ‚îÄ .gitignore                # Untracked files \n‚îú‚îÄ‚îÄ pyproject.toml            # TOML file\n‚îÇ\n‚îÇ\n‚îú‚îÄ‚îÄ README.md                 # README\n‚îî‚îÄ‚îÄ LICENSE                   # License information\n\n\n\nYou might notice that in our updated structure the requirements.txt is absent. In many cases, if you have a pyproject.toml file, you may not need a requirements.txt file anymore, since the pyproject.toml file is part of the new standardized Python packaging format (defined in PEP 518) and can include dependencies.\nHowever, some deployment and CI/CD pipelines might still expect a requirements.txt file, because a set of fixed dependency versions creates more stable pipelines. For simple projects, you can still prefer to use a requirements.txt for its simplicity and wide adoption.\n\nIt is not considered best practice to use the pyproject.toml to pin dependencies to specific versions or to specify sub-dependencies (i.e.¬†dependencies of your dependencies). This is overly-restrictive, and prevents a user from gaining the benefit of dependency upgrades. For more info, see this discussion.\n\n\nlib/ and build/ directories\nIt is possible that you might have lib/ and build/ directories in your project. These directories are not part of the standard Python package structure, but they can be created by certain tools or processes.\n\nThe build/ directory is typically used to store compiled or built artifacts of your project, such as binary executables, wheels, or other distribution files. This directory is usually not part of your source code repository and is generated during the (automated) build or packaging process.\nThe lib/ directory stores third-party libraries or dependencies that are not installed through a package manager. By specifying your project‚Äôs dependencies in the pyproject.toml file, and using a package manager like pip or poetry to install and manage them, these dependencies will be automatically downloaded and installed in the appropriate location (usually the site packages directory).\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nGood integration practices\nPython documentation packages",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#local-package-installation",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#local-package-installation",
    "title": "Creating a Python package",
    "section": "Local package installation",
    "text": "Local package installation\nBy installing a Python package locally during development you can test your changes in an environment that mimics how the package will be used once it is deployed. This process allows you to ensure that your package works correctly when installed and imported by others.\nYou can use pip to install your package in editable mode (-e). This way, changes you make to the source code are immediately available without needing to reinstall the package.\npip install -e .",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/packaging/packaging_python.html#next-steps",
    "href": "docs/software/releases_archiving/packaging/packaging_python.html#next-steps",
    "title": "Creating a Python package",
    "section": "Next steps",
    "text": "Next steps\nOnce you have your package ready, you can publish it. Visit our Release your Python package guide for information on how to publish your package to PyPI.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Packaging",
      "Create a Python package"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html",
    "href": "docs/software/releases_archiving/releases/releases.html",
    "title": "Releases",
    "section": "",
    "text": "Once you have your software packaged, you can release it to platforms like PyPI or CRAN. While your software might be already publicly available on GitHub, releasing it to a package repository allows easy access and installation for users. The most common platforms for releasing software packages are PyPI for Python packages and CRAN for R packages.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#changelog",
    "href": "docs/software/releases_archiving/releases/releases.html#changelog",
    "title": "Releases",
    "section": "Changelog",
    "text": "Changelog\nMaintaining a changelog provides a clear history of changes made to your software and version information. It helps users and developers understand what has changed between versions, what modifications, fixes, and enhancements have been made.\n\nHow do I make a good changelog?\nExample from eScience Center",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#semantic-versioning",
    "href": "docs/software/releases_archiving/releases/releases.html#semantic-versioning",
    "title": "Releases",
    "section": "Semantic versioning",
    "text": "Semantic versioning\nSemantic Versioning (SemVer) is a versioning scheme that reflects changes in your software systematically. It consists of three numbers: major, minor, and patch (e.g., 1.9.1).\n\nMajor version increments are meant for significant changes that may make backward-incompatible changes.\nMinor version increments add functionality in a backward-compatible manner.\nPatch version increments are for backward-compatible bug fixes only.\n\nAdopting this practice helps both users and developers anticipate the impact of updating the software. It clearly communicates the nature of the changes.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases.html#release-notification",
    "href": "docs/software/releases_archiving/releases/releases.html#release-notification",
    "title": "Releases",
    "section": "Release notification",
    "text": "Release notification\nAfter a release, it is important to communicate what has changed. Release notes are detailed descriptions of the new changes, fixes, and sometimes known issues. They are usually published alongside the changelog in repositories. You could use GitHub Releases, it is a feature that allows you to present your software, along with the corresponding source code, changelog, and release notes.\nIt is typical for a release on PyPI or CRAN to mirror what was put under a GitHub release. This is not a requirement, but it is a good practice to keep the information consistent across platforms.\n\nAutomatically generated release notes\nGitHub provides a useful feature to automatically generate release notes for new versions of your software. It scans the commits between your releases and compiles a summary of the changes, fixes, and enhancements made. This can not only save time but also help to avoid undocumenting changes.\nHow to enable automatic release notes:\n\nGo to your repository on GitHub and navigate to the releases section.\nDraft a new release. When you select a tag, GitHub will offer an option to auto-generate the release notes based on the commits since the last release.\nCustomize the release notes. You can edit the auto-generated content to add more details or format it according to your preferences.\nWhen publishing, the release notes will be attached to your release.\n\nThis feature is particularly helpful for maintaining accurate and up-to-date release documentation.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html",
    "title": "Release your Python package",
    "section": "",
    "text": "After bundling your source code into a package, you can publish it to PyPI. This will allow others to easily install your package using pip.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html#testing-packaging-on-testpypi-before-publishing-to-pypi",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html#testing-packaging-on-testpypi-before-publishing-to-pypi",
    "title": "Release your Python package",
    "section": "Testing packaging on TestPyPI before publishing to PyPI",
    "text": "Testing packaging on TestPyPI before publishing to PyPI\nBy testing your package on TestPyPI before publishing it to PyPI, you can identify and address any issues with your package metadata, dependencies, or distribution files before making your package publicly available.\nYou‚Äôll need to create an account for TestPyPI. The next step is to create distribution packages for your package. These packages are archives that can be uploaded to TestPyPI/PyPI and installed using pip. Then you can use Twine to upload your package to TestPyPI.\n\nRegister on TestPyPI.\nCheck if PyPA build is installed:\npip install --upgrade build\nTo create distribution packages, navigate to the directory where your pyproject.toml file is located, and run:\n\n\nLinux/macOSWindows\n\n\npython3 -m build\n\n\npy -m build\n\n\n\n\n\n\n\n\n\n Output\n\n\n\nAfter running this command, you‚Äôll see a substantial amount of text output. Upon completion, it will generate two files (a wheel and .tar.gz file) in the dist/ directory. The .tar.gz file represents a source distribution, while the .whl file is a built distribution. More recent versions of pip prioritize the installation of built distributions, reverting to source distributions if necessary. It‚Äôs advisable to always upload a source distribution and include built distributions compatible with the platforms your project supports.\n\n\n\nThen, install Twine:\npip install twine\nUpload to TestPyPI by specifying the --repository flag:\ntwine upload --repository testpypi dist/*\n\nYou will be prompted to enter your username and password for TestPyPI.\nIf you have a ~/.pypirc file, Twine will use the credentials from there.\n\nYou will find your package on https://test.pypi.org/project/your_pkg_name. You can then pip install it by adding the --index-url flag:\npip install --index-url https://test.pypi.org/simple/your_pkg_name",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/releases_archiving/releases/releases_pypi.html#publishing-to-pypi",
    "href": "docs/software/releases_archiving/releases/releases_pypi.html#publishing-to-pypi",
    "title": "Release your Python package",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nOnce you have tested your package on TestPyPI and ensured everything works as expected, you can publish it to the official PyPI repository. It will be accessible to anyone in the Python community, allowing them to install your package using a simple pip install your_pkg_name command.\nYou‚Äôll also need an account for PyPI. TestPyPI and PyPI use separate databases so you need to register on both sites. However, the workflow is about the same:\n\nRegister on PyPI.\nRun pip install --upgrade build to ensure you have the latest version of the build tool.\nNavigate to the directory where your pyproject.toml file is located, and run:\n\n\nLinux/macOSWindows\n\n\npython3 -m build\n\n\npy -m build\n\n\n\n\nUpload your package to PyPI:\ntwine upload dist/*\n\nInput your credentials associated with the account you registered on the official PyPI platform.\n\nYour package is live on PyPI!\nYou can now install it by simply pip install your_pkg_name.\n\n\n\n\n\n\n\n Tip\n\n\n\nIf you need a particular name for your package, check whether it is taken on PyPI and claim it as soon as possible if available.\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nRealPython Packaging guide\nTwine Read the Docs\nUsing TestPyPI",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Publish and share",
      "Releases",
      "Release your Python package on PyPI"
    ]
  },
  {
    "objectID": "docs/software/testing/intermediate.html",
    "href": "docs/software/testing/intermediate.html",
    "title": "More testing concepts",
    "section": "",
    "text": "Code Coverage\nCode coverage measures how much of your code is executed during testing. It is a useful metric to ensure that your tests are comprehensive and indicate your code‚Äôs quality. If your software becomes a dependency for others, a code coverage of 70% or higher is recommended for unit tests.\n\nPythonMATLAB\n\n\n\npytest-cov plugin (for pytest)\nCoverage.py documentation\n\n\n\n\nCollect code coverage with Command Window execution (since R2023b)\nCode coverage with Test Browser (since R2023a)\nCollect code coverage metrics\n\n\n\n\n\n\nError handling\nTests should check if your code behaves as expected when it encounters errors. This includes testing if the code raises the correct exceptions when given invalid input or when an error occurs.\n\nPythonMATLAB\n\n\n\nAssert raised exceptions\n\nExample in Python:\ndef divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n\ndef test_divide_by_zero():\n    with pytest.raises(ValueError):\n        divide(1, 0)\n\n\n\nVerify function throws specific exceptions\n\n\n\n\n\n\nFixtures\nFixtures are predefined states or sets of data used to set up the testing environment, ensuring consistent conditions for tests to run reliably. Fixtures can be used to set up databases, create temporary files, or initialize other resources, that then available to all tests in a test suite.\n\nPythonMATLAB\n\n\n\nHow to use fixtures\n\n\n\n\nCreate shared fixtures\n\n\n\n\n\n\nParameterization\nParameterization involves running the same test with different inputs or configurations to ensure broader coverage and identify potential edge cases.\n\nPythonMATLAB\n\n\n\nParameterizing unit tests\n\n\n\n\nCreate a basic parameterized test\n\n\n\n\n\n\nMocking\nMocking (or monkeypatching) is a technique used to simulate the behavior of dependencies or external systems during testing, allowing isolated testing of specific components. For example, if your software requires a connection to a database, you can mock this interaction during testing.\n\n\n\nMathWorks, MATLAB Mocking Diagram, MATLAB Documentation, link to image.\n\n\n\n\n\n\n\n\nüêí Monkeypatching?\n\n\n\n\n\nThe term monkey patch seems to have come from an earlier term, guerrilla patch, which referred to changing code sneakily ‚Äì and possibly incompatibly with other such patches ‚Äì at runtime. The word guerrilla, nearly homophonous with gorilla, became monkey, possibly to make the patch sound less intimidating.\n\n\n\n\nPythonMATLAB\n\n\n\nHow to monkeypatch/mock modules and environments\n\n\n\n\nCreate Mock Object\n\n\n\n\n\n\nMarks and tags\nYou can use test tags to group tests into categories and then run tests with specified tags. This is useful when you want to run only a subset of tests, such as regression tests, or when ignoring slow tests during development.\n\nPythonMATLAB\n\n\n\nWorking with custom markers\n\n\n\n\nTag unit tests",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Advanced concepts"
    ]
  },
  {
    "objectID": "docs/software/testing/python.html",
    "href": "docs/software/testing/python.html",
    "title": "Testing in Python",
    "section": "",
    "text": "In Python, two widely used testing frameworks are pytest and unittest. This guide focuses on pytest, which is recommended for its simplicity and readability. If you are new to testing in Python, pytest is a great starting point.\n\n\n\n\n\n\n What is the difference between pytest and unittest?\n\n\n\n\n\n\npytest is a third-party testing framework that is more user-friendly, requires less boilerplate, and offers better readability.\nunittest is part of the Python standard library and follows a more traditional object-oriented style of writing tests.\n\n\n\n\n\nStep 1. Setup a testing framework\nInstall pytest\npip install pytest\nA good practice is to organize the codebase into a src directory for the source code and a tests directory for the test suite. For example:\nsrc/\n    mypkg/\n        __init__.py\n        add.py\n        draw_random_number.py\ntests/\n    test_add.py\n    test_draw_random_number.py\n    ...\n\n\nStep 2. Identify testable units\nIdentify functions, methods, or classes on your code that should be tested. Focus on critical computations and potential points of failure.\n\n\nStep 3. Write test cases\nWrite test functions using pytest. Here‚Äôs an example:\n# src/mypkg/add.py\ndef add(x,y):\n    return x + y\n\n# tests/test_add.py\nfrom mypkg.add import add\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(0, 0) == 0\n    assert add(-1, -1) == -2\n\n\n\n\n\n\n Tip\n\n\n\nLimit the number of assert statements in a single test function. Otherwise, when a particular assert fails, the remaining assertions in the test function will not be executed.\n\n\n\n\nStep 4. Run tests locally\nRun all tests in the project by executing the following command:\npytest\nRun a specific test file:\npytest tests/test_add.py\n\n\nStep 5. Debug and fix failing tests\nThe test results displayed in the console will help you to identify any failures or errors. If errors occur, debug the failing tests by examining failure messages and stack traces.\n\n\nStep 6. Run coverage report locally\nGenerate a coverage report to gain insights into which parts of the codebase have been executed during testing (see Code Coverage).\npip install pytest-cov\npytest --cov=mypkg tests/\n\n\nStep 7. Automate testing with Continuous Integration\nIntegrate youre test suite with a Continuous Integration service (e.g., GitHub Actions) to run tests automatically on every code change.\n\n\n\n\n\n\n Learning materials for automated testing\n\n\n\n\nIntermediate Research Software Development - CI for Automated Testing\nCode Refinery - Automated testing\n\n\n\n\n\nExamples of repositories with tests\n\neScience Center - Project matchms\nPandas library - Repository tests\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nPytest - Getting Started\nCode Refinery - Pytest exercise\nRealPython - Effective testing with pytest",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Testing in Python"
    ]
  },
  {
    "objectID": "docs/software/testing/strategies.html",
    "href": "docs/software/testing/strategies.html",
    "title": "Testing strategy",
    "section": "",
    "text": "In designing test cases for research software, it can be useful to conceptually differentiate between tests that verify the technical correctness of the code and tests that check the scientific validity of the results. With technical software tests, you check whether a function behaves as expected. With a scientific test, you compare the outcome of a function to known (experimental) scientific results.\nThe following questions can help you decide what to test in your software:\n\nHow can I ensure the algorithms and mathematical models implemented in the software are correct?\nHow can I verify that the input data types, formats, and ranges adhere to expected standards and constraints?\nHow does the software behave at boundary conditions and extreme values of input parameters?\nHow does the software perform under varying workloads and dataset sizes, and is it scalable for large-scale simulations or data processing tasks?\nHow do I compare the software‚Äôs results against existing literature, experimental data, or previous simulations to check their accuracy?\n\n\nRoadmap to testing\n\nNew to testingFamiliar with testing\n\n\n\n\n\n\n\n\n1. Familiarize yourself with the basics\n\n\n\n\n\nBegin by learning a testing framework that is well-suited for your programming language; for example, you might explore pytest if you are using Python. It is also important to understand the basic types of tests, focusing primarily on unit tests and simple integration tests.\n\n\n\n\n\n\n\n\n\n2. Identify critical components in your code\n\n\n\n\n\nTake the time to inspect your codebase and determine which parts are most important or particularly prone to error. These are the areas where you should focus your testing efforts.\n\n\n\n\n\n\n\n\n\n3. Start small\n\n\n\n\n\nBegin by writing minimal (unit) tests for individual functions or modules. Creating tests based on expected inputs and outputs will help confirm that your code behaves as intended. The primary goal at this stage is to understand the testing process rather than aiming for complete coverage from the start.\n\n\n\n\n\n\n\n\n\n4. Incrementally expand the number of tests\n\n\n\n\n\nAs you introduce new functionality, adopt an approach by writing tests alongside your new code. Over time, gradually add tests to your existing code ‚Äì especially when you make changes or improvements ‚Äì to steadily increase overall test coverage and improve the reliability of your software. Make sure to focus on testing the critical parts of your codebase first.\n\n\n\n\n\n\n\n\n\n5. Refactor your code for testability\n\n\n\n\n\nIf you find it difficult to write tests for your codebase, consider refactoring it into smaller, more testable units. Clear documentation and comments on your functions will further aid in writing tests by providing a well-defined understanding of each component‚Äôs intent.\n\n\n\n\n\n\n\n\n\n6. Automate and adopt\n\n\n\n\n\nFinally, add automated testing to your development workflow by using a continuous integration tool to run tests automatically with each code change. Establishing a regular habit of testing will, over time, lead to significant improvements in the quality and reliability of your research code.\n\n\n\n\n\n\n\n\n\n\n\n1. Develop a testing strategy\n\n\n\n\n\nFor researchers already experienced with testing, it is useful to develop a testing strategy. Start by adopting the test pyramid approach: ensure that all core functions and algorithms are covered by unit tests, verify that modules interact correctly with integration tests, and, when applicable, use end-to-end tests to simulate real-world user workflows. Regularly running regression tests is also important to catch any unintended side effects of code changes.\nAim for comprehensive test coverage to ensure that critical parts of your codebase are thoroughly tested. A good benchmark is to test at least 70% of your code base with unit tests.\n\n\n\n\n\n\n\n\n\n2. Adopt a practice of writing tests first\n\n\n\n\n\nConsider methodologies such as Test-Driven Development (TDD) into your workflow. With TDD, you write tests before you write the actual code to define the desired behavior, ensure clear specifications, and obtain immediate feedback.\n\n\n\n\n\n\n\n\n\n3. Research-specific testing\n\n\n\n\n\nIn a research context, certain quality measures become particularly important. Prioritize reproducibility by writing tests that verify experiments yield consistent outputs for a given dataset and configuration. If your code relies on statistical methods, use fixed random seeds to ensure reproducibility across different runs. Additionally, consider implementing validation tests that compare your results against known benchmarks or experimental data.\n\n\n\n\n\n\n\n\n\n4. Consider advanced testing practices\n\n\n\n\n\n\nCoverage Analysis:\nEmploy code coverage tools to identify untested paths and critical areas that require additional testing.\nParameterization:\nRun tests with a range of inputs to validate robustness across different scenarios.\nError Handling: Verify that your code behaves as expected when encountering errors.\nFixtures: Use fixtures to set up a consistent and reusable testing environment.\nMocking:\nUse mocks to simulate external systems or heavy computations, isolating tests for faster feedback.\nTagging and Filtering: Organize tests into categories and run specific subsets based on tags or filters.  Advanced testing practices\n\n\n\n\n\n\n\n\n\n\n5. Integrate with CI/CD\n\n\n\n\n\nFinally, integrate your testing processes into a continuous integration/deployment pipeline. Automate your test runs so that every code commit triggers a suite of tests to catch issues early in the development cycle. Monitor test performance and code coverage over time to continuously refine and enhance your testing strategy, ensuring a high level of quality and reliability in your research software.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Software testing",
      "Roadmap to testing"
    ]
  },
  {
    "objectID": "docs/software/tools/tools.html",
    "href": "docs/software/tools/tools.html",
    "title": "Tooling",
    "section": "",
    "text": "üèóÔ∏è Under construction!",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Tooling"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html",
    "href": "docs/software/development_workflow/branch_management.html",
    "title": "Branch management",
    "section": "",
    "text": "Branch management in Git is essential for collaborative software development in version-controlled environments. The core advantage of branching is that it provides separate, dedicated environments for code development, independent of the main (working) version. This approach enables parallel development streams, allowing for experimentation and modification without impacting the stable main version of the project. Note, this approach is also valuable for projects with only one developer!\nA branching strategy defines a set of best practices for writing, merging, and releasing code. Choosing the right approach helps teams collaborate efficiently while maintaining a stable codebase.",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html#branch-models",
    "href": "docs/software/development_workflow/branch_management.html#branch-models",
    "title": "Branch management",
    "section": "Branch models",
    "text": "Branch models\nWhile multiple branching strategies exist, GitHub Flow and GitFlow are well-suited for research software projects, depending on collaboration style and release cycle:\n\nGitHub Flow is a simplified and straightforward workflow, relying on a single main branch. Developers create so-called feature branches off of the main branch, work on their changes, and then merge them back into the main branch via pull requests. The process is built around the principle of continuous collaboration and is particularly useful for projects where regular updates and deployments are common.\nGitflow relies on two primary branches - main and develop. Developers create feature branches off of the develop branch. Once a feature is complete, it is merged back into the develop branch, which itself is merged with main for each new release of the software. It can be a better choice for managing larger projects with distinct release cycles and versioning requirements (e.g., tools tied to publications) or requiring parallel development of features and experiments.\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nGitHub Flow getting started\nIntroduction GitHub Flow",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  },
  {
    "objectID": "docs/software/development_workflow/branch_management.html#branch-management-in-action",
    "href": "docs/software/development_workflow/branch_management.html#branch-management-in-action",
    "title": "Branch management",
    "section": "Branch management in action",
    "text": "Branch management in action\n\nPersonal projects and small teams\nGitHub Flow model:\n\nTypically starts with just the main branch.\nUse branches for unfinished/untested ideas.\nUse branches when you are not sure about a change.\nAdd tags to mark important milestones.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#bf8040',\n              'git2': '#40bfbf'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph LR:\n   commit\n   commit\n   branch \"feature_a\"\n   checkout \"feature_a\"\n   commit\n   commit type:REVERSE\n   checkout main\n\n   commit\n   branch \"feature_b\"\n   checkout \"feature_b\"\n   commit\n   commit\n   commit\n\n   checkout main\n   merge \"feature_b\"\n   commit tag:\"v1.0.0\"\n\n\n\n\n\n\n\nWhen applying this workflow for small teams, you accept things breaking sometimes.\nWhen there is more control required, follow:\n\nThe main branch is write-protected.\nYou create new feature branches for changes.\nChanges are reviewed before they are merged to the main branch.\nOnly merges to main branch through pull requests (and optionally code reviews).\n\n\n\n\n\n\n\nWhat is the difference between master and main branches?\n\n\n\n\n\nGitHub decided to rename the default branch from master to main for new repositories after October 2020. This change was part of a broader industry move to replace terms that may be considered insensitive or non-inclusive. It‚Äôs important to note that while the terms main and master refer to the default branch in a repository, they are functionally the same. Be aware that repositories created before October 2020 may still use master instead of main.\n\n\n\n\n\nDistributing releases\nWhen you need to distribute releases, your main branch will serve as the latest stable version.\n\nThe main branch is protected and read-only.\nYou set up a develop branch for active development.\nCreate feature branches of the develop branch.\nMerge feature branches (through Pull Requests) back to develop.\nOnly merge develop into main when releasing a new stable (and tested) version.\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#bf8040',\n              'git3': '#40bfbf'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n    \n   commit tag:\"v0.1.0\"\n   branch develop\n   commit\n\n   checkout develop\n   commit\n   branch feature_a\n   checkout feature_a\n   commit\n   checkout develop\n   merge feature_a\n   commit\n   commit\n   branch feature_b\n   checkout feature_b\n   commit\n   commit\n   checkout develop\n   merge feature_b\n   commit\n\n   checkout main\n   merge develop tag:\"v0.2.0\"\n\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop tag:\"v0.3.0\"\n\n   checkout develop\n   commit\n   commit\n\n\n\n\n\n\n\n\n\nAdd additional supporting branches\nWhen a critical bug in the stable version must be resolved immediately, a hotfix branch may be branched off from the corresponding tag on the main branch that marks this version. After fixing, hotfix is then merged with both main and develop.\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#7da97a'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n    \n   commit tag:\"v0.2.0\"\n   branch develop\n   commit\n   commit\n   commit\n   commit\n   checkout main\n   branch hotfix\n   commit\n   commit\n   checkout develop\n   merge hotfix\n   checkout main\n   merge hotfix tag:\"v0.2.1\"\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop tag:\"v0.3.0\"\n   \n\n\n\n\n\n\n\n\nThe complete Gitflow model in action\nGitFlow‚Äôs structured approach with parallel production and integration branches, supplemented by feature/release/hotfix branches, was specifically designed for versioned software requiring maintenance of multiple production releases.\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': {\n              'git0': '#bfbf40',\n              'git1': '#4080bf',\n              'git2': '#bf8040',\n              'git3': '#eae0b8',\n              'git4': '#7da97a'\n              }, 'gitGraph': {'showCommitLabel': false}}}%%\ngitGraph\n   commit\n   branch develop\n   commit\n   checkout develop\n   commit\n   branch feature\n   checkout feature\n   commit\n   commit\n   checkout develop\n   merge feature\n   commit\n   branch release\n   checkout release\n   commit\n   commit\n   checkout main\n   merge release\n   checkout develop\n   merge release\n   checkout main\n   branch hotfix/security\n   checkout hotfix/security\n   commit\n   checkout main\n   merge hotfix/security\n   checkout develop\n   merge hotfix/security\n\n\n\n\n\n\n\n\n\n\n\n\n Learn more\n\n\n\n\nCode Refinery - Branching models\nGitHub - Branch protection rules\nTag names following semantic versioning\nGitHub tags and releases",
    "crumbs": [
      "<ins>Guides</ins>",
      "![](/docs/img/laptop-code-solid.svg){height=1em style='filter: invert(0.5) sepia(1) saturate(5) hue-rotate(175deg)'} **Research Software**",
      "Development workflow",
      "Branch management"
    ]
  }
]